diff --git a/.gitignore b/.gitignore
index de64a2a2..7b58c46c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -6,6 +6,7 @@ logs/
 data/
 !test/data
 conf/hotspot_compiler
+conf/cassandra.yaml
 doc/cql3/CQL.html
 doc/build/
 lib/
diff --git a/conf/cassandra.yaml b/conf/cassandra.yaml
deleted file mode 100644
index 0f329834..00000000
--- a/conf/cassandra.yaml
+++ /dev/null
@@ -1,1803 +0,0 @@
-
-# Cassandra storage config YAML
-
-# NOTE:
-#   See https://cassandra.apache.org/doc/latest/configuration/ for
-#   full explanations of configuration directives
-# /NOTE
-
-# The name of the cluster. This is mainly used to prevent machines in
-# one logical cluster from joining another.
-cluster_name: 'Test Cluster'
-
-# This defines the number of tokens randomly assigned to this node on the ring
-# The more tokens, relative to other nodes, the larger the proportion of data
-# that this node will store. You probably want all nodes to have the same number
-# of tokens assuming they have equal hardware capability.
-#
-# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,
-# and will use the initial_token as described below.
-#
-# Specifying initial_token will override this setting on the node's initial start,
-# on subsequent starts, this setting will apply even if initial token is set.
-#
-# See https://cassandra.apache.org/doc/latest/getting_started/production.html#tokens for
-# best practice information about num_tokens.
-#
-num_tokens: 1
-
-# Triggers automatic allocation of num_tokens tokens for this node. The allocation
-# algorithm attempts to choose tokens in a way that optimizes replicated load over
-# the nodes in the datacenter for the replica factor.
-#
-# The load assigned to each node will be close to proportional to its number of
-# vnodes.
-#
-# Only supported with the Murmur3Partitioner.
-
-# Replica factor is determined via the replication strategy used by the specified
-# keyspace.
-# allocate_tokens_for_keyspace: KEYSPACE
-
-# Replica factor is explicitly set, regardless of keyspace or datacenter.
-# This is the replica factor within the datacenter, like NTS.
-allocate_tokens_for_local_replication_factor: 3
-
-# initial_token allows you to specify tokens manually.  While you can use it with
-# vnodes (num_tokens > 1, above) -- in which case you should provide a 
-# comma-separated list -- it's primarily used when adding nodes to legacy clusters 
-# that do not have vnodes enabled.
-# initial_token:
-
-# May either be "true" or "false" to enable globally
-hinted_handoff_enabled: true
-
-# When hinted_handoff_enabled is true, a black list of data centers that will not
-# perform hinted handoff
-# hinted_handoff_disabled_datacenters:
-#    - DC1
-#    - DC2
-
-# this defines the maximum amount of time a dead host will have hints
-# generated.  After it has been dead this long, new hints for it will not be
-# created until it has been seen alive and gone down again.
-# Min unit: ms
-max_hint_window: 3h
-
-# Maximum throttle in KiBs per second, per delivery thread.  This will be
-# reduced proportionally to the number of nodes in the cluster.  (If there
-# are two nodes in the cluster, each delivery thread will use the maximum
-# rate; if there are three, each will throttle to half of the maximum,
-# since we expect two nodes to be delivering hints simultaneously.)
-# Min unit: KiB
-hinted_handoff_throttle: 1024KiB
-
-# Number of threads with which to deliver hints;
-# Consider increasing this number when you have multi-dc deployments, since
-# cross-dc handoff tends to be slower
-max_hints_delivery_threads: 2
-
-# Directory where Cassandra should store hints.
-# If not set, the default directory is $CASSANDRA_HOME/data/hints.
-# hints_directory: /var/lib/cassandra/hints
-
-# How often hints should be flushed from the internal buffers to disk.
-# Will *not* trigger fsync.
-# Min unit: ms
-hints_flush_period: 10000ms
-
-# Maximum size for a single hints file, in mebibytes.
-# Min unit: MiB
-max_hints_file_size: 128MiB
-
-# The file size limit to store hints for an unreachable host, in mebibytes.
-# Once the local hints files have reached the limit, no more new hints will be created.
-# Set a non-positive value will disable the size limit.
-# max_hints_size_per_host: 0MiB
-
-# Enable / disable automatic cleanup for the expired and orphaned hints file.
-# Disable the option in order to preserve those hints on the disk.
-auto_hints_cleanup_enabled: false
-
-# Compression to apply to the hint files. If omitted, hints files
-# will be written uncompressed. LZ4, Snappy, and Deflate compressors
-# are supported.
-#hints_compression:
-#   - class_name: LZ4Compressor
-#     parameters:
-#         -
-
-# Enable / disable persistent hint windows.
-#
-# If set to false, a hint will be stored only in case a respective node
-# that hint is for is down less than or equal to max_hint_window.
-#
-# If set to true, a hint will be stored in case there is not any
-# hint which was stored earlier than max_hint_window. This is for cases
-# when a node keeps to restart and hints are not delivered yet, we would be saving
-# hints for that node indefinitely.
-#
-# Defaults to true.
-#
-# hint_window_persistent_enabled: true
-
-# Maximum throttle in KiBs per second, total. This will be
-# reduced proportionally to the number of nodes in the cluster.
-# Min unit: KiB
-batchlog_replay_throttle: 1024KiB
-
-# Authentication backend, implementing IAuthenticator; used to identify users
-# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,
-# PasswordAuthenticator}.
-#
-# - AllowAllAuthenticator performs no checks - set it to disable authentication.
-# - PasswordAuthenticator relies on username/password pairs to authenticate
-#   users. It keeps usernames and hashed passwords in system_auth.roles table.
-#   Please increase system_auth keyspace replication factor if you use this authenticator.
-#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)
-authenticator: AllowAllAuthenticator
-
-# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions
-# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,
-# CassandraAuthorizer}.
-#
-# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.
-# - CassandraAuthorizer stores permissions in system_auth.role_permissions table. Please
-#   increase system_auth keyspace replication factor if you use this authorizer.
-authorizer: AllowAllAuthorizer
-
-# Part of the Authentication & Authorization backend, implementing IRoleManager; used
-# to maintain grants and memberships between roles.
-# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,
-# which stores role information in the system_auth keyspace. Most functions of the
-# IRoleManager require an authenticated login, so unless the configured IAuthenticator
-# actually implements authentication, most of this functionality will be unavailable.
-#
-# - CassandraRoleManager stores role data in the system_auth keyspace. Please
-#   increase system_auth keyspace replication factor if you use this role manager.
-role_manager: CassandraRoleManager
-
-# Network authorization backend, implementing INetworkAuthorizer; used to restrict user
-# access to certain DCs
-# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllNetworkAuthorizer,
-# CassandraNetworkAuthorizer}.
-#
-# - AllowAllNetworkAuthorizer allows access to any DC to any user - set it to disable authorization.
-# - CassandraNetworkAuthorizer stores permissions in system_auth.network_permissions table. Please
-#   increase system_auth keyspace replication factor if you use this authorizer.
-network_authorizer: AllowAllNetworkAuthorizer
-
-# Depending on the auth strategy of the cluster, it can be beneficial to iterate
-# from root to table (root -> ks -> table) instead of table to root (table -> ks -> root).
-# As the auth entries are whitelisting, once a permission is found you know it to be
-# valid. We default to false as the legacy behavior is to query at the table level then
-# move back up to the root. See CASSANDRA-17016 for details.
-# traverse_auth_from_root: false
-
-# Validity period for roles cache (fetching granted roles can be an expensive
-# operation depending on the role manager, CassandraRoleManager is one example)
-# Granted roles are cached for authenticated sessions in AuthenticatedUser and
-# after the period specified here, become eligible for (async) reload.
-# Defaults to 2000, set to 0 to disable caching entirely.
-# Will be disabled automatically for AllowAllAuthenticator.
-# For a long-running cache using roles_cache_active_update, consider
-# setting to something longer such as a daily validation: 86400000
-# Min unit: ms
-roles_validity: 2000ms
-
-# Refresh interval for roles cache (if enabled).
-# After this interval, cache entries become eligible for refresh. Upon next
-# access, an async reload is scheduled and the old value returned until it
-# completes. If roles_validity is non-zero, then this must be
-# also.
-# This setting is also used to inform the interval of auto-updating if
-# using roles_cache_active_update.
-# Defaults to the same value as roles_validity.
-# For a long-running cache, consider setting this to 60000 (1 hour) etc.
-# Min unit: ms
-# roles_update_interval: 2000ms
-
-# If true, cache contents are actively updated by a background task at the
-# interval set by roles_update_interval. If false, cache entries
-# become eligible for refresh after their update interval. Upon next access,
-# an async reload is scheduled and the old value returned until it completes.
-# roles_cache_active_update: false
-
-# Validity period for permissions cache (fetching permissions can be an
-# expensive operation depending on the authorizer, CassandraAuthorizer is
-# one example). Defaults to 2000, set to 0 to disable.
-# Will be disabled automatically for AllowAllAuthorizer.
-# For a long-running cache using permissions_cache_active_update, consider
-# setting to something longer such as a daily validation: 86400000ms
-# Min unit: ms
-permissions_validity: 2000ms
-
-# Refresh interval for permissions cache (if enabled).
-# After this interval, cache entries become eligible for refresh. Upon next
-# access, an async reload is scheduled and the old value returned until it
-# completes. If permissions_validity is non-zero, then this must be
-# also.
-# This setting is also used to inform the interval of auto-updating if
-# using permissions_cache_active_update.
-# Defaults to the same value as permissions_validity.
-# For a longer-running permissions cache, consider setting to update hourly (60000)
-# Min unit: ms
-# permissions_update_interval: 2000ms
-
-# If true, cache contents are actively updated by a background task at the
-# interval set by permissions_update_interval. If false, cache entries
-# become eligible for refresh after their update interval. Upon next access,
-# an async reload is scheduled and the old value returned until it completes.
-# permissions_cache_active_update: false
-
-# Validity period for credentials cache. This cache is tightly coupled to
-# the provided PasswordAuthenticator implementation of IAuthenticator. If
-# another IAuthenticator implementation is configured, this cache will not
-# be automatically used and so the following settings will have no effect.
-# Please note, credentials are cached in their encrypted form, so while
-# activating this cache may reduce the number of queries made to the
-# underlying table, it may not  bring a significant reduction in the
-# latency of individual authentication attempts.
-# Defaults to 2000, set to 0 to disable credentials caching.
-# For a long-running cache using credentials_cache_active_update, consider
-# setting to something longer such as a daily validation: 86400000
-# Min unit: ms
-credentials_validity: 2000ms
-
-# Refresh interval for credentials cache (if enabled).
-# After this interval, cache entries become eligible for refresh. Upon next
-# access, an async reload is scheduled and the old value returned until it
-# completes. If credentials_validity is non-zero, then this must be
-# also.
-# This setting is also used to inform the interval of auto-updating if
-# using credentials_cache_active_update.
-# Defaults to the same value as credentials_validity.
-# For a longer-running permissions cache, consider setting to update hourly (60000)
-# Min unit: ms
-# credentials_update_interval: 2000ms
-
-# If true, cache contents are actively updated by a background task at the
-# interval set by credentials_update_interval. If false (default), cache entries
-# become eligible for refresh after their update interval. Upon next access,
-# an async reload is scheduled and the old value returned until it completes.
-# credentials_cache_active_update: false
-
-# The partitioner is responsible for distributing groups of rows (by
-# partition key) across nodes in the cluster. The partitioner can NOT be
-# changed without reloading all data.  If you are adding nodes or upgrading,
-# you should set this to the same partitioner that you are currently using.
-#
-# The default partitioner is the Murmur3Partitioner. Older partitioners
-# such as the RandomPartitioner, ByteOrderedPartitioner, and
-# OrderPreservingPartitioner have been included for backward compatibility only.
-# For new clusters, you should NOT change this value.
-#
-partitioner: org.apache.cassandra.dht.Murmur3Partitioner
-
-# Directories where Cassandra should store data on disk. If multiple
-# directories are specified, Cassandra will spread data evenly across 
-# them by partitioning the token ranges.
-# If not set, the default directory is $CASSANDRA_HOME/data/data.
-# data_file_directories:
-#     - /var/lib/cassandra/data
-
-# Directory were Cassandra should store the data of the local system keyspaces.
-# By default Cassandra will store the data of the local system keyspaces in the first of the data directories specified
-# by data_file_directories.
-# This approach ensures that if one of the other disks is lost Cassandra can continue to operate. For extra security
-# this setting allows to store those data on a different directory that provides redundancy.
-# local_system_data_file_directory:
-
-# commit log.  when running on magnetic HDD, this should be a
-# separate spindle than the data directories.
-# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.
-# commitlog_directory: /var/lib/cassandra/commitlog
-
-# Enable / disable CDC functionality on a per-node basis. This modifies the logic used
-# for write path allocation rejection (standard: never reject. cdc: reject Mutation
-# containing a CDC-enabled table if at space limit in cdc_raw_directory).
-cdc_enabled: false
-
-# CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the
-# segment contains mutations for a CDC-enabled table. This should be placed on a
-# separate spindle than the data directories. If not set, the default directory is
-# $CASSANDRA_HOME/data/cdc_raw.
-# cdc_raw_directory: /var/lib/cassandra/cdc_raw
-
-# Policy for data disk failures:
-#
-# die
-#   shut down gossip and client transports and kill the JVM for any fs errors or
-#   single-sstable errors, so the node can be replaced.
-#
-# stop_paranoid
-#   shut down gossip and client transports even for single-sstable errors,
-#   kill the JVM for errors during startup.
-#
-# stop
-#   shut down gossip and client transports, leaving the node effectively dead, but
-#   can still be inspected via JMX, kill the JVM for errors during startup.
-#
-# best_effort
-#    stop using the failed disk and respond to requests based on
-#    remaining available sstables.  This means you WILL see obsolete
-#    data at CL.ONE!
-#
-# ignore
-#    ignore fatal errors and let requests fail, as in pre-1.2 Cassandra
-disk_failure_policy: stop
-
-# Policy for commit disk failures:
-#
-# die
-#   shut down the node and kill the JVM, so the node can be replaced.
-#
-# stop
-#   shut down the node, leaving the node effectively dead, but
-#   can still be inspected via JMX.
-#
-# stop_commit
-#   shutdown the commit log, letting writes collect but
-#   continuing to service reads, as in pre-2.0.5 Cassandra
-#
-# ignore
-#   ignore fatal errors and let the batches fail
-commit_failure_policy: stop
-
-# Maximum size of the native protocol prepared statement cache
-#
-# Valid values are either "auto" (omitting the value) or a value greater 0.
-#
-# Note that specifying a too large value will result in long running GCs and possbily
-# out-of-memory errors. Keep the value at a small fraction of the heap.
-#
-# If you constantly see "prepared statements discarded in the last minute because
-# cache limit reached" messages, the first step is to investigate the root cause
-# of these messages and check whether prepared statements are used correctly -
-# i.e. use bind markers for variable parts.
-#
-# Do only change the default value, if you really have more prepared statements than
-# fit in the cache. In most cases it is not neccessary to change this value.
-# Constantly re-preparing statements is a performance penalty.
-#
-# Default value ("auto") is 1/256th of the heap or 10MiB, whichever is greater
-# Min unit: MiB
-prepared_statements_cache_size:
-
-# Maximum size of the key cache in memory.
-#
-# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the
-# minimum, sometimes more. The key cache is fairly tiny for the amount of
-# time it saves, so it's worthwhile to use it at large numbers.
-# The row cache saves even more time, but must contain the entire row,
-# so it is extremely space-intensive. It's best to only use the
-# row cache if you have hot rows or static rows.
-#
-# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.
-#
-# Default value is empty to make it "auto" (min(5% of Heap (in MiB), 100MiB)). Set to 0 to disable key cache.
-# Min unit: MiB
-key_cache_size:
-
-# Duration in seconds after which Cassandra should
-# save the key cache. Caches are saved to saved_caches_directory as
-# specified in this configuration file.
-#
-# Saved caches greatly improve cold-start speeds, and is relatively cheap in
-# terms of I/O for the key cache. Row cache saving is much more expensive and
-# has limited use.
-#
-# Default is 14400 or 4 hours.
-# Min unit: s
-key_cache_save_period: 4h
-
-# Number of keys from the key cache to save
-# Disabled by default, meaning all keys are going to be saved
-# key_cache_keys_to_save: 100
-
-# Row cache implementation class name. Available implementations:
-#
-# org.apache.cassandra.cache.OHCProvider
-#   Fully off-heap row cache implementation (default).
-#
-# org.apache.cassandra.cache.SerializingCacheProvider
-#   This is the row cache implementation availabile
-#   in previous releases of Cassandra.
-# row_cache_class_name: org.apache.cassandra.cache.OHCProvider
-
-# Maximum size of the row cache in memory.
-# Please note that OHC cache implementation requires some additional off-heap memory to manage
-# the map structures and some in-flight memory during operations before/after cache entries can be
-# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.
-# Do not specify more memory that the system can afford in the worst usual situation and leave some
-# headroom for OS block level cache. Do never allow your system to swap.
-#
-# Default value is 0, to disable row caching.
-# Min unit: MiB
-row_cache_size: 0MiB
-
-# Duration in seconds after which Cassandra should save the row cache.
-# Caches are saved to saved_caches_directory as specified in this configuration file.
-#
-# Saved caches greatly improve cold-start speeds, and is relatively cheap in
-# terms of I/O for the key cache. Row cache saving is much more expensive and
-# has limited use.
-#
-# Default is 0 to disable saving the row cache.
-# Min unit: s
-row_cache_save_period: 0s
-
-# Number of keys from the row cache to save.
-# Specify 0 (which is the default), meaning all keys are going to be saved
-# row_cache_keys_to_save: 100
-
-# Maximum size of the counter cache in memory.
-#
-# Counter cache helps to reduce counter locks' contention for hot counter cells.
-# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before
-# write entirely. With RF > 1 a counter cache hit will still help to reduce the duration
-# of the lock hold, helping with hot counter cell updates, but will not allow skipping
-# the read entirely. Only the local (clock, count) tuple of a counter cell is kept
-# in memory, not the whole counter, so it's relatively cheap.
-#
-# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.
-#
-# Default value is empty to make it "auto" (min(2.5% of Heap (in MiB), 50MiB)). Set to 0 to disable counter cache.
-# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.
-# Min unit: MiB
-counter_cache_size:
-
-# Duration in seconds after which Cassandra should
-# save the counter cache (keys only). Caches are saved to saved_caches_directory as
-# specified in this configuration file.
-#
-# Default is 7200 or 2 hours.
-# Min unit: s
-counter_cache_save_period: 7200s
-
-# Number of keys from the counter cache to save
-# Disabled by default, meaning all keys are going to be saved
-# counter_cache_keys_to_save: 100
-
-# saved caches
-# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
-# saved_caches_directory: /var/lib/cassandra/saved_caches
-
-# Number of seconds the server will wait for each cache (row, key, etc ...) to load while starting
-# the Cassandra process. Setting this to zero is equivalent to disabling all cache loading on startup
-# while still having the cache during runtime.
-# Min unit: s
-# cache_load_timeout: 30s
-
-# commitlog_sync may be either "periodic", "group", or "batch." 
-# 
-# When in batch mode, Cassandra won't ack writes until the commit log
-# has been flushed to disk.  Each incoming write will trigger the flush task.
-# commitlog_sync_batch_window_in_ms is a deprecated value. Previously it had
-# almost no value, and is being removed.
-#
-# commitlog_sync_batch_window_in_ms: 2
-#
-# group mode is similar to batch mode, where Cassandra will not ack writes
-# until the commit log has been flushed to disk. The difference is group
-# mode will wait up to commitlog_sync_group_window between flushes.
-#
-# Min unit: ms
-# commitlog_sync_group_window: 1000ms
-#
-# the default option is "periodic" where writes may be acked immediately
-# and the CommitLog is simply synced every commitlog_sync_period
-# milliseconds.
-commitlog_sync: periodic
-# Min unit: ms
-commitlog_sync_period: 10000ms
-
-# When in periodic commitlog mode, the number of milliseconds to block writes
-# while waiting for a slow disk flush to complete.
-# Min unit: ms
-# periodic_commitlog_sync_lag_block:
-
-# The size of the individual commitlog file segments.  A commitlog
-# segment may be archived, deleted, or recycled once all the data
-# in it (potentially from each columnfamily in the system) has been
-# flushed to sstables.
-#
-# The default size is 32, which is almost always fine, but if you are
-# archiving commitlog segments (see commitlog_archiving.properties),
-# then you probably want a finer granularity of archiving; 8 or 16 MB
-# is reasonable.
-# Max mutation size is also configurable via max_mutation_size setting in
-# cassandra.yaml. The default is half the size commitlog_segment_size in bytes.
-# This should be positive and less than 2048.
-#
-# NOTE: If max_mutation_size is set explicitly then commitlog_segment_size must
-# be set to at least twice the size of max_mutation_size
-#
-# Min unit: MiB
-commitlog_segment_size: 32MiB
-
-# Compression to apply to the commit log. If omitted, the commit log
-# will be written uncompressed.  LZ4, Snappy, and Deflate compressors
-# are supported.
-# commitlog_compression:
-#   - class_name: LZ4Compressor
-#     parameters:
-#         -
-
-# Compression to apply to SSTables as they flush for compressed tables.
-# Note that tables without compression enabled do not respect this flag.
-#
-# As high ratio compressors like LZ4HC, Zstd, and Deflate can potentially
-# block flushes for too long, the default is to flush with a known fast
-# compressor in those cases. Options are:
-#
-# none : Flush without compressing blocks but while still doing checksums.
-# fast : Flush with a fast compressor. If the table is already using a
-#        fast compressor that compressor is used.
-# table: Always flush with the same compressor that the table uses. This
-#        was the pre 4.0 behavior.
-#
-# flush_compression: fast
-
-# any class that implements the SeedProvider interface and has a
-# constructor that takes a Map<String, String> of parameters will do.
-seed_provider:
-  # Addresses of hosts that are deemed contact points.
-  # Cassandra nodes use this list of hosts to find each other and learn
-  # the topology of the ring.  You must change this if you are running
-  # multiple nodes!
-  - class_name: org.apache.cassandra.locator.SimpleSeedProvider
-    parameters:
-      # seeds is actually a comma-delimited list of addresses.
-      # Ex: "<ip1>,<ip2>,<ip3>"
-      - seeds: "10.10.10.14,10.10.10.15,10.10.10.16,10.10.10.17,10.10.10.18,10.10.10.19"
-
-# For workloads with more data than can fit in memory, Cassandra's
-# bottleneck will be reads that need to fetch data from
-# disk. "concurrent_reads" should be set to (16 * number_of_drives) in
-# order to allow the operations to enqueue low enough in the stack
-# that the OS and drives can reorder them. Same applies to
-# "concurrent_counter_writes", since counter writes read the current
-# values before incrementing and writing them back.
-#
-# On the other hand, since writes are almost never IO bound, the ideal
-# number of "concurrent_writes" is dependent on the number of cores in
-# your system; (8 * number_of_cores) is a good rule of thumb.
-
-ec_data_nodes: 2
-parity_nodes: 2
-compaction_threshold: 2
-concurrent_ec: 3
-
-concurrent_reads: 32
-concurrent_writes: 32
-concurrent_counter_writes: 32
-
-# For materialized view writes, as there is a read involved, so this should
-# be limited by the less of concurrent reads or concurrent writes.
-concurrent_materialized_view_writes: 32
-
-# Maximum memory to use for inter-node and client-server networking buffers.
-#
-# Defaults to the smaller of 1/16 of heap or 128MB. This pool is allocated off-heap,
-# so is in addition to the memory allocated for heap. The cache also has on-heap
-# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size
-# if the default 64k chunk size is used).
-# Memory is only allocated when needed.
-# Min unit: MiB
-# networking_cache_size: 128MiB
-
-# Enable the sstable chunk cache.  The chunk cache will store recently accessed
-# sections of the sstable in-memory as uncompressed buffers.
-# file_cache_enabled: false
-
-# Maximum memory to use for sstable chunk cache and buffer pooling.
-# 32MB of this are reserved for pooling buffers, the rest is used for chunk cache
-# that holds uncompressed sstable chunks.
-# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,
-# so is in addition to the memory allocated for heap. The cache also has on-heap
-# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size
-# if the default 64k chunk size is used).
-# Memory is only allocated when needed.
-# Min unit: MiB
-# file_cache_size: 512MiB
-
-# Flag indicating whether to allocate on or off heap when the sstable buffer
-# pool is exhausted, that is when it has exceeded the maximum memory
-# file_cache_size, beyond which it will not cache buffers but allocate on request.
-
-# buffer_pool_use_heap_if_exhausted: true
-
-# The strategy for optimizing disk read
-# Possible values are:
-# ssd (for solid state disks, the default)
-# spinning (for spinning disks)
-# disk_optimization_strategy: ssd
-
-# Total permitted memory to use for memtables. Cassandra will stop
-# accepting writes when the limit is exceeded until a flush completes,
-# and will trigger a flush based on memtable_cleanup_threshold
-# If omitted, Cassandra will set both to 1/4 the size of the heap.
-# Min unit: MiB
-memtable_heap_space: 4MiB
-# Min unit: MiB
-memtable_offheap_space: 4MiB
-
-# memtable_cleanup_threshold is deprecated. The default calculation
-# is the only reasonable choice. See the comments on  memtable_flush_writers
-# for more information.
-#
-# Ratio of occupied non-flushing memtable size to total permitted size
-# that will trigger a flush of the largest memtable. Larger mct will
-# mean larger flushes and hence less compaction, but also less concurrent
-# flush activity which can make it difficult to keep your disks fed
-# under heavy write load.
-#
-# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)
-# memtable_cleanup_threshold: 0.11
-
-# Specify the way Cassandra allocates and manages memtable memory.
-# Options are:
-#
-# heap_buffers
-#   on heap nio buffers
-#
-# offheap_buffers
-#   off heap (direct) nio buffers
-#
-# offheap_objects
-#    off heap objects
-memtable_allocation_type: heap_buffers
-
-# Limit memory usage for Merkle tree calculations during repairs. The default
-# is 1/16th of the available heap. The main tradeoff is that smaller trees
-# have less resolution, which can lead to over-streaming data. If you see heap
-# pressure during repairs, consider lowering this, but you cannot go below
-# one mebibyte. If you see lots of over-streaming, consider raising
-# this or using subrange repair.
-#
-# For more details see https://issues.apache.org/jira/browse/CASSANDRA-14096.
-#
-# Min unit: MiB
-# repair_session_space:
-
-# Total space to use for commit logs on disk.
-#
-# If space gets above this value, Cassandra will flush every dirty CF
-# in the oldest segment and remove it.  So a small total commitlog space
-# will tend to cause more flush activity on less-active columnfamilies.
-#
-# The default value is the smaller of 8192, and 1/4 of the total space
-# of the commitlog volume.
-#
-# commitlog_total_space: 8192MiB
-
-# This sets the number of memtable flush writer threads per disk
-# as well as the total number of memtables that can be flushed concurrently.
-# These are generally a combination of compute and IO bound.
-#
-# Memtable flushing is more CPU efficient than memtable ingest and a single thread
-# can keep up with the ingest rate of a whole server on a single fast disk
-# until it temporarily becomes IO bound under contention typically with compaction.
-# At that point you need multiple flush threads. At some point in the future
-# it may become CPU bound all the time.
-#
-# You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation
-# metric which should be 0, but will be non-zero if threads are blocked waiting on flushing
-# to free memory.
-#
-# memtable_flush_writers defaults to two for a single data directory.
-# This means that two  memtables can be flushed concurrently to the single data directory.
-# If you have multiple data directories the default is one memtable flushing at a time
-# but the flush will use a thread per data directory so you will get two or more writers.
-#
-# Two is generally enough to flush on a fast disk [array] mounted as a single data directory.
-# Adding more flush writers will result in smaller more frequent flushes that introduce more
-# compaction overhead.
-#
-# There is a direct tradeoff between number of memtables that can be flushed concurrently
-# and flush size and frequency. More is not better you just need enough flush writers
-# to never stall waiting for flushing to free memory.
-#
-# memtable_flush_writers: 2
-
-# Total space to use for change-data-capture logs on disk.
-#
-# If space gets above this value, Cassandra will throw WriteTimeoutException
-# on Mutations including tables with CDC enabled. A CDCCompactor is responsible
-# for parsing the raw CDC logs and deleting them when parsing is completed.
-#
-# The default value is the min of 4096 MiB and 1/8th of the total space
-# of the drive where cdc_raw_directory resides.
-# Min unit: MiB
-# cdc_total_space: 4096MiB
-
-# When we hit our cdc_raw limit and the CDCCompactor is either running behind
-# or experiencing backpressure, we check at the following interval to see if any
-# new space for cdc-tracked tables has been made available. Default to 250ms
-# Min unit: ms
-# cdc_free_space_check_interval: 250ms
-
-# A fixed memory pool size in MB for for SSTable index summaries. If left
-# empty, this will default to 5% of the heap size. If the memory usage of
-# all index summaries exceeds this limit, SSTables with low read rates will
-# shrink their index summaries in order to meet this limit.  However, this
-# is a best-effort process. In extreme conditions Cassandra may need to use
-# more than this amount of memory.
-# Min unit: KiB
-index_summary_capacity:
-
-# How frequently index summaries should be resampled.  This is done
-# periodically to redistribute memory from the fixed-size pool to sstables
-# proportional their recent read rates.  Setting to null value will disable this
-# process, leaving existing index summaries at their current sampling level.
-# Min unit: m
-index_summary_resize_interval: 60m
-
-# Whether to, when doing sequential writing, fsync() at intervals in
-# order to force the operating system to flush the dirty
-# buffers. Enable this to avoid sudden dirty buffer flushing from
-# impacting read latencies. Almost always a good idea on SSDs; not
-# necessarily on platters.
-trickle_fsync: false
-# Min unit: KiB
-trickle_fsync_interval: 10240KiB
-
-# TCP port, for commands and data
-# For security reasons, you should not expose this port to the internet.  Firewall it if needed.
-storage_port: 7000
-
-# SSL port, for legacy encrypted communication. This property is unused unless enabled in
-# server_encryption_options (see below). As of cassandra 4.0, this property is deprecated
-# as a single port can be used for either/both secure and insecure connections.
-# For security reasons, you should not expose this port to the internet. Firewall it if needed.
-ssl_storage_port: 7001
-
-# Address or interface to bind to and tell other Cassandra nodes to connect to.
-# You _must_ change this if you want multiple nodes to be able to communicate!
-#
-# Set listen_address OR listen_interface, not both.
-#
-# Leaving it blank leaves it up to InetAddress.getLocalHost(). This
-# will always do the Right Thing _if_ the node is properly configured
-# (hostname, name resolution, etc), and the Right Thing is to use the
-# address associated with the hostname (it might not be). If unresolvable
-# it will fall back to InetAddress.getLoopbackAddress(), which is wrong for production systems.
-#
-# Setting listen_address to 0.0.0.0 is always wrong.
-#
-listen_address: 10.10.10.14
-
-# Set listen_address OR listen_interface, not both. Interfaces must correspond
-# to a single address, IP aliasing is not supported.
-# listen_interface: eth0
-
-# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address
-# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4
-# address will be used. If true the first ipv6 address will be used. Defaults to false preferring
-# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.
-# listen_interface_prefer_ipv6: false
-
-# Address to broadcast to other Cassandra nodes
-# Leaving this blank will set it to the same value as listen_address
-# broadcast_address: 1.2.3.4
-
-# When using multiple physical network interfaces, set this
-# to true to listen on broadcast_address in addition to
-# the listen_address, allowing nodes to communicate in both
-# interfaces.
-# Ignore this property if the network configuration automatically
-# routes  between the public and private networks such as EC2.
-# listen_on_broadcast_address: false
-
-# Internode authentication backend, implementing IInternodeAuthenticator;
-# used to allow/disallow connections from peer nodes.
-# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator
-
-# Whether to start the native transport server.
-# The address on which the native transport is bound is defined by rpc_address.
-start_native_transport: true
-# port for the CQL native transport to listen for clients on
-# For security reasons, you should not expose this port to the internet.  Firewall it if needed.
-native_transport_port: 9042
-# Enabling native transport encryption in client_encryption_options allows you to either use
-# encryption for the standard port or to use a dedicated, additional port along with the unencrypted
-# standard native_transport_port.
-# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption
-# for native_transport_port. Setting native_transport_port_ssl to a different value
-# from native_transport_port will use encryption for native_transport_port_ssl while
-# keeping native_transport_port unencrypted.
-# native_transport_port_ssl: 9142
-# The maximum threads for handling requests (note that idle threads are stopped
-# after 30 seconds so there is not corresponding minimum setting).
-# native_transport_max_threads: 128
-#
-# The maximum size of allowed frame. Frame (requests) larger than this will
-# be rejected as invalid. The default is 16MiB. If you're changing this parameter,
-# you may want to adjust max_value_size accordingly. This should be positive and less than 2048.
-# Min unit: MiB
-# native_transport_max_frame_size: 16MiB
-
-# The maximum number of concurrent client connections.
-# The default is -1, which means unlimited.
-# native_transport_max_concurrent_connections: -1
-
-# The maximum number of concurrent client connections per source ip.
-# The default is -1, which means unlimited.
-# native_transport_max_concurrent_connections_per_ip: -1
-
-# Controls whether Cassandra honors older, yet currently supported, protocol versions.
-# The default is true, which means all supported protocols will be honored.
-native_transport_allow_older_protocols: true
-
-# Controls when idle client connections are closed. Idle connections are ones that had neither reads
-# nor writes for a time period.
-#
-# Clients may implement heartbeats by sending OPTIONS native protocol message after a timeout, which
-# will reset idle timeout timer on the server side. To close idle client connections, corresponding
-# values for heartbeat intervals have to be set on the client side.
-#
-# Idle connection timeouts are disabled by default.
-# Min unit: ms
-# native_transport_idle_timeout: 60000ms
-
-# When enabled, limits the number of native transport requests dispatched for processing per second.
-# Behavior once the limit has been breached depends on the value of THROW_ON_OVERLOAD specified in
-# the STARTUP message sent by the client during connection establishment. (See section "4.1.1. STARTUP"
-# in "CQL BINARY PROTOCOL v5".) With the THROW_ON_OVERLOAD flag enabled, messages that breach the limit
-# are dropped, and an OverloadedException is thrown for the client to handle. When the flag is not
-# enabled, the server will stop consuming messages from the channel/socket, putting backpressure on
-# the client while already dispatched messages are processed.
-# native_transport_rate_limiting_enabled: false
-# native_transport_max_requests_per_second: 1000000
-
-# The address or interface to bind the native transport server to.
-#
-# Set rpc_address OR rpc_interface, not both.
-#
-# Leaving rpc_address blank has the same effect as on listen_address
-# (i.e. it will be based on the configured hostname of the node).
-#
-# Note that unlike listen_address, you can specify 0.0.0.0, but you must also
-# set broadcast_rpc_address to a value other than 0.0.0.0.
-#
-# For security reasons, you should not expose this port to the internet.  Firewall it if needed.
-rpc_address: 10.10.10.14
-
-# Set rpc_address OR rpc_interface, not both. Interfaces must correspond
-# to a single address, IP aliasing is not supported.
-# rpc_interface: eth1
-
-# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address
-# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4
-# address will be used. If true the first ipv6 address will be used. Defaults to false preferring
-# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.
-# rpc_interface_prefer_ipv6: false
-
-# RPC address to broadcast to drivers and other Cassandra nodes. This cannot
-# be set to 0.0.0.0. If left blank, this will be set to the value of
-# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must
-# be set.
-# broadcast_rpc_address: 1.2.3.4
-
-# enable or disable keepalive on rpc/native connections
-rpc_keepalive: true
-
-# Uncomment to set socket buffer size for internode communication
-# Note that when setting this, the buffer size is limited by net.core.wmem_max
-# and when not setting it it is defined by net.ipv4.tcp_wmem
-# See also:
-# /proc/sys/net/core/wmem_max
-# /proc/sys/net/core/rmem_max
-# /proc/sys/net/ipv4/tcp_wmem
-# /proc/sys/net/ipv4/tcp_wmem
-# and 'man tcp'
-# Min unit: B
-# internode_socket_send_buffer_size:
-
-# Uncomment to set socket buffer size for internode communication
-# Note that when setting this, the buffer size is limited by net.core.wmem_max
-# and when not setting it it is defined by net.ipv4.tcp_wmem
-# Min unit: B
-# internode_socket_receive_buffer_size:
-
-# Set to true to have Cassandra create a hard link to each sstable
-# flushed or streamed locally in a backups/ subdirectory of the
-# keyspace data.  Removing these links is the operator's
-# responsibility.
-incremental_backups: false
-
-# Whether or not to take a snapshot before each compaction.  Be
-# careful using this option, since Cassandra won't clean up the
-# snapshots for you.  Mostly useful if you're paranoid when there
-# is a data format change.
-snapshot_before_compaction: false
-
-# Whether or not a snapshot is taken of the data before keyspace truncation
-# or dropping of column families. The STRONGLY advised default of true 
-# should be used to provide data safety. If you set this flag to false, you will
-# lose data on truncation or drop.
-auto_snapshot: true
-
-# Adds a time-to-live (TTL) to auto snapshots generated by table
-# truncation or drop (when enabled).
-# After the TTL is elapsed, the snapshot is automatically cleared.
-# By default, auto snapshots *do not* have TTL, uncomment the property below
-# to enable TTL on auto snapshots.
-# Accepted units: d (days), h (hours) or m (minutes)
-# auto_snapshot_ttl: 30d
-
-# The act of creating or clearing a snapshot involves creating or removing
-# potentially tens of thousands of links, which can cause significant performance
-# impact, especially on consumer grade SSDs. A non-zero value here can
-# be used to throttle these links to avoid negative performance impact of
-# taking and clearing snapshots
-snapshot_links_per_second: 0
-
-# Granularity of the collation index of rows within a partition.
-# Increase if your rows are large, or if you have a very large
-# number of rows per partition.  The competing goals are these:
-#
-# - a smaller granularity means more index entries are generated
-#   and looking up rows withing the partition by collation column
-#   is faster
-# - but, Cassandra will keep the collation index in memory for hot
-#   rows (as part of the key cache), so a larger granularity means
-#   you can cache more hot rows
-# Min unit: KiB
-column_index_size: 64KiB
-
-# Per sstable indexed key cache entries (the collation index in memory
-# mentioned above) exceeding this size will not be held on heap.
-# This means that only partition information is held on heap and the
-# index entries are read from disk.
-#
-# Note that this size refers to the size of the
-# serialized index information and not the size of the partition.
-# Min unit: KiB
-column_index_cache_size: 2KiB
-
-# Number of simultaneous compactions to allow, NOT including
-# validation "compactions" for anti-entropy repair.  Simultaneous
-# compactions can help preserve read performance in a mixed read/write
-# workload, by mitigating the tendency of small sstables to accumulate
-# during a single long running compactions. The default is usually
-# fine and if you experience problems with compaction running too
-# slowly or too fast, you should look at
-# compaction_throughput first.
-#
-# concurrent_compactors defaults to the smaller of (number of disks,
-# number of cores), with a minimum of 2 and a maximum of 8.
-# 
-# If your data directories are backed by SSD, you should increase this
-# to the number of cores.
-# concurrent_compactors: 1
-
-# Number of simultaneous repair validations to allow. If not set or set to
-# a value less than 1, it defaults to the value of concurrent_compactors.
-# To set a value greeater than concurrent_compactors at startup, the system
-# property cassandra.allow_unlimited_concurrent_validations must be set to
-# true. To dynamically resize to a value > concurrent_compactors on a running
-# node, first call the bypassConcurrentValidatorsLimit method on the
-# org.apache.cassandra.db:type=StorageService mbean
-# concurrent_validations: 0
-
-# Number of simultaneous materialized view builder tasks to allow.
-concurrent_materialized_view_builders: 1
-
-# Throttles compaction to the given total throughput across the entire
-# system. The faster you insert data, the faster you need to compact in
-# order to keep the sstable count down, but in general, setting this to
-# 16 to 32 times the rate you are inserting data is more than sufficient.
-# Setting this to 0 disables throttling. Note that this accounts for all types
-# of compaction, including validation compaction (building Merkle trees
-# for repairs).
-compaction_throughput: 64MiB/s
-
-# When compacting, the replacement sstable(s) can be opened before they
-# are completely written, and used in place of the prior sstables for
-# any range that has been written. This helps to smoothly transfer reads 
-# between the sstables, reducing page cache churn and keeping hot rows hot
-# Set sstable_preemptive_open_interval to null for disabled which is equivalent to
-# sstable_preemptive_open_interval_in_mb being negative
-# Min unit: MiB
-sstable_preemptive_open_interval: 50MiB
-
-# Starting from 4.1 sstables support UUID based generation identifiers. They are disabled by default
-# because once enabled, there is no easy way to downgrade. When the node is restarted with this option
-# set to true, each newly created sstable will have a UUID based generation identifier and such files are
-# not readable by previous Cassandra versions. At some point, this option will become true by default
-# and eventually get removed from the configuration.
-uuid_sstable_identifiers_enabled: false
-
-# When enabled, permits Cassandra to zero-copy stream entire eligible
-# SSTables between nodes, including every component.
-# This speeds up the network transfer significantly subject to
-# throttling specified by entire_sstable_stream_throughput_outbound,
-# and entire_sstable_inter_dc_stream_throughput_outbound
-# for inter-DC transfers.
-# Enabling this will reduce the GC pressure on sending and receiving node.
-# When unset, the default is enabled. While this feature tries to keep the
-# disks balanced, it cannot guarantee it. This feature will be automatically
-# disabled if internode encryption is enabled.
-# stream_entire_sstables: true
-
-# Throttles entire SSTable outbound streaming file transfers on
-# this node to the given total throughput in Mbps.
-# Setting this value to 0 it disables throttling.
-# When unset, the default is 200 Mbps or 24 MiB/s.
-# entire_sstable_stream_throughput_outbound: 24MiB/s
-
-# Throttles entire SSTable file streaming between datacenters.
-# Setting this value to 0 disables throttling for entire SSTable inter-DC file streaming.
-# When unset, the default is 200 Mbps or 24 MiB/s.
-# entire_sstable_inter_dc_stream_throughput_outbound: 24MiB/s
-
-# Throttles all outbound streaming file transfers on this node to the
-# given total throughput in Mbps. This is necessary because Cassandra does
-# mostly sequential IO when streaming data during bootstrap or repair, which
-# can lead to saturating the network connection and degrading rpc performance.
-# When unset, the default is 200 Mbps or 24 MiB/s.
-# stream_throughput_outbound: 24MiB/s
-
-# Throttles all streaming file transfer between the datacenters,
-# this setting allows users to throttle inter dc stream throughput in addition
-# to throttling all network stream traffic as configured with
-# stream_throughput_outbound_megabits_per_sec
-# When unset, the default is 200 Mbps or 24 MiB/s.
-# inter_dc_stream_throughput_outbound: 24MiB/s
-
-# Server side timeouts for requests. The server will return a timeout exception
-# to the client if it can't complete an operation within the corresponding
-# timeout. Those settings are a protection against:
-#   1) having client wait on an operation that might never terminate due to some
-#      failures.
-#   2) operations that use too much CPU/read too much data (leading to memory build
-#      up) by putting a limit to how long an operation will execute.
-# For this reason, you should avoid putting these settings too high. In other words,
-# if you are timing out requests because of underlying resource constraints then
-# increasing the timeout will just cause more problems. Of course putting them too
-# low is equally ill-advised since clients could get timeouts even for successful
-# operations just because the timeout setting is too tight.
-
-# How long the coordinator should wait for read operations to complete.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-read_request_timeout: 5000ms
-# How long the coordinator should wait for seq or index scans to complete.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-range_request_timeout: 10000ms
-# How long the coordinator should wait for writes to complete.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-write_request_timeout: 2000ms
-# How long the coordinator should wait for counter writes to complete.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-counter_write_request_timeout: 5000ms
-# How long a coordinator should continue to retry a CAS operation
-# that contends with other proposals for the same row.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-cas_contention_timeout: 1000ms
-# How long the coordinator should wait for truncates to complete
-# (This can be much longer, because unless auto_snapshot is disabled
-# we need to flush first so we can snapshot before removing the data.)
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-truncate_request_timeout: 60000ms
-# The default timeout for other, miscellaneous operations.
-# Lowest acceptable value is 10 ms.
-# Min unit: ms
-request_timeout: 10000ms
-
-# Defensive settings for protecting Cassandra from true network partitions.
-# See (CASSANDRA-14358) for details.
-#
-# The amount of time to wait for internode tcp connections to establish.
-# Min unit: ms
-# internode_tcp_connect_timeout: 2000ms
-#
-# The amount of time unacknowledged data is allowed on a connection before we throw out the connection
-# Note this is only supported on Linux + epoll, and it appears to behave oddly above a setting of 30000
-# (it takes much longer than 30s) as of Linux 4.12. If you want something that high set this to 0
-# which picks up the OS default and configure the net.ipv4.tcp_retries2 sysctl to be ~8.
-# Min unit: ms
-# internode_tcp_user_timeout: 30000ms
-
-# The amount of time unacknowledged data is allowed on a streaming connection.
-# The default is 5 minutes. Increase it or set it to 0 in order to increase the timeout.
-# Min unit: ms
-# internode_streaming_tcp_user_timeout: 300000ms
-
-# Global, per-endpoint and per-connection limits imposed on messages queued for delivery to other nodes
-# and waiting to be processed on arrival from other nodes in the cluster.  These limits are applied to the on-wire
-# size of the message being sent or received.
-#
-# The basic per-link limit is consumed in isolation before any endpoint or global limit is imposed.
-# Each node-pair has three links: urgent, small and large.  So any given node may have a maximum of
-# N*3*(internode_application_send_queue_capacity+internode_application_receive_queue_capacity)
-# messages queued without any coordination between them although in practice, with token-aware routing, only RF*tokens
-# nodes should need to communicate with significant bandwidth.
-#
-# The per-endpoint limit is imposed on all messages exceeding the per-link limit, simultaneously with the global limit,
-# on all links to or from a single node in the cluster.
-# The global limit is imposed on all messages exceeding the per-link limit, simultaneously with the per-endpoint limit,
-# on all links to or from any node in the cluster.
-#
-# Min unit: B
-# internode_application_send_queue_capacity: 4MiB
-# internode_application_send_queue_reserve_endpoint_capacity: 128MiB
-# internode_application_send_queue_reserve_global_capacity: 512MiB
-# internode_application_receive_queue_capacity: 4MiB
-# internode_application_receive_queue_reserve_endpoint_capacity: 128MiB
-# internode_application_receive_queue_reserve_global_capacity: 512MiB
-
-
-# How long before a node logs slow queries. Select queries that take longer than
-# this timeout to execute, will generate an aggregated log message, so that slow queries
-# can be identified. Set this value to zero to disable slow query logging.
-# Min unit: ms
-slow_query_log_timeout: 500ms
-
-# Enable operation timeout information exchange between nodes to accurately
-# measure request timeouts.  If disabled, replicas will assume that requests
-# were forwarded to them instantly by the coordinator, which means that
-# under overload conditions we will waste that much extra time processing 
-# already-timed-out requests.
-#
-# Warning: It is generally assumed that users have setup NTP on their clusters, and that clocks are modestly in sync, 
-# since this is a requirement for general correctness of last write wins.
-# internode_timeout: true
-
-# Set period for idle state control messages for earlier detection of failed streams
-# This node will send a keep-alive message periodically on the streaming's control channel.
-# This ensures that any eventual SocketTimeoutException will occur within 2 keep-alive cycles
-# If the node cannot send, or timeouts sending, the keep-alive message on the netty control channel
-# the stream session is closed.
-# Default value is 300s (5 minutes), which means stalled streams
-# are detected within 10 minutes
-# Specify 0 to disable.
-# Min unit: s
-# streaming_keep_alive_period: 300s
-
-# Limit number of connections per host for streaming
-# Increase this when you notice that joins are CPU-bound rather that network
-# bound (for example a few nodes with big files).
-# streaming_connections_per_host: 1
-
-# Allows denying configurable access (rw/rr) to operations on configured ks, table, and partitions, intended for use by
-# operators to manage cluster health vs application access. See CASSANDRA-12106 and CEP-13 for more details.
-# partition_denylist_enabled: false
-
-# denylist_writes_enabled: true
-# denylist_reads_enabled: true
-# denylist_range_reads_enabled: true
-
-# The interval at which keys in the cache for denylisting will "expire" and async refresh from the backing DB.
-# Note: this serves only as a fail-safe, as the usage pattern is expected to be "mutate state, refresh cache" on any
-# changes to the underlying denylist entries. See documentation for details.
-# Min unit: s
-# denylist_refresh: 600s
-
-# In the event of errors on attempting to load the denylist cache, retry on this interval.
-# Min unit: s
-# denylist_initial_load_retry: 5s
-
-# We cap the number of denylisted keys allowed per table to keep things from growing unbounded. Nodes will warn above
-# this limit while allowing new denylisted keys to be inserted. Denied keys are loaded in natural query / clustering
-# ordering by partition key in case of overflow.
-# denylist_max_keys_per_table: 1000
-
-# We cap the total number of denylisted keys allowed in the cluster to keep things from growing unbounded.
-# Nodes will warn on initial cache load that there are too many keys and be direct the operator to trim down excess
-# entries to within the configured limits.
-# denylist_max_keys_total: 10000
-
-# Since the denylist in many ways serves to protect the health of the cluster from partitions operators have identified
-# as being in a bad state, we usually want more robustness than just CL.ONE on operations to/from these tables to
-# ensure that these safeguards are in place. That said, we allow users to configure this if they're so inclined.
-# denylist_consistency_level: QUORUM
-
-# phi value that must be reached for a host to be marked down.
-# most users should never need to adjust this.
-# phi_convict_threshold: 8
-
-# endpoint_snitch -- Set this to a class that implements
-# IEndpointSnitch.  The snitch has two functions:
-#
-# - it teaches Cassandra enough about your network topology to route
-#   requests efficiently
-# - it allows Cassandra to spread replicas around your cluster to avoid
-#   correlated failures. It does this by grouping machines into
-#   "datacenters" and "racks."  Cassandra will do its best not to have
-#   more than one replica on the same "rack" (which may not actually
-#   be a physical location)
-#
-# CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH
-# ONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.
-# This means that if you start with the default SimpleSnitch, which
-# locates every node on "rack1" in "datacenter1", your only options
-# if you need to add another datacenter are GossipingPropertyFileSnitch
-# (and the older PFS).  From there, if you want to migrate to an
-# incompatible snitch like Ec2Snitch you can do it by adding new nodes
-# under Ec2Snitch (which will locate them in a new "datacenter") and
-# decommissioning the old ones.
-#
-# Out of the box, Cassandra provides:
-#
-# SimpleSnitch:
-#    Treats Strategy order as proximity. This can improve cache
-#    locality when disabling read repair.  Only appropriate for
-#    single-datacenter deployments.
-#
-# GossipingPropertyFileSnitch
-#    This should be your go-to snitch for production use.  The rack
-#    and datacenter for the local node are defined in
-#    cassandra-rackdc.properties and propagated to other nodes via
-#    gossip.  If cassandra-topology.properties exists, it is used as a
-#    fallback, allowing migration from the PropertyFileSnitch.
-#
-# PropertyFileSnitch:
-#    Proximity is determined by rack and data center, which are
-#    explicitly configured in cassandra-topology.properties.
-#
-# Ec2Snitch:
-#    Appropriate for EC2 deployments in a single Region. Loads Region
-#    and Availability Zone information from the EC2 API. The Region is
-#    treated as the datacenter, and the Availability Zone as the rack.
-#    Only private IPs are used, so this will not work across multiple
-#    Regions.
-#
-# Ec2MultiRegionSnitch:
-#    Uses public IPs as broadcast_address to allow cross-region
-#    connectivity.  (Thus, you should set seed addresses to the public
-#    IP as well.) You will need to open the storage_port or
-#    ssl_storage_port on the public IP firewall.  (For intra-Region
-#    traffic, Cassandra will switch to the private IP after
-#    establishing a connection.)
-#
-# RackInferringSnitch:
-#    Proximity is determined by rack and data center, which are
-#    assumed to correspond to the 3rd and 2nd octet of each node's IP
-#    address, respectively.  Unless this happens to match your
-#    deployment conventions, this is best used as an example of
-#    writing a custom Snitch class and is provided in that spirit.
-#
-# You can use a custom Snitch by setting this to the full class name
-# of the snitch, which will be assumed to be on your classpath.
-endpoint_snitch: SimpleSnitch
-
-# controls how often to perform the more expensive part of host score
-# calculation
-# Min unit: ms
-dynamic_snitch_update_interval: 100ms
-# controls how often to reset all host scores, allowing a bad host to
-# possibly recover
-# Min unit: ms
-dynamic_snitch_reset_interval: 600000ms
-# if set greater than zero, this will allow
-# 'pinning' of replicas to hosts in order to increase cache capacity.
-# The badness threshold will control how much worse the pinned host has to be
-# before the dynamic snitch will prefer other replicas over it.  This is
-# expressed as a double which represents a percentage.  Thus, a value of
-# 0.2 means Cassandra would continue to prefer the static snitch values
-# until the pinned host was 20% worse than the fastest.
-dynamic_snitch_badness_threshold: 1.0
-
-# Configure server-to-server internode encryption
-#
-# JVM and netty defaults for supported SSL socket protocols and cipher suites can
-# be replaced using custom encryption options. This is not recommended
-# unless you have policies in place that dictate certain settings, or
-# need to disable vulnerable ciphers or protocols in case the JVM cannot
-# be updated.
-#
-# FIPS compliant settings can be configured at JVM level and should not
-# involve changing encryption settings here:
-# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html
-#
-# **NOTE** this default configuration is an insecure configuration. If you need to
-# enable server-to-server encryption generate server keystores (and truststores for mutual
-# authentication) per:
-# http://download.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore
-# Then perform the following configuration changes:
-#
-# Step 1: Set internode_encryption=<dc|rack|all> and explicitly set optional=true. Restart all nodes
-#
-# Step 2: Set optional=false (or remove it) and if you generated truststores and want to use mutual
-# auth set require_client_auth=true. Restart all nodes
-server_encryption_options:
-  # On outbound connections, determine which type of peers to securely connect to.
-  #   The available options are :
-  #     none : Do not encrypt outgoing connections
-  #     dc   : Encrypt connections to peers in other datacenters but not within datacenters
-  #     rack : Encrypt connections to peers in other racks but not within racks
-  #     all  : Always use encrypted connections
-  internode_encryption: none
-  # When set to true, encrypted and unencrypted connections are allowed on the storage_port
-  # This should _only be true_ while in unencrypted or transitional operation
-  # optional defaults to true if internode_encryption is none
-  # optional: true
-  # If enabled, will open up an encrypted listening socket on ssl_storage_port. Should only be used
-  # during upgrade to 4.0; otherwise, set to false.
-  legacy_ssl_storage_port_enabled: false
-  # Set to a valid keystore if internode_encryption is dc, rack or all
-  keystore: conf/.keystore
-  keystore_password: cassandra
-  # Verify peer server certificates
-  require_client_auth: false
-  # Set to a valid trustore if require_client_auth is true
-  truststore: conf/.truststore
-  truststore_password: cassandra
-  # Verify that the host name in the certificate matches the connected host
-  require_endpoint_verification: false
-  # More advanced defaults:
-  # protocol: TLS
-  # store_type: JKS
-  # cipher_suites: [
-  #   TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
-  #   TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,
-  #   TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA,
-  #   TLS_RSA_WITH_AES_256_CBC_SHA
-  # ]
-
-# Configure client-to-server encryption.
-#
-# **NOTE** this default configuration is an insecure configuration. If you need to
-# enable client-to-server encryption generate server keystores (and truststores for mutual
-# authentication) per:
-# http://download.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore
-# Then perform the following configuration changes:
-#
-# Step 1: Set enabled=true and explicitly set optional=true. Restart all nodes
-#
-# Step 2: Set optional=false (or remove it) and if you generated truststores and want to use mutual
-# auth set require_client_auth=true. Restart all nodes
-client_encryption_options:
-  # Enable client-to-server encryption
-  enabled: false
-  # When set to true, encrypted and unencrypted connections are allowed on the native_transport_port
-  # This should _only be true_ while in unencrypted or transitional operation
-  # optional defaults to true when enabled is false, and false when enabled is true.
-  # optional: true
-  # Set keystore and keystore_password to valid keystores if enabled is true
-  keystore: conf/.keystore
-  keystore_password: cassandra
-  # Verify client certificates
-  require_client_auth: false
-  # Set trustore and truststore_password if require_client_auth is true
-  # truststore: conf/.truststore
-  # truststore_password: cassandra
-  # More advanced defaults:
-  # protocol: TLS
-  # store_type: JKS
-  # cipher_suites: [
-  #   TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
-  #   TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,
-  #   TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA,
-  #   TLS_RSA_WITH_AES_256_CBC_SHA
-  # ]
-
-# internode_compression controls whether traffic between nodes is
-# compressed.
-# Can be:
-#
-# all
-#   all traffic is compressed
-#
-# dc
-#   traffic between different datacenters is compressed
-#
-# none
-#   nothing is compressed.
-internode_compression: dc
-
-# Enable or disable tcp_nodelay for inter-dc communication.
-# Disabling it will result in larger (but fewer) network packets being sent,
-# reducing overhead from the TCP protocol itself, at the cost of increasing
-# latency if you block for cross-datacenter responses.
-inter_dc_tcp_nodelay: false
-
-# TTL for different trace types used during logging of the repair process.
-# Min unit: s
-trace_type_query_ttl: 1d
-# Min unit: s
-trace_type_repair_ttl: 7d
-
-# If unset, all GC Pauses greater than gc_log_threshold will log at
-# INFO level
-# UDFs (user defined functions) are disabled by default.
-# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.
-user_defined_functions_enabled: false
-
-# Enables scripted UDFs (JavaScript UDFs).
-# Java UDFs are always enabled, if user_defined_functions_enabled is true.
-# Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.
-# This option has no effect, if user_defined_functions_enabled is false.
-scripted_user_defined_functions_enabled: false
-
-# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from
-# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by
-# the "key_alias" is the only key that will be used for encrypt opertaions; previously used keys
-# can still (and should!) be in the keystore and will be used on decrypt operations
-# (to handle the case of key rotation).
-#
-# It is strongly recommended to download and install Java Cryptography Extension (JCE)
-# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.
-# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)
-#
-# Currently, only the following file types are supported for transparent data encryption, although
-# more are coming in future cassandra releases: commitlog, hints
-transparent_data_encryption_options:
-  enabled: false
-  chunk_length_kb: 64
-  cipher: AES/CBC/PKCS5Padding
-  key_alias: testing:1
-  # CBC IV length for AES needs to be 16 bytes (which is also the default size)
-  # iv_length: 16
-  key_provider:
-    - class_name: org.apache.cassandra.security.JKSKeyProvider
-      parameters:
-        - keystore: conf/.keystore
-          keystore_password: cassandra
-          store_type: JCEKS
-          key_password: cassandra
-
-
-#####################
-# SAFETY THRESHOLDS #
-#####################
-
-# When executing a scan, within or across a partition, we need to keep the
-# tombstones seen in memory so we can return them to the coordinator, which
-# will use them to make sure other replicas also know about the deleted rows.
-# With workloads that generate a lot of tombstones, this can cause performance
-# problems and even exaust the server heap.
-# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)
-# Adjust the thresholds here if you understand the dangers and want to
-# scan more tombstones anyway.  These thresholds may also be adjusted at runtime
-# using the StorageService mbean.
-tombstone_warn_threshold: 1000
-tombstone_failure_threshold: 100000
-
-# Filtering and secondary index queries at read consistency levels above ONE/LOCAL_ONE use a
-# mechanism called replica filtering protection to ensure that results from stale replicas do
-# not violate consistency. (See CASSANDRA-8272 and CASSANDRA-15907 for more details.) This
-# mechanism materializes replica results by partition on-heap at the coordinator. The more possibly
-# stale results returned by the replicas, the more rows materialized during the query.
-replica_filtering_protection:
-    # These thresholds exist to limit the damage severely out-of-date replicas can cause during these
-    # queries. They limit the number of rows from all replicas individual index and filtering queries
-    # can materialize on-heap to return correct results at the desired read consistency level.
-    #
-    # "cached_replica_rows_warn_threshold" is the per-query threshold at which a warning will be logged.
-    # "cached_replica_rows_fail_threshold" is the per-query threshold at which the query will fail.
-    #
-    # These thresholds may also be adjusted at runtime using the StorageService mbean.
-    #
-    # If the failure threshold is breached, it is likely that either the current page/fetch size
-    # is too large or one or more replicas is severely out-of-sync and in need of repair.
-    cached_rows_warn_threshold: 2000
-    cached_rows_fail_threshold: 32000
-
-# Log WARN on any multiple-partition batch size exceeding this value. 5KiB per batch by default.
-# Caution should be taken on increasing the size of this threshold as it can lead to node instability.
-# Min unit: KiB
-batch_size_warn_threshold: 5KiB
-
-# Fail any multiple-partition batch exceeding this value. 50KiB (10x warn threshold) by default.
-# Min unit: KiB
-batch_size_fail_threshold: 50KiB
-
-# Log WARN on any batches not of type LOGGED than span across more partitions than this limit
-unlogged_batch_across_partitions_warn_threshold: 10
-
-# Log a warning when compacting partitions larger than this value
-compaction_large_partition_warning_threshold: 100MiB
-
-# Log a warning when writing more tombstones than this value to a partition
-compaction_tombstone_warning_threshold: 100000
-
-# GC Pauses greater than 200 ms will be logged at INFO level
-# This threshold can be adjusted to minimize logging if necessary
-# Min unit: ms
-# gc_log_threshold: 200ms
-
-# GC Pauses greater than gc_warn_threshold will be logged at WARN level
-# Adjust the threshold based on your application throughput requirement. Setting to 0
-# will deactivate the feature.
-# Min unit: ms
-# gc_warn_threshold: 1000ms
-
-# Maximum size of any value in SSTables. Safety measure to detect SSTable corruption
-# early. Any value size larger than this threshold will result into marking an SSTable
-# as corrupted. This should be positive and less than 2GiB.
-# Min unit: MiB
-# max_value_size: 256MiB
-
-# ** Impact on keyspace creation **
-# If replication factor is not mentioned as part of keyspace creation, default_keyspace_rf would apply.
-# Changing this configuration would only take effect for keyspaces created after the change, but does not impact
-# existing keyspaces created prior to the change.
-# ** Impact on keyspace alter **
-# When altering a keyspace from NetworkTopologyStrategy to SimpleStrategy, default_keyspace_rf is applied if rf is not
-# explicitly mentioned.
-# ** Impact on system keyspaces **
-# This would also apply for any system keyspaces that need replication factor.
-# A further note about system keyspaces - system_traces and system_distributed keyspaces take RF of 2 or default,
-# whichever is higher, and system_auth keyspace takes RF of 1 or default, whichever is higher.
-# Suggested value for use in production: 3
-# default_keyspace_rf: 1
-
-# Track a metric per keyspace indicating whether replication achieved the ideal consistency
-# level for writes without timing out. This is different from the consistency level requested by
-# each write which may be lower in order to facilitate availability.
-# ideal_consistency_level: EACH_QUORUM
-
-# Automatically upgrade sstables after upgrade - if there is no ordinary compaction to do, the
-# oldest non-upgraded sstable will get upgraded to the latest version
-# automatic_sstable_upgrade: false
-# Limit the number of concurrent sstable upgrades
-# max_concurrent_automatic_sstable_upgrades: 1
-
-# Audit logging - Logs every incoming CQL command request, authentication to a node. See the docs
-# on audit_logging for full details about the various configuration options.
-audit_logging_options:
-  enabled: false
-  logger:
-    - class_name: BinAuditLogger
-  # audit_logs_dir:
-  # included_keyspaces:
-  # excluded_keyspaces: system, system_schema, system_virtual_schema
-  # included_categories:
-  # excluded_categories:
-  # included_users:
-  # excluded_users:
-  # roll_cycle: HOURLY
-  # block: true
-  # max_queue_weight: 268435456 # 256 MiB
-  # max_log_size: 17179869184 # 16 GiB
-  ## archive command is "/path/to/script.sh %path" where %path is replaced with the file being rolled:
-  # archive_command:
-  # max_archive_retries: 10
-
-
-# default options for full query logging - these can be overridden from command line when executing
-# nodetool enablefullquerylog
-# full_query_logging_options:
-  # log_dir:
-  # roll_cycle: HOURLY
-  # block: true
-  # max_queue_weight: 268435456 # 256 MiB
-  # max_log_size: 17179869184 # 16 GiB
-  ## archive command is "/path/to/script.sh %path" where %path is replaced with the file being rolled:
-  # archive_command:
-  # max_archive_retries: 10
-
-# validate tombstones on reads and compaction
-# can be either "disabled", "warn" or "exception"
-# corrupted_tombstone_strategy: disabled
-
-# Diagnostic Events #
-# If enabled, diagnostic events can be helpful for troubleshooting operational issues. Emitted events contain details
-# on internal state and temporal relationships across events, accessible by clients via JMX.
-diagnostic_events_enabled: false
-
-# Use native transport TCP message coalescing. If on upgrade to 4.0 you found your throughput decreasing, and in
-# particular you run an old kernel or have very fewer client connections, this option might be worth evaluating.
-#native_transport_flush_in_batches_legacy: false
-
-# Enable tracking of repaired state of data during reads and comparison between replicas
-# Mismatches between the repaired sets of replicas can be characterized as either confirmed
-# or unconfirmed. In this context, unconfirmed indicates that the presence of pending repair
-# sessions, unrepaired partition tombstones, or some other condition means that the disparity
-# cannot be considered conclusive. Confirmed mismatches should be a trigger for investigation
-# as they may be indicative of corruption or data loss.
-# There are separate flags for range vs partition reads as single partition reads are only tracked
-# when CL > 1 and a digest mismatch occurs. Currently, range queries don't use digests so if
-# enabled for range reads, all range reads will include repaired data tracking. As this adds
-# some overhead, operators may wish to disable it whilst still enabling it for partition reads
-repaired_data_tracking_for_range_reads_enabled: false
-repaired_data_tracking_for_partition_reads_enabled: false
-# If false, only confirmed mismatches will be reported. If true, a separate metric for unconfirmed
-# mismatches will also be recorded. This is to avoid potential signal:noise issues are unconfirmed
-# mismatches are less actionable than confirmed ones.
-report_unconfirmed_repaired_data_mismatches: false
-
-# Having many tables and/or keyspaces negatively affects performance of many operations in the
-# cluster. When the number of tables/keyspaces in the cluster exceeds the following thresholds
-# a client warning will be sent back to the user when creating a table or keyspace.
-# As of cassandra 4.1, these properties are deprecated in favor of keyspaces_warn_threshold and tables_warn_threshold
-# table_count_warn_threshold: 150
-# keyspace_count_warn_threshold: 40
-
-# configure the read and write consistency levels for modifications to auth tables
-# auth_read_consistency_level: LOCAL_QUORUM
-# auth_write_consistency_level: EACH_QUORUM
-
-# Delays on auth resolution can lead to a thundering herd problem on reconnects; this option will enable
-# warming of auth caches prior to node completing startup. See CASSANDRA-16958
-# auth_cache_warming_enabled: false
-
-#########################
-# EXPERIMENTAL FEATURES #
-#########################
-
-# Enables materialized view creation on this node.
-# Materialized views are considered experimental and are not recommended for production use.
-materialized_views_enabled: false
-
-# Enables SASI index creation on this node.
-# SASI indexes are considered experimental and are not recommended for production use.
-sasi_indexes_enabled: false
-
-# Enables creation of transiently replicated keyspaces on this node.
-# Transient replication is experimental and is not recommended for production use.
-transient_replication_enabled: false
-
-# Enables the used of 'ALTER ... DROP COMPACT STORAGE' statements on this node.
-# 'ALTER ... DROP COMPACT STORAGE' is considered experimental and is not recommended for production use.
-drop_compact_storage_enabled: false
-
-# Whether or not USE <keyspace> is allowed. This is enabled by default to avoid failure on upgrade.
-#use_statements_enabled: true
-
-# When the client triggers a protocol exception or unknown issue (Cassandra bug) we increment
-# a client metric showing this; this logic will exclude specific subnets from updating these
-# metrics
-#client_error_reporting_exclusions:
-#  subnets:
-#    - 127.0.0.1
-#    - 127.0.0.0/31
-
-# Enables read thresholds (warn/fail) across all replicas for reporting back to the client.
-# See: CASSANDRA-16850
-# read_thresholds_enabled: false # scheduled to be set true in 4.2
-# When read_thresholds_enabled: true, this tracks the materialized size of a query on the
-# coordinator. If coordinator_read_size_warn_threshold is defined, this will emit a warning
-# to clients with details on what query triggered this as well as the size of the result set; if
-# coordinator_read_size_fail_threshold is defined, this will fail the query after it
-# has exceeded this threshold, returning a read error to the user.
-# coordinator_read_size_warn_threshold:
-# coordinator_read_size_fail_threshold:
-# When read_thresholds_enabled: true, this tracks the size of the local read (as defined by
-# heap size), and will warn/fail based off these thresholds; undefined disables these checks.
-# local_read_size_warn_threshold:
-# local_read_size_fail_threshold:
-# When read_thresholds_enabled: true, this tracks the expected memory size of the RowIndexEntry
-# and will warn/fail based off these thresholds; undefined disables these checks
-# row_index_read_size_warn_threshold:
-# row_index_read_size_fail_threshold:
-
-# Guardrail to warn or fail when creating more user keyspaces than threshold.
-# The two thresholds default to -1 to disable.
-# keyspaces_warn_threshold: -1
-# keyspaces_fail_threshold: -1
-# Guardrail to warn or fail when creating more user tables than threshold.
-# The two thresholds default to -1 to disable.
-# tables_warn_threshold: -1
-# tables_fail_threshold: -1
-# Guardrail to enable or disable the ability to create uncompressed tables
-# uncompressed_tables_enabled: true
-# Guardrail to warn or fail when creating/altering a table with more columns per table than threshold.
-# The two thresholds default to -1 to disable.
-# columns_per_table_warn_threshold: -1
-# columns_per_table_fail_threshold: -1
-# Guardrail to warn or fail when creating more secondary indexes per table than threshold.
-# The two thresholds default to -1 to disable.
-# secondary_indexes_per_table_warn_threshold: -1
-# secondary_indexes_per_table_fail_threshold: -1
-# Guardrail to enable or disable the creation of secondary indexes
-# secondary_indexes_enabled: true
-# Guardrail to warn or fail when creating more materialized views per table than threshold.
-# The two thresholds default to -1 to disable.
-# materialized_views_per_table_warn_threshold: -1
-# materialized_views_per_table_fail_threshold: -1
-# Guardrail to warn about, ignore or reject properties when creating tables. By default all properties are allowed.
-# table_properties_warned: []
-# table_properties_ignored: []
-# table_properties_disallowed: []
-# Guardrail to allow/disallow user-provided timestamps. Defaults to true.
-# user_timestamps_enabled: true
-# Guardrail to allow/disallow GROUP BY functionality.
-# group_by_enabled: true
-# Guardrail to allow/disallow TRUNCATE and DROP TABLE statements
-# drop_truncate_table_enabled: true
-# Guardrail to warn or fail when using a page size greater than threshold.
-# The two thresholds default to -1 to disable.
-# page_size_warn_threshold: -1
-# page_size_fail_threshold: -1
-# Guardrail to allow/disallow list operations that require read before write, i.e. setting list element by index and
-# removing list elements by either index or value. Defaults to true.
-# read_before_write_list_operations_enabled: true
-# Guardrail to warn or fail when querying with an IN restriction selecting more partition keys than threshold.
-# The two thresholds default to -1 to disable.
-# partition_keys_in_select_warn_threshold: -1
-# partition_keys_in_select_fail_threshold: -1
-# Guardrail to warn or fail when an IN query creates a cartesian product with a size exceeding threshold,
-# eg. "a in (1,2,...10) and b in (1,2...10)" results in cartesian product of 100.
-# The two thresholds default to -1 to disable.
-# in_select_cartesian_product_warn_threshold: -1
-# in_select_cartesian_product_fail_threshold: -1
-# Guardrail to warn about or reject read consistency levels. By default, all consistency levels are allowed.
-# read_consistency_levels_warned: []
-# read_consistency_levels_disallowed: []
-# Guardrail to warn about or reject write consistency levels. By default, all consistency levels are allowed.
-# write_consistency_levels_warned: []
-# write_consistency_levels_disallowed: []
-# Guardrail to warn or fail when encountering larger size of collection data than threshold.
-# At query time this guardrail is applied only to the collection fragment that is being writen, even though in the case
-# of non-frozen collections there could be unaccounted parts of the collection on the sstables. This is done this way to
-# prevent read-before-write. The guardrail is also checked at sstable write time to detect large non-frozen collections,
-# although in that case exceeding the fail threshold will only log an error message, without interrupting the operation.
-# The two thresholds default to null to disable.
-# Min unit: B
-# collection_size_warn_threshold:
-# Min unit: B
-# collection_size_fail_threshold:
-# Guardrail to warn or fail when encountering more elements in collection than threshold.
-# At query time this guardrail is applied only to the collection fragment that is being writen, even though in the case
-# of non-frozen collections there could be unaccounted parts of the collection on the sstables. This is done this way to
-# prevent read-before-write. The guardrail is also checked at sstable write time to detect large non-frozen collections,
-# although in that case exceeding the fail threshold will only log an error message, without interrupting the operation.
-# The two thresholds default to -1 to disable.
-# items_per_collection_warn_threshold: -1
-# items_per_collection_fail_threshold: -1
-# Guardrail to allow/disallow querying with ALLOW FILTERING. Defaults to true.
-# allow_filtering_enabled: true
-# Guardrail to warn or fail when creating a user-defined-type with more fields in than threshold.
-# Default -1 to disable.
-# fields_per_udt_warn_threshold: -1
-# fields_per_udt_fail_threshold: -1
-# Guardrail to warn or fail when local data disk usage percentage exceeds threshold. Valid values are in [1, 100].
-# This is only used for the disks storing data directories, so it won't count any separate disks used for storing
-# the commitlog, hints nor saved caches. The disk usage is the ratio between the amount of space used by the data
-# directories and the addition of that same space and the remaining free space on disk. The main purpose of this
-# guardrail is rejecting user writes when the disks are over the defined usage percentage, so the writes done by
-# background processes such as compaction and streaming don't fail due to a full disk. The limits should be defined
-# accordingly to the expected data growth due to those background processes, so for example a compaction strategy
-# doubling the size of the data would require to keep the disk usage under 50%.
-# The two thresholds default to -1 to disable.
-# data_disk_usage_percentage_warn_threshold: -1
-# data_disk_usage_percentage_fail_threshold: -1
-# Allows defining the max disk size of the data directories when calculating thresholds for
-# disk_usage_percentage_warn_threshold and disk_usage_percentage_fail_threshold, so if this is greater than zero they
-# become percentages of a fixed size on disk instead of percentages of the physically available disk size. This should
-# be useful when we have a large disk and we only want to use a part of it for Cassandra's data directories.
-# Valid values are in [1, max available disk size of all data directories].
-# Defaults to null to disable and use the physically available disk size of data directories during calculations.
-# Min unit: B
-# data_disk_usage_max_disk_size:
-# Guardrail to warn or fail when the minimum replication factor is lesser than threshold.
-# This would also apply to system keyspaces.
-# Suggested value for use in production: 2 or higher
-# minimum_replication_factor_warn_threshold: -1
-# minimum_replication_factor_fail_threshold: -1
-
-# Startup Checks are executed as part of Cassandra startup process, not all of them
-# are configurable (so you can disable them) but these which are enumerated bellow.
-# Uncomment the startup checks and configure them appropriately to cover your needs.
-#
-#startup_checks:
-# Verifies correct ownership of attached locations on disk at startup. See CASSANDRA-16879 for more details.
-#  check_filesystem_ownership:
-#    enabled: false
-#    ownership_token: "sometoken" # (overriden by "CassandraOwnershipToken" system property)
-#    ownership_filename: ".cassandra_fs_ownership" # (overriden by "cassandra.fs_ownership_filename")
-# Prevents a node from starting if snitch's data center differs from previous data center.
-#  check_dc:
-#    enabled: true # (overriden by cassandra.ignore_dc system property)
-# Prevents a node from starting if snitch's rack differs from previous rack.
-#  check_rack:
-#    enabled: true # (overriden by cassandra.ignore_rack system property)
-# Enable this property to fail startup if the node is down for longer than gc_grace_seconds, to potentially
-# prevent data resurrection on tables with deletes. By default, this will run against all keyspaces and tables
-# except the ones specified on excluded_keyspaces and excluded_tables.
-#  check_data_resurrection:
-#    enabled: false
-# file where Cassandra periodically writes the last time it was known to run
-#    heartbeat_file: /var/lib/cassandra/data/cassandra-heartbeat
-#    excluded_keyspaces: # comma separated list of keyspaces to exclude from the check
-#    excluded_tables: # comma separated list of keyspace.table pairs to exclude from the check
diff --git a/scripts/hosts.ini b/scripts/hosts.ini
index 17ddc8e0..c952316f 100644
--- a/scripts/hosts.ini
+++ b/scripts/hosts.ini
@@ -1,10 +1,8 @@
 [cassandra_servers]
-server1 ansible_host=node11
-server2 ansible_host=node12
-server3 ansible_host=node13
-server4 ansible_host=node14
-server5 ansible_host=node15
-server6 ansible_host=node16
+server1 ansible_host=node17
+server2 ansible_host=node18
+server3 ansible_host=node19
+server4 ansible_host=node20
 
 [cassandra_client]
-client ansible_host=node16
+client ansible_host=node20
diff --git a/scripts/playbook.yml b/scripts/playbook.yml
index 7b531c4e..f910269d 100644
--- a/scripts/playbook.yml
+++ b/scripts/playbook.yml
@@ -38,7 +38,7 @@
 
   tasks:
   - name: Execute script start-client.sh
-    command: sh /mnt/ssd/Debug/CassandraEC/scripts/start-client.sh {{ ansible_hostname }} 4 4 # <coordinator address>, <sstable size in MB>, <fanout size>
+    command: sh /mnt/ssd/Debug/CassandraEC/scripts/start-client.sh {{ ansible_hostname }} 4 10 # <coordinator address>, <sstable size in MB>, <fanout size>
     register: command_result
     ignore_errors: True
 
@@ -52,7 +52,7 @@
 
   tasks:
   - name: Execute script start-evaluation.sh
-    command: sh /mnt/ssd/Debug/CassandraEC/scripts/start-evaluation.sh {{ ansible_hostname }} 3000000 1024 # <coordinator address>, <record count>, <field length>
+    command: sh /mnt/ssd/Debug/CassandraEC/scripts/start-evaluation.sh {{ ansible_hostname }} 1000000 1000 # <coordinator address>, <record count>, <field length>
     register: command_result
     ignore_errors: True
 
diff --git a/scripts/start-client.sh b/scripts/start-client.sh
index 39bc4366..15bee6a7 100644
--- a/scripts/start-client.sh
+++ b/scripts/start-client.sh
@@ -14,9 +14,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 func() {
-    coordinator=$1
+    coordinator="192.168.10.30"
     sstable_size=$2
     fanout_size=$3
     cd /home/yjren/cassandra
@@ -29,4 +28,4 @@ func() {
     consistency all;"
 }
 
-func "$1" "$2" "$3"
\ No newline at end of file
+func "$1" "$2" "$3"
diff --git a/scripts/start-evaluation.sh b/scripts/start-evaluation.sh
index df42ef2e..89c1a8da 100644
--- a/scripts/start-evaluation.sh
+++ b/scripts/start-evaluation.sh
@@ -14,9 +14,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 func() {
-    coordinator=$1
+    coordinator="192.168.10.30"
     record_count=$2
     field_length=$3
 
@@ -34,7 +33,11 @@ func() {
     sed -i "s/recordcount=.*$/recordcount=${record_count}/" workloads/workload_template
     sed -i "s/fieldlength=.*$/fieldlength=${field_length}/" workloads/workload_template
     file_name="$(date +%s)-${record_count}-${field_length}"
-    nohup bin/ycsb load cassandra-cql -p hosts=$coordinator -s -P workloads/workload_template > logs/${file_name}.log 2>&1 &
+    nohup bin/ycsb load cassandra-cql -p hosts=$coordinator -s -P workloads/workload_template >logs/${file_name}.log 2>&1 &
+    tail -f logs/${file_name}.log
+    sleep 60
+    nohup bin/ycsb run cassandra-cql -p hosts=$coordinator -s -P workloads/workload_template >logs/${file_name}.log 2>&1 &
+    tail -f logs/${file_name}.log
 }
 
 func "$1" "$2" "$3"
diff --git a/scripts/start-server.sh b/scripts/start-server.sh
index 52312d7e..dcb8f0cb 100644
--- a/scripts/start-server.sh
+++ b/scripts/start-server.sh
@@ -15,10 +15,11 @@
 # limitations under the License.
 
 . /etc/profile
+unset JAVA_HOME
 
 cd /mnt/ssd/Debug/CassandraEC
-git pull origin yuanming
-kill -9 $(ps aux | grep cassandra| grep -v grep | awk 'NR == 1'  | awk {'print $2'})
+git pull origin yanjing
+kill -9 $(ps aux | grep cassandra | grep -v grep | awk 'NR == 1' | awk {'print $2'})
 rm -rf data logs
 mkdir -p data/receivedParityHashes/
 mkdir -p data/localParityHashes/
@@ -27,4 +28,4 @@ mkdir -p data/tmp/
 mkdir -p logs
 ant realclean && ant -Duse.jdk11=true
 cp src/native/src/org/apache/cassandra/io/erasurecode/libec.so lib/sigar-bin
-nohup bin/cassandra &> logs/debug.log &
\ No newline at end of file
+nohup bin/cassandra &>logs/debug.log &
diff --git a/src/java/org/apache/cassandra/db/AbstractReadQuery.java b/src/java/org/apache/cassandra/db/AbstractReadQuery.java
index 374d2b25..0a1156c7 100644
--- a/src/java/org/apache/cassandra/db/AbstractReadQuery.java
+++ b/src/java/org/apache/cassandra/db/AbstractReadQuery.java
@@ -29,17 +29,16 @@ import org.apache.cassandra.schema.TableMetadata;
 /**
  * Base class for {@code ReadQuery} implementations.
  */
-abstract class AbstractReadQuery extends MonitorableImpl implements ReadQuery
-{
-    private final TableMetadata metadata;
+abstract class AbstractReadQuery extends MonitorableImpl implements ReadQuery {
+    private TableMetadata metadata;
     private final int nowInSec;
 
-    private final ColumnFilter columnFilter;
-    private final RowFilter rowFilter;
+    private ColumnFilter columnFilter;
+    private RowFilter rowFilter;
     private final DataLimits limits;
 
-    protected AbstractReadQuery(TableMetadata metadata, int nowInSec, ColumnFilter columnFilter, RowFilter rowFilter, DataLimits limits)
-    {
+    protected AbstractReadQuery(TableMetadata metadata, int nowInSec, ColumnFilter columnFilter, RowFilter rowFilter,
+            DataLimits limits) {
         this.metadata = metadata;
         this.nowInSec = nowInSec;
         this.columnFilter = columnFilter;
@@ -48,64 +47,75 @@ abstract class AbstractReadQuery extends MonitorableImpl implements ReadQuery
     }
 
     @Override
-    public TableMetadata metadata()
-    {
+    public TableMetadata metadata() {
         return metadata;
     }
 
+    public Boolean updateTableMetadata(TableMetadata newTableMetadata) {
+        metadata = newTableMetadata;
+        return true;
+    }
+
     // Monitorable interface
-    public String name()
-    {
+    public String name() {
         return toCQLString();
     }
 
     @Override
-    public PartitionIterator executeInternal(ReadExecutionController controller)
-    {
+    public PartitionIterator executeInternal(ReadExecutionController controller) {
         return UnfilteredPartitionIterators.filter(executeLocally(controller), nowInSec());
     }
 
     @Override
-    public DataLimits limits()
-    {
+    public DataLimits limits() {
         return limits;
     }
 
     @Override
-    public int nowInSec()
-    {
+    public int nowInSec() {
         return nowInSec;
     }
 
     @Override
-    public RowFilter rowFilter()
-    {
+    public RowFilter rowFilter() {
         return rowFilter;
     }
 
     @Override
-    public ColumnFilter columnFilter()
-    {
+    public ColumnFilter columnFilter() {
         return columnFilter;
     }
 
+    public Boolean updateColumnFilter(ColumnFilter newFilter) {
+        this.columnFilter = newFilter;
+        return newFilter == columnFilter ? true : false;
+    }
+
+    public Boolean updateRowFilter(RowFilter newFilter) {
+        this.rowFilter = newFilter;
+        return newFilter == rowFilter ? true : false;
+    }
+
     /**
      * Recreate the CQL string corresponding to this query.
      * <p>
-     * Note that in general the returned string will not be exactly the original user string, first
-     * because there isn't always a single syntax for a given query,  but also because we don't have
-     * all the information needed (we know the non-PK columns queried but not the PK ones as internally
-     * we query them all). So this shouldn't be relied too strongly, but this should be good enough for
+     * Note that in general the returned string will not be exactly the original
+     * user string, first
+     * because there isn't always a single syntax for a given query, but also
+     * because we don't have
+     * all the information needed (we know the non-PK columns queried but not the PK
+     * ones as internally
+     * we query them all). So this shouldn't be relied too strongly, but this should
+     * be good enough for
      * debugging purpose which is what this is for.
      */
-    public String toCQLString()
-    {
+    public String toCQLString() {
         StringBuilder sb = new StringBuilder().append("SELECT ")
-                                              .append(columnFilter().toCQLString())
-                                              .append(" FROM ")
-                                              .append(ColumnIdentifier.maybeQuote(metadata().keyspace))
-                                              .append('.')
-                                              .append(ColumnIdentifier.maybeQuote(metadata().name));
+                .append(columnFilter().toCQLString())
+                .append(" FROM ")
+                .append(ColumnIdentifier.maybeQuote(metadata().keyspace))
+                .append('.')
+                .append(ColumnIdentifier.maybeQuote(metadata().name));
         appendCQLWhereClause(sb);
 
         if (limits() != DataLimits.NONE)
diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
index 532a1dc3..1c81a125 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
@@ -17,6 +17,9 @@
  */
 package org.apache.cassandra.db;
 
+import java.io.BufferedWriter;
+import java.io.FileWriter;
+
 import java.io.IOException;
 import java.io.PrintStream;
 import java.lang.reflect.Constructor;
@@ -461,10 +464,10 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
     /**
      * Get scheduled tasks for sending SSTables to do erasure coding.
      * 
-     * @param keyspaceName 
+     * @param keyspaceName
      * @param cfName
-     * @param sendSSTLevel 
-     * @param delay the selected SSTable's age must >= this value
+     * @param sendSSTLevel
+     * @param delay        the selected SSTable's age must >= this value
      * 
      */
 
@@ -522,13 +525,15 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
 
                                     // Sync sstable with secondary nodes for rewrite
                                     ECNetutils.syncSSTableWithSecondaryNodes(sstable, replicaNodes, sstHashID);
-                                    
+
                                     // Send selected sstable for perform erasure coding.
-                                    ECMessage ecMessage = new ECMessage(sstContent, new ECMessageContent(sstHashID, keyspaceName, cfName,
-                                                                        replicaNodes));
+                                    ECMessage ecMessage = new ECMessage(sstContent,
+                                            new ECMessageContent(sstHashID, keyspaceName, cfName,
+                                                    replicaNodes));
                                     ecMessage.sendSSTableToParity();
-                                    StorageService.instance.globalSSTHashToParityNodesMap.put(ecMessage.ecMessageContent.sstHashID,
-                                                                                              ecMessage.ecMessageContent.parityNodes);
+                                    StorageService.instance.globalSSTHashToParityNodesMap.put(
+                                            ecMessage.ecMessageContent.sstHashID,
+                                            ecMessage.ecMessageContent.parityNodes);
 
                                     if (!sstable.SetIsReplicationTransferredToErasureCoding()) {
                                         logger.error("rymERROR: set IsReplicationTransferredToErasureCoding failed!");
@@ -546,7 +551,8 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
                             continue;
                         }
                     } else {
-                        throw new IllegalStateException("The method of getting sstables from a certain level is error!");
+                        throw new IllegalStateException(
+                                "The method of getting sstables from a certain level is error!");
                     }
 
                 }
@@ -1427,7 +1433,8 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
                 }
 
                 @Override
-                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                        throws Exception {
                     // TODO Auto-generated method stub
                     throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
                 }
@@ -1509,7 +1516,19 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
     {
         long start = nanoTime();
         try {
+            try {
+                FileWriter writer = new FileWriter("logs/" + metadata.name, true);
+                BufferedWriter buffer = new BufferedWriter(writer);
+                buffer.write(
+                        update.partitionKey().getToken() + "\t" + update.partitionKey().getRawKey(metadata.get())
+                                + "\n");
+                buffer.close();
+            } catch (IOException e) {
+                // TODO Auto-generated catch block
+                e.printStackTrace();
+            }
             Memtable mt = data.getMemtableFor(opGroup, commitLogPosition);
+
             long timeDelta = mt.put(update, indexer, opGroup);
             DecoratedKey key = update.partitionKey();
             invalidateCachedPartition(key);
@@ -1798,7 +1817,7 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
 
     // rewrite the sstables based on the source decorated keys
     public CompactionManager.AllSSTableOpStatus sstablesRewrite(final DecoratedKey first,
-            final DecoratedKey last, 
+            final DecoratedKey last,
             List<SSTableReader> sstables,
             ECMetadata metadata, String fileNamePrefix,
             final LifecycleTransaction txn,
@@ -1808,16 +1827,18 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
             final int jobs) throws ExecutionException, InterruptedException {
         logger.debug("rymDebug: this is sstablesRewrite");
 
-        return CompactionManager.instance.performSSTableRewrite(ColumnFamilyStore.this, first, last, sstables, metadata, fileNamePrefix, txn,
+        return CompactionManager.instance.performSSTableRewrite(ColumnFamilyStore.this, first, last, sstables, metadata,
+                fileNamePrefix, txn,
                 skipIfCurrentVersion,
                 skipIfNewerThanTimestamp, skipIfCompressionMatches, jobs);
     }
 
     // [CASSANDRAEC]
-    public void replaceSSTable(ECMetadata metadata, ColumnFamilyStore cfs, String fileNamePrefix, final LifecycleTransaction txn) {
+    public void replaceSSTable(ECMetadata metadata, ColumnFamilyStore cfs, String fileNamePrefix,
+            final LifecycleTransaction txn) {
         // notify sstable changes to view and leveled generation
         // unmark sstable compacting status
-        
+
         try {
             SSTableReader ecSSTable = SSTableReader.openECSSTable(metadata, cfs, fileNamePrefix);
             if (!ecSSTable.SetIsReplicationTransferredToErasureCoding()) {
@@ -1835,8 +1856,7 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean, Memtable.Owner
             // TODO Auto-generated catch block
             e.printStackTrace();
         }
-        
-        
+
     }
 
     public CompactionManager.AllSSTableOpStatus relocateSSTables(int jobs)
diff --git a/src/java/org/apache/cassandra/db/Columns.java b/src/java/org/apache/cassandra/db/Columns.java
index 88185abf..a41ebda8 100644
--- a/src/java/org/apache/cassandra/db/Columns.java
+++ b/src/java/org/apache/cassandra/db/Columns.java
@@ -46,43 +46,40 @@ import org.apache.cassandra.utils.btree.BTreeRemoval;
 /**
  * An immutable and sorted list of (non-PK) columns for a given table.
  * <p>
- * Note that in practice, it will either store only static columns, or only regular ones. When
- * we need both type of columns, we use a {@link RegularAndStaticColumns} object.
+ * Note that in practice, it will either store only static columns, or only
+ * regular ones. When
+ * we need both type of columns, we use a {@link RegularAndStaticColumns}
+ * object.
  */
-public class Columns extends AbstractCollection<ColumnMetadata> implements Collection<ColumnMetadata>
-{
+public class Columns extends AbstractCollection<ColumnMetadata> implements Collection<ColumnMetadata> {
     public static final Serializer serializer = new Serializer();
     public static final Columns NONE = new Columns(BTree.empty(), 0);
     static final long EMPTY_SIZE = ObjectSizes.measure(NONE);
 
-    public static final ColumnMetadata FIRST_COMPLEX_STATIC =
-        new ColumnMetadata("",
-                           "",
-                           ColumnIdentifier.getInterned(ByteBufferUtil.EMPTY_BYTE_BUFFER, UTF8Type.instance),
-                           SetType.getInstance(UTF8Type.instance, true),
-                           ColumnMetadata.NO_POSITION,
-                           ColumnMetadata.Kind.STATIC);
-
-    public static final ColumnMetadata FIRST_COMPLEX_REGULAR =
-        new ColumnMetadata("",
-                           "",
-                           ColumnIdentifier.getInterned(ByteBufferUtil.EMPTY_BYTE_BUFFER, UTF8Type.instance),
-                           SetType.getInstance(UTF8Type.instance, true),
-                           ColumnMetadata.NO_POSITION,
-                           ColumnMetadata.Kind.REGULAR);
+    public static final ColumnMetadata FIRST_COMPLEX_STATIC = new ColumnMetadata("",
+            "",
+            ColumnIdentifier.getInterned(ByteBufferUtil.EMPTY_BYTE_BUFFER, UTF8Type.instance),
+            SetType.getInstance(UTF8Type.instance, true),
+            ColumnMetadata.NO_POSITION,
+            ColumnMetadata.Kind.STATIC);
+
+    public static final ColumnMetadata FIRST_COMPLEX_REGULAR = new ColumnMetadata("",
+            "",
+            ColumnIdentifier.getInterned(ByteBufferUtil.EMPTY_BYTE_BUFFER, UTF8Type.instance),
+            SetType.getInstance(UTF8Type.instance, true),
+            ColumnMetadata.NO_POSITION,
+            ColumnMetadata.Kind.REGULAR);
 
     private final Object[] columns;
     private final int complexIdx; // Index of the first complex column
 
-    private Columns(Object[] columns, int complexIdx)
-    {
+    private Columns(Object[] columns, int complexIdx) {
         assert complexIdx <= BTree.size(columns);
         this.columns = columns;
         this.complexIdx = complexIdx;
     }
 
-    private Columns(Object[] columns)
-    {
+    private Columns(Object[] columns) {
         this(columns, findFirstComplexIdx(columns));
     }
 
@@ -93,57 +90,56 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return the newly created {@code Columns} containing only {@code c}.
      */
-    public static Columns of(ColumnMetadata c)
-    {
+    public static Columns of(ColumnMetadata c) {
         return new Columns(BTree.singleton(c), c.isComplex() ? 0 : 1);
     }
 
-   /**
-    * Returns a new {@code Columns} object holing the same columns as the provided Row.
-    *
-    * @param row the row from which to create the new {@code Columns}.
-    * @return the newly created {@code Columns} containing the columns from {@code row}.
-    */
-   public static Columns from(Row row)
-   {
-       try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder())
-       {
-           for (ColumnData cd : row)
-               builder.add(cd.column());
-           Object[] tree = builder.build();
-           return new Columns(tree, findFirstComplexIdx(tree));
-       }
-   }
-
-   public static Columns from(BTree.Builder<ColumnMetadata> builder)
-   {
-       Object[] tree = builder.build();
-       return new Columns(tree, findFirstComplexIdx(tree));
-   }
+    /**
+     * Returns a new {@code Columns} object holing the same columns as the provided
+     * Row.
+     *
+     * @param row the row from which to create the new {@code Columns}.
+     * @return the newly created {@code Columns} containing the columns from
+     *         {@code row}.
+     */
+    public static Columns from(Row row) {
+        try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder()) {
+            for (ColumnData cd : row)
+                builder.add(cd.column());
+            Object[] tree = builder.build();
+            return new Columns(tree, findFirstComplexIdx(tree));
+        }
+    }
+
+    public static Columns from(BTree.Builder<ColumnMetadata> builder) {
+        Object[] tree = builder.build();
+        return new Columns(tree, findFirstComplexIdx(tree));
+    }
 
     /**
-    * Returns a new {@code Columns} object holding the same columns than the provided set.
+     * Returns a new {@code Columns} object holding the same columns than the
+     * provided set.
      * This method assumes nothing about the order of {@code s}.
      *
      * @param s the set from which to create the new {@code Columns}.
-     * @return the newly created {@code Columns} containing the columns from {@code s}.
+     * @return the newly created {@code Columns} containing the columns from
+     *         {@code s}.
      */
-    public static Columns from(Collection<ColumnMetadata> s)
-    {
+    public static Columns from(Collection<ColumnMetadata> s) {
         Object[] tree = BTree.<ColumnMetadata>builder(Comparator.naturalOrder()).addAll(s).build();
         return new Columns(tree, findFirstComplexIdx(tree));
     }
 
-    private static int findFirstComplexIdx(Object[] tree)
-    {
+    private static int findFirstComplexIdx(Object[] tree) {
         if (BTree.isEmpty(tree))
             return 0;
 
         int size = BTree.size(tree);
         ColumnMetadata last = BTree.findByIndex(tree, size - 1);
         return last.isSimple()
-             ? size
-             : BTree.ceilIndex(tree, Comparator.naturalOrder(), last.isStatic() ? FIRST_COMPLEX_STATIC : FIRST_COMPLEX_REGULAR);
+                ? size
+                : BTree.ceilIndex(tree, Comparator.naturalOrder(),
+                        last.isStatic() ? FIRST_COMPLEX_STATIC : FIRST_COMPLEX_REGULAR);
     }
 
     /**
@@ -151,8 +147,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return whether this columns is empty.
      */
-    public boolean isEmpty()
-    {
+    public boolean isEmpty() {
         return BTree.isEmpty(columns);
     }
 
@@ -161,18 +156,17 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return the number of simple columns in this object.
      */
-    public int simpleColumnCount()
-    {
+    public int simpleColumnCount() {
         return complexIdx;
     }
 
     /**
-     * The number of complex columns (non-frozen collections, udts, ...) in this object.
+     * The number of complex columns (non-frozen collections, udts, ...) in this
+     * object.
      *
      * @return the number of complex columns in this object.
      */
-    public int complexColumnCount()
-    {
+    public int complexColumnCount() {
         return BTree.size(columns) - complexIdx;
     }
 
@@ -181,8 +175,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return the total number of columns in this object.
      */
-    public int size()
-    {
+    public int size() {
         return BTree.size(columns);
     }
 
@@ -191,8 +184,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return whether this objects contains simple columns.
      */
-    public boolean hasSimple()
-    {
+    public boolean hasSimple() {
         return complexIdx > 0;
     }
 
@@ -201,8 +193,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return whether this objects contains complex columns.
      */
-    public boolean hasComplex()
-    {
+    public boolean hasComplex() {
         return complexIdx < BTree.size(columns);
     }
 
@@ -210,12 +201,11 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      * Returns the ith simple column of this object.
      *
      * @param i the index for the simple column to fectch. This must
-     * satisfy {@code 0 <= i < simpleColumnCount()}.
+     *          satisfy {@code 0 <= i < simpleColumnCount()}.
      *
      * @return the {@code i}th simple column in this object.
      */
-    public ColumnMetadata getSimple(int i)
-    {
+    public ColumnMetadata getSimple(int i) {
         return BTree.findByIndex(columns, i);
     }
 
@@ -223,12 +213,11 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      * Returns the ith complex column of this object.
      *
      * @param i the index for the complex column to fectch. This must
-     * satisfy {@code 0 <= i < complexColumnCount()}.
+     *          satisfy {@code 0 <= i < complexColumnCount()}.
      *
      * @return the {@code i}th complex column in this object.
      */
-    public ColumnMetadata getComplex(int i)
-    {
+    public ColumnMetadata getComplex(int i) {
         return BTree.findByIndex(columns, complexIdx + i);
     }
 
@@ -239,10 +228,9 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      * @param c the simple column for which to return the index of.
      *
      * @return the index for simple column {@code c} if it is contains in this
-     * object
+     *         object
      */
-    public int simpleIdx(ColumnMetadata c)
-    {
+    public int simpleIdx(ColumnMetadata c) {
         return BTree.findIndex(columns, Comparator.naturalOrder(), c);
     }
 
@@ -253,10 +241,9 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      * @param c the complex column for which to return the index of.
      *
      * @return the index for complex column {@code c} if it is contains in this
-     * object
+     *         object
      */
-    public int complexIdx(ColumnMetadata c)
-    {
+    public int complexIdx(ColumnMetadata c) {
         return BTree.findIndex(columns, Comparator.naturalOrder(), c) - complexIdx;
     }
 
@@ -267,8 +254,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return whether {@code c} is contained by this object.
      */
-    public boolean contains(ColumnMetadata c)
-    {
+    public boolean contains(ColumnMetadata c) {
         return BTree.findIndex(columns, Comparator.naturalOrder(), c) >= 0;
     }
 
@@ -279,11 +265,10 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      * @param other the other {@code Columns} to merge this object with.
      *
      * @return the result of merging/taking the union of {@code this} and
-     * {@code other}. The returned object may be one of the operand and that
-     * operand is a subset of the other operand.
+     *         {@code other}. The returned object may be one of the operand and that
+     *         operand is a subset of the other operand.
      */
-    public Columns mergeTo(Columns other)
-    {
+    public Columns mergeTo(Columns other) {
         if (this == other || other == NONE)
             return this;
         if (this == NONE)
@@ -299,20 +284,22 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
     }
 
     /**
-     * Whether this object is a superset of the provided other {@code Columns object}.
+     * Whether this object is a superset of the provided other
+     * {@code Columns object}.
      *
      * @param other the other object to test for inclusion in this object.
      *
-     * @return whether all the columns of {@code other} are contained by this object.
+     * @return whether all the columns of {@code other} are contained by this
+     *         object.
      */
-    public boolean containsAll(Collection<?> other)
-    {
+    public boolean containsAll(Collection<?> other) {
         if (other == this)
             return true;
         if (other.size() > this.size())
             return false;
 
-        BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = BTree.slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC);
+        BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = BTree.slice(columns, Comparator.naturalOrder(),
+                BTree.Dir.ASC);
         for (Object def : other)
             if (iter.next((ColumnMetadata) def) == null)
                 return false;
@@ -324,8 +311,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return an iterator over the simple columns of this object.
      */
-    public Iterator<ColumnMetadata> simpleColumns()
-    {
+    public Iterator<ColumnMetadata> simpleColumns() {
         return BTree.iterator(columns, 0, complexIdx - 1, BTree.Dir.ASC);
     }
 
@@ -334,8 +320,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return an iterator over the complex columns of this object.
      */
-    public Iterator<ColumnMetadata> complexColumns()
-    {
+    public Iterator<ColumnMetadata> complexColumns() {
         return BTree.iterator(columns, complexIdx, BTree.size(columns) - 1, BTree.Dir.ASC);
     }
 
@@ -344,8 +329,7 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return an iterator over all the columns of this object.
      */
-    public BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iterator()
-    {
+    public BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iterator() {
         return BTree.<ColumnMetadata, ColumnMetadata>slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC);
     }
 
@@ -356,17 +340,14 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @return an iterator returning columns in alphabetical order.
      */
-    public Iterator<ColumnMetadata> selectOrderIterator()
-    {
+    public Iterator<ColumnMetadata> selectOrderIterator() {
         // In wildcard selection, we want to return all columns in alphabetical order,
         // irregarding of whether they are complex or not
-        return Iterators.<ColumnMetadata>
-                         mergeSorted(ImmutableList.of(simpleColumns(), complexColumns()),
-                                     (s, c) ->
-                                     {
-                                         assert !s.kind.isPrimaryKeyKind();
-                                         return s.name.bytes.compareTo(c.name.bytes);
-                                     });
+        return Iterators.<ColumnMetadata>mergeSorted(ImmutableList.of(simpleColumns(), complexColumns()),
+                (s, c) -> {
+                    assert !s.kind.isPrimaryKeyKind();
+                    return s.name.bytes.compareTo(c.name.bytes);
+                });
     }
 
     /**
@@ -374,11 +355,11 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
      *
      * @param column the column to remove.
      *
-     * @return newly allocated columns containing all the columns of {@code this} expect
-     * for {@code column}.
+     * @return newly allocated columns containing all the columns of {@code this}
+     *         expect
+     *         for {@code column}.
      */
-    public Columns without(ColumnMetadata column)
-    {
+    public Columns without(ColumnMetadata column) {
         if (!contains(column))
             return this;
 
@@ -387,106 +368,102 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
     }
 
     /**
-     * Returns a predicate to test whether columns are included in this {@code Columns} object,
+     * Returns a predicate to test whether columns are included in this
+     * {@code Columns} object,
      * assuming that tes tested columns are passed to the predicate in sorted order.
      *
      * @return a predicate to test the inclusion of sorted columns in this object.
      */
-    public Predicate<ColumnMetadata> inOrderInclusionTester()
-    {
-        SearchIterator<ColumnMetadata, ColumnMetadata> iter = BTree.slice(columns, Comparator.naturalOrder(), BTree.Dir.ASC);
+    public Predicate<ColumnMetadata> inOrderInclusionTester() {
+        SearchIterator<ColumnMetadata, ColumnMetadata> iter = BTree.slice(columns, Comparator.naturalOrder(),
+                BTree.Dir.ASC);
         return column -> iter.next(column) != null;
     }
 
-    public void digest(Digest digest)
-    {
-        for (ColumnMetadata c : this)
+    public void digest(Digest digest) {
+        for (ColumnMetadata c : this) {
             digest.update(c.name.bytes);
+        }
+
     }
 
     /**
      * Apply a function to each column definition in forwards or reversed order.
+     * 
      * @param function
      */
-    public void apply(Consumer<ColumnMetadata> function)
-    {
+    public void apply(Consumer<ColumnMetadata> function) {
         BTree.apply(columns, function);
     }
 
     @Override
-    public boolean equals(Object other)
-    {
+    public boolean equals(Object other) {
         if (other == this)
             return true;
         if (!(other instanceof Columns))
             return false;
 
-        Columns that = (Columns)other;
+        Columns that = (Columns) other;
         return this.complexIdx == that.complexIdx && BTree.equals(this.columns, that.columns);
     }
 
     @Override
-    public int hashCode()
-    {
+    public int hashCode() {
         return Objects.hash(complexIdx, BTree.hashCode(columns));
     }
 
-    public long unsharedHeapSize()
-    {
-        if(this == NONE)
+    public long unsharedHeapSize() {
+        if (this == NONE)
             return 0;
 
         return EMPTY_SIZE;
     }
 
     @Override
-    public String toString()
-    {
+    public String toString() {
         StringBuilder sb = new StringBuilder("[");
         boolean first = true;
-        for (ColumnMetadata def : this)
-        {
-            if (first) first = false; else sb.append(" ");
+        for (ColumnMetadata def : this) {
+            if (first)
+                first = false;
+            else
+                sb.append(" ");
             sb.append(def.name);
         }
         return sb.append("]").toString();
     }
 
-    public static class Serializer
-    {
-        public void serialize(Columns columns, DataOutputPlus out) throws IOException
-        {
+    public static class Serializer {
+        public void serialize(Columns columns, DataOutputPlus out) throws IOException {
             out.writeUnsignedVInt(columns.size());
             for (ColumnMetadata column : columns)
                 ByteBufferUtil.writeWithVIntLength(column.name.bytes, out);
         }
 
-        public long serializedSize(Columns columns)
-        {
+        public long serializedSize(Columns columns) {
             long size = TypeSizes.sizeofUnsignedVInt(columns.size());
             for (ColumnMetadata column : columns)
                 size += ByteBufferUtil.serializedSizeWithVIntLength(column.name.bytes);
             return size;
         }
 
-        public Columns deserialize(DataInputPlus in, TableMetadata metadata) throws IOException
-        {
-            int length = (int)in.readUnsignedVInt();
-            try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder())
-            {
-                for (int i = 0; i < length; i++)
-                {
+        public Columns deserialize(DataInputPlus in, TableMetadata metadata) throws IOException {
+            int length = (int) in.readUnsignedVInt();
+            try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder()) {
+                for (int i = 0; i < length; i++) {
                     ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);
                     ColumnMetadata column = metadata.getColumn(name);
-                    if (column == null)
-                    {
-                        // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't
-                        // fail deserialization because of that. So we grab a "fake" ColumnMetadata that ensure proper
+                    if (column == null) {
+                        // If we don't find the definition, it could be we have data for a dropped
+                        // column, and we shouldn't
+                        // fail deserialization because of that. So we grab a "fake" ColumnMetadata that
+                        // ensure proper
                         // deserialization. The column will be ignore later on anyway.
                         column = metadata.getDroppedColumn(name);
 
                         if (column == null)
-                            throw new RuntimeException("Unknown column " + UTF8Type.instance.getString(name) + " during deserialization");
+                            throw new RuntimeException(
+                                    "Unknown column " + UTF8Type.instance.getString(name) + " during deserialization");
                     }
                     builder.add(column);
                 }
@@ -495,77 +472,63 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
         }
 
         /**
-         * If both ends have a pre-shared superset of the columns we are serializing, we can send them much
+         * If both ends have a pre-shared superset of the columns we are serializing, we
+         * can send them much
          * more efficiently. Both ends must provide the identically same set of columns.
          */
-        public void serializeSubset(Collection<ColumnMetadata> columns, Columns superset, DataOutputPlus out) throws IOException
-        {
+        public void serializeSubset(Collection<ColumnMetadata> columns, Columns superset, DataOutputPlus out)
+                throws IOException {
             /**
-             * We weight this towards small sets, and sets where the majority of items are present, since
+             * We weight this towards small sets, and sets where the majority of items are
+             * present, since
              * we expect this to mostly be used for serializing result sets.
              *
-             * For supersets with fewer than 64 columns, we encode a bitmap of *missing* columns,
-             * which equates to a zero (single byte) when all columns are present, and otherwise
+             * For supersets with fewer than 64 columns, we encode a bitmap of *missing*
+             * columns,
+             * which equates to a zero (single byte) when all columns are present, and
+             * otherwise
              * a positive integer that can typically be vint encoded efficiently.
              *
-             * If we have 64 or more columns, we cannot neatly perform a bitmap encoding, so we just switch
-             * to a vint encoded set of deltas, either adding or subtracting (whichever is most efficient).
+             * If we have 64 or more columns, we cannot neatly perform a bitmap encoding, so
+             * we just switch
+             * to a vint encoded set of deltas, either adding or subtracting (whichever is
+             * most efficient).
              * We indicate this switch by sending our bitmap with every bit set, i.e. -1L
              */
             int columnCount = columns.size();
             int supersetCount = superset.size();
-            if (columnCount == supersetCount)
-            {
+            if (columnCount == supersetCount) {
                 out.writeUnsignedVInt(0);
-            }
-            else if (supersetCount < 64)
-            {
+            } else if (supersetCount < 64) {
                 out.writeUnsignedVInt(encodeBitmap(columns, superset, supersetCount));
-            }
-            else
-            {
+            } else {
                 serializeLargeSubset(columns, columnCount, superset, supersetCount, out);
             }
         }
 
-        public long serializedSubsetSize(Collection<ColumnMetadata> columns, Columns superset)
-        {
+        public long serializedSubsetSize(Collection<ColumnMetadata> columns, Columns superset) {
             int columnCount = columns.size();
             int supersetCount = superset.size();
-            if (columnCount == supersetCount)
-            {
+            if (columnCount == supersetCount) {
                 return TypeSizes.sizeofUnsignedVInt(0);
-            }
-            else if (supersetCount < 64)
-            {
+            } else if (supersetCount < 64) {
                 return TypeSizes.sizeofUnsignedVInt(encodeBitmap(columns, superset, supersetCount));
-            }
-            else
-            {
+            } else {
                 return serializeLargeSubsetSize(columns, columnCount, superset, supersetCount);
             }
         }
 
-        public Columns deserializeSubset(Columns superset, DataInputPlus in) throws IOException
-        {
+        public Columns deserializeSubset(Columns superset, DataInputPlus in) throws IOException {
             long encoded = in.readUnsignedVInt();
-            if (encoded == 0L)
-            {
+            if (encoded == 0L) {
                 return superset;
-            }
-            else if (superset.size() >= 64)
-            {
+            } else if (superset.size() >= 64) {
                 return deserializeLargeSubset(in, superset, (int) encoded);
-            }
-            else
-            {
-                try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder())
-                {
+            } else {
+                try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder()) {
                     int firstComplexIdx = 0;
-                    for (ColumnMetadata column : superset)
-                    {
-                        if ((encoded & 1) == 0)
-                        {
+                    for (ColumnMetadata column : superset) {
+                        if ((encoded & 1) == 0) {
                             builder.add(column);
                             if (column.isSimple())
                                 ++firstComplexIdx;
@@ -573,29 +536,30 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
                         encoded >>>= 1;
                     }
                     if (encoded != 0)
-                        throw new IOException("Invalid Columns subset bytes; too many bits set:" + Long.toBinaryString(encoded));
+                        throw new IOException(
+                                "Invalid Columns subset bytes; too many bits set:" + Long.toBinaryString(encoded));
                     return new Columns(builder.build(), firstComplexIdx);
                 }
             }
         }
 
-        // encodes a 1 bit for every *missing* column, on the assumption presence is more common,
+        // encodes a 1 bit for every *missing* column, on the assumption presence is
+        // more common,
         // and because this is consistent with encoding 0 to represent all present
-        private static long encodeBitmap(Collection<ColumnMetadata> columns, Columns superset, int supersetCount)
-        {
+        private static long encodeBitmap(Collection<ColumnMetadata> columns, Columns superset, int supersetCount) {
             long bitmap = 0L;
             BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = superset.iterator();
             // the index we would encounter next if all columns are present
             int expectIndex = 0;
-            for (ColumnMetadata column : columns)
-            {
+            for (ColumnMetadata column : columns) {
                 if (iter.next(column) == null)
                     throw new IllegalStateException(columns + " is not a subset of " + superset);
 
                 int currentIndex = iter.indexOfCurrent();
                 int count = currentIndex - expectIndex;
                 // (1L << count) - 1 gives us count bits set at the bottom of the register
-                // so << expectIndex moves these bits to start at expectIndex, which is where our missing portion
+                // so << expectIndex moves these bits to start at expectIndex, which is where
+                // our missing portion
                 // begins (assuming count > 0; if not, we're adding 0 bits, so it's a no-op)
                 bitmap |= ((1L << count) - 1) << expectIndex;
                 expectIndex = currentIndex + 1;
@@ -606,27 +570,22 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
         }
 
         @DontInline
-        private void serializeLargeSubset(Collection<ColumnMetadata> columns, int columnCount, Columns superset, int supersetCount, DataOutputPlus out) throws IOException
-        {
+        private void serializeLargeSubset(Collection<ColumnMetadata> columns, int columnCount, Columns superset,
+                int supersetCount, DataOutputPlus out) throws IOException {
             // write flag indicating we're in lengthy mode
             out.writeUnsignedVInt(supersetCount - columnCount);
             BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = superset.iterator();
-            if (columnCount < supersetCount / 2)
-            {
+            if (columnCount < supersetCount / 2) {
                 // write present columns
-                for (ColumnMetadata column : columns)
-                {
+                for (ColumnMetadata column : columns) {
                     if (iter.next(column) == null)
                         throw new IllegalStateException();
                     out.writeUnsignedVInt(iter.indexOfCurrent());
                 }
-            }
-            else
-            {
+            } else {
                 // write missing columns
                 int prev = -1;
-                for (ColumnMetadata column : columns)
-                {
+                for (ColumnMetadata column : columns) {
                     if (iter.next(column) == null)
                         throw new IllegalStateException();
                     int cur = iter.indexOfCurrent();
@@ -639,31 +598,23 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
         }
 
         @DontInline
-        private Columns deserializeLargeSubset(DataInputPlus in, Columns superset, int delta) throws IOException
-        {
+        private Columns deserializeLargeSubset(DataInputPlus in, Columns superset, int delta) throws IOException {
             int supersetCount = superset.size();
             int columnCount = supersetCount - delta;
 
-            try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder())
-            {
-                if (columnCount < supersetCount / 2)
-                {
-                    for (int i = 0 ; i < columnCount ; i++)
-                    {
+            try (BTree.FastBuilder<ColumnMetadata> builder = BTree.fastBuilder()) {
+                if (columnCount < supersetCount / 2) {
+                    for (int i = 0; i < columnCount; i++) {
                         int idx = (int) in.readUnsignedVInt();
                         builder.add(BTree.findByIndex(superset.columns, idx));
                     }
-                }
-                else
-                {
+                } else {
                     Iterator<ColumnMetadata> iter = superset.iterator();
                     int idx = 0;
                     int skipped = 0;
-                    while (true)
-                    {
-                        int nextMissingIndex = skipped < delta ? (int)in.readUnsignedVInt() : supersetCount;
-                        while (idx < nextMissingIndex)
-                        {
+                    while (true) {
+                        int nextMissingIndex = skipped < delta ? (int) in.readUnsignedVInt() : supersetCount;
+                        while (idx < nextMissingIndex) {
                             ColumnMetadata def = iter.next();
                             builder.add(def);
                             idx++;
@@ -680,27 +631,22 @@ public class Columns extends AbstractCollection<ColumnMetadata> implements Colle
         }
 
         @DontInline
-        private int serializeLargeSubsetSize(Collection<ColumnMetadata> columns, int columnCount, Columns superset, int supersetCount)
-        {
+        private int serializeLargeSubsetSize(Collection<ColumnMetadata> columns, int columnCount, Columns superset,
+                int supersetCount) {
             // write flag indicating we're in lengthy mode
             int size = TypeSizes.sizeofUnsignedVInt(supersetCount - columnCount);
             BTreeSearchIterator<ColumnMetadata, ColumnMetadata> iter = superset.iterator();
-            if (columnCount < supersetCount / 2)
-            {
+            if (columnCount < supersetCount / 2) {
                 // write present columns
-                for (ColumnMetadata column : columns)
-                {
+                for (ColumnMetadata column : columns) {
                     if (iter.next(column) == null)
                         throw new IllegalStateException();
                     size += TypeSizes.sizeofUnsignedVInt(iter.indexOfCurrent());
                 }
-            }
-            else
-            {
+            } else {
                 // write missing columns
                 int prev = -1;
-                for (ColumnMetadata column : columns)
-                {
+                for (ColumnMetadata column : columns) {
                     if (iter.next(column) == null)
                         throw new IllegalStateException();
                     int cur = iter.indexOfCurrent();
diff --git a/src/java/org/apache/cassandra/db/Keyspace.java b/src/java/org/apache/cassandra/db/Keyspace.java
index b63f5cba..1a6f9107 100644
--- a/src/java/org/apache/cassandra/db/Keyspace.java
+++ b/src/java/org/apache/cassandra/db/Keyspace.java
@@ -17,8 +17,6 @@
  */
 package org.apache.cassandra.db;
 
-import java.io.BufferedWriter;
-import java.io.FileWriter;
 import java.io.IOException;
 import java.time.Instant;
 import java.util.ArrayList;
@@ -108,19 +106,23 @@ public class Keyspace {
     private static int TEST_FAIL_MV_LOCKS_COUNT = Integer.getInteger("cassandra.test.fail_mv_locks_count", 0);
 
     private static final String ycsbUsertableCql = "CREATE TABLE %s ("
-                                    + "y_id varchar primary key,"
-                                    + "field0 varchar,"
-                                    + "field1 varchar,"
-                                    + "field2 varchar,"
-                                    + "field3 varchar,"
-                                    + "field4 varchar,"
-                                    + "field5 varchar,"
-                                    + "field6 varchar,"
-                                    + "field7 varchar,"
-                                    + "field8 varchar,"
-                                    + "field9 varchar)";
+            + "y_id varchar primary key,"
+            + "field0 varchar,"
+            + "field1 varchar,"
+            + "field2 varchar,"
+            + "field3 varchar,"
+            + "field4 varchar,"
+            + "field5 varchar,"
+            + "field6 varchar,"
+            + "field7 varchar,"
+            + "field8 varchar,"
+            + "field9 varchar)";
 
     public final KeyspaceMetrics metric;
+    public final InetAddress localAddress = FBUtilities.getJustBroadcastAddress();
+    public final String primaryColumnNameStr = "usertable";
+    public final String secondaryColumn1NameStr = "usertable1";
+    public final String secondaryColumn2NameStr = "usertable2";
 
     // It is possible to call Keyspace.open without a running daemon, so it makes
     // sense to ensure
@@ -459,7 +461,7 @@ public class Keyspace {
                 globalNodeIDtoCFIDMap.put(globalNodeIDtoCFIDMap.size(), metadata.id);
                 // logger.debug("rymDebug: globalNodeIDtoCFIDMap is {}", globalNodeIDtoCFIDMap);
             }
-            
+
             // CFS mbean instantiation will error out before we hit this, but in case that
             // changes...
             if (oldCfs != null)
@@ -478,13 +480,12 @@ public class Keyspace {
         if (keyspaceName.equals("ycsb")) {
 
             return applyInternalYCSB(mutation, writeCommitLog, updateIndexes, true, true,
-                                    new AsyncPromise<>());
+                    new AsyncPromise<>());
         } else {
 
             return applyInternal(mutation, writeCommitLog, updateIndexes, true, true, new AsyncPromise<>());
         }
 
-        
     }
 
     public Future<?> applyFuture(Mutation mutation, boolean writeCommitLog, boolean updateIndexes, boolean isDroppable,
@@ -497,10 +498,10 @@ public class Keyspace {
                     new AsyncPromise<>());
         } else {
 
-            return applyInternal(mutation, writeCommitLog, updateIndexes, isDroppable, isDeferrable, new AsyncPromise<>());
+            return applyInternal(mutation, writeCommitLog, updateIndexes, isDroppable, isDeferrable,
+                    new AsyncPromise<>());
         }
 
-        
     }
 
     public void apply(Mutation mutation, boolean writeCommitLog, boolean updateIndexes) {
@@ -535,19 +536,19 @@ public class Keyspace {
         String keyspaceName = mutation.getKeyspaceName();
         if (keyspaceName.equals("ycsb")) {
             // if(keyspaceName.equals("ycsb")) {
-            //     for(PartitionUpdate upd : mutation.getPartitionUpdates()) {
-            //         String fileName = "receivedkeyLocal";
-            //         try {
-            //             FileWriter writer = new FileWriter("logs/" + fileName, true);
-            //             BufferedWriter buffer = new BufferedWriter(writer);
-            //             buffer.write(upd.partitionKey().getRawKey(upd.metadata()) + "\n");
-            //             buffer.close();
-            //         } catch (IOException e) {
-            //             // TODO Auto-generated catch block
-            //             e.printStackTrace();
-            //         }
-    
-            //     }
+            // for(PartitionUpdate upd : mutation.getPartitionUpdates()) {
+            // String fileName = "receivedkeyLocal";
+            // try {
+            // FileWriter writer = new FileWriter("logs/" + fileName, true);
+            // BufferedWriter buffer = new BufferedWriter(writer);
+            // buffer.write(upd.partitionKey().getRawKey(upd.metadata()) + "\n");
+            // buffer.close();
+            // } catch (IOException e) {
+            // // TODO Auto-generated catch block
+            // e.printStackTrace();
+            // }
+
+            // }
             // }
 
             applyInternalYCSB(mutation, makeDurable, updateIndexes, isDroppable, false, null);
@@ -555,7 +556,6 @@ public class Keyspace {
             applyInternal(mutation, makeDurable, updateIndexes, isDroppable, false, null);
         }
 
-        
     }
 
     private Future<?> applyInternalYCSB(final Mutation mutation,
@@ -658,7 +658,6 @@ public class Keyspace {
             // ColumnFamilyStore cfs = columnFamilyStores.get("replicaUUID");
             for (PartitionUpdate upd : mutation.getPartitionUpdates()) {
 
-
                 // ColumnFamilyStore cfs = columnFamilyStores.get(upd.metadata().id);
                 ColumnFamilyStore cfs = getColumnFamilyStore(upd);
                 if (cfs == null) {
@@ -710,38 +709,27 @@ public class Keyspace {
         List<InetAddress> eps = StorageService.instance.getNaturalEndpoints(keyspaceName, key);
         InetAddress localAddress = FBUtilities.getJustBroadcastAddress();
         TableId replicaUUID = null;
-        // logger.debug("rymDebug: Storage servers list size :{}, list content : {}", columnFamilyStores.size(), ep);
+        // logger.debug("rymDebug: Storage servers list size :{}, list content : {}",
+        // columnFamilyStores.size(), ep);
         // logger.debug("localAddress is {}, replicaNodes are {}",localAddress, eps);
         // make sure whether the mutation is for a primary or not.
-        int  index = eps.indexOf(localAddress);
+        int index = eps.indexOf(localAddress);
         replicaUUID = globalNodeIDtoCFIDMap.get(index);
         String fileName = "usertable";
-        if(index!=0) {
-            fileName+=index;
-        }
-
-        try {
-            FileWriter writer = new FileWriter("logs/usertableAll", true);
-            BufferedWriter buffer = new BufferedWriter(writer);
-            buffer.write(upd.partitionKey().getRawKey(upd.metadata())+"\n");
-            buffer.close();
-        } catch (IOException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
+        if (index != 0) {
+            fileName += index;
         }
 
+        // try {
+        // FileWriter writer = new FileWriter("logs/usertableAll", true);
+        // BufferedWriter buffer = new BufferedWriter(writer);
+        // buffer.write(upd.partitionKey().getToken() + "\n");
+        // buffer.close();
+        // } catch (IOException e) {
+        // // TODO Auto-generated catch block
+        // e.printStackTrace();
+        // }
 
-
-        try {
-            FileWriter writer = new FileWriter("logs/"+fileName, true);
-            BufferedWriter buffer = new BufferedWriter(writer);
-            buffer.write(upd.partitionKey().getRawKey(upd.metadata())+"\n");
-            buffer.close();
-        } catch (IOException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-        }
-        
         return columnFamilyStores.get(replicaUUID);
     }
 
diff --git a/src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java b/src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
index 057d2620..1bc99591 100644
--- a/src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
+++ b/src/java/org/apache/cassandra/db/PartitionRangeReadCommand.java
@@ -333,8 +333,7 @@ public class PartitionRangeReadCommand extends ReadCommand implements PartitionR
                     SSTableReadsListener readCountUpdater = newReadCountUpdater();
                     for (Memtable memtable : view.memtables) {
                         @SuppressWarnings("resource") // We close on exception and on closing the result returned by
-                                                      // this
-                                                      // method
+                                                      // this method
                         UnfilteredPartitionIterator iter = memtable.partitionIterator(columnFilter(), dataRange(),
                                 readCountUpdater);
                         controller.updateMinOldestUnrepairedTombstone(memtable.getMinLocalDeletionTime());
@@ -663,4 +662,5 @@ public class PartitionRangeReadCommand extends ReadCommand implements PartitionR
             return executionController();
         }
     }
+
 }
diff --git a/src/java/org/apache/cassandra/db/ReadCommand.java b/src/java/org/apache/cassandra/db/ReadCommand.java
index 7e2886db..12ff2adb 100644
--- a/src/java/org/apache/cassandra/db/ReadCommand.java
+++ b/src/java/org/apache/cassandra/db/ReadCommand.java
@@ -96,7 +96,7 @@ public abstract class ReadCommand extends AbstractReadQuery {
 
     private final Kind kind;
 
-    private final boolean isDigestQuery;
+    private boolean isDigestQuery;
     private final boolean acceptsTransient;
     // if a digest query, the version for which the digest is expected. Ignored if
     // not a digest.
@@ -191,6 +191,11 @@ public abstract class ReadCommand extends AbstractReadQuery {
         return isDigestQuery;
     }
 
+    public boolean setIsDigestQuery(boolean isDigestQueryFlag) {
+        isDigestQuery = isDigestQueryFlag;
+        return isDigestQuery;
+    }
+
     /**
      * If the query is a digest one, the requested digest version.
      *
@@ -312,19 +317,8 @@ public abstract class ReadCommand extends AbstractReadQuery {
         return copyAsDigestQuery();
     }
 
-    /**
-     * Returns a copy of this command with isDigestQuery set to true.
-     */
-    public ReadCommand copyAsDigestQuery(Replica replica, int replicationIDIndicator) {
-        Preconditions.checkArgument(replica.isFull(),
-                "Can't make a digest request on a transient replica " + replica);
-        return copyAsDigestQuery(replicationIDIndicator);
-    }
-
     protected abstract ReadCommand copyAsDigestQuery();
 
-    protected abstract ReadCommand copyAsDigestQuery(int replicationIDIndicator);
-
     protected abstract UnfilteredPartitionIterator queryStorage(ColumnFamilyStore cfs,
             ReadExecutionController executionController);
 
diff --git a/src/java/org/apache/cassandra/db/ReadCommandVerbHandler.java b/src/java/org/apache/cassandra/db/ReadCommandVerbHandler.java
index d8bb002e..953927bd 100644
--- a/src/java/org/apache/cassandra/db/ReadCommandVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/ReadCommandVerbHandler.java
@@ -44,6 +44,21 @@ public class ReadCommandVerbHandler implements IVerbHandler<ReadCommand> {
         }
 
         ReadCommand command = message.payload;
+
+        if (command.metadata().keyspace.equals("ycsb")) {
+            logger.debug(
+                    "[Tinoryj] Received read command from {}, target table is {}, target key is {}, key token is {}",
+                    message.from(),
+                    command.metadata().name,
+                    command instanceof SinglePartitionReadCommand
+                            ? ((SinglePartitionReadCommand) command).partitionKey().getRawKey(command.metadata())
+                            : null,
+                    command instanceof SinglePartitionReadCommand
+                            ? ((SinglePartitionReadCommand) command).partitionKey().getToken()
+                            : null);
+
+        }
+
         validateTransientStatus(message);
         MessageParams.reset();
 
@@ -51,14 +66,42 @@ public class ReadCommandVerbHandler implements IVerbHandler<ReadCommand> {
         command.setMonitoringTime(message.createdAtNanos(), message.isCrossNode(), timeout,
                 DatabaseDescriptor.getSlowQueryTimeout(NANOSECONDS));
 
-        if (message.trackWarnings())
+        if (message.trackWarnings()) {
             command.trackWarnings();
+        }
 
         ReadResponse response;
         try (ReadExecutionController controller = command.executionController(message.trackRepairedData());
                 UnfilteredPartitionIterator iterator = command.executeLocally(controller)) {
             response = command.createResponse(iterator, controller.getRepairedDataInfo());
+            if (command.metadata().keyspace.equals("ycsb")) {
+                logger.debug(
+                        "[Tinoryj] ReadCommandVerbHandler from {}, Read Command target table is {}, target key is {}, key token is {}, response is {}",
+                        message.from(),
+                        command.metadata().name,
+                        command instanceof SinglePartitionReadCommand
+                                ? ((SinglePartitionReadCommand) command).partitionKey().getRawKey(command.metadata())
+                                : null,
+                        command instanceof SinglePartitionReadCommand
+                                ? ((SinglePartitionReadCommand) command).partitionKey().getToken()
+                                : null,
+                        response.toDebugString(command, command instanceof SinglePartitionReadCommand
+                                ? ((SinglePartitionReadCommand) command).partitionKey()
+                                : null));
+            }
         } catch (RejectException e) {
+            if (command.metadata().keyspace.equals("ycsb")) {
+                logger.debug(
+                        "[Tinoryj] ReadCommandVerbHandler from {}, Read Command target table is {}, target key is {}, key token is {}, meet errors",
+                        message.from(),
+                        command.metadata().name,
+                        command instanceof SinglePartitionReadCommand
+                                ? ((SinglePartitionReadCommand) command).partitionKey().getRawKey(command.metadata())
+                                : null,
+                        command instanceof SinglePartitionReadCommand
+                                ? ((SinglePartitionReadCommand) command).partitionKey().getToken()
+                                : null);
+            }
             if (!command.isTrackingWarnings())
                 throw e;
 
@@ -94,12 +137,14 @@ public class ReadCommandVerbHandler implements IVerbHandler<ReadCommand> {
 
         if (command instanceof SinglePartitionReadCommand) {
             token = ((SinglePartitionReadCommand) command).partitionKey().getToken();
-            logger.debug("[Tinoryj] touch SinglePartitionReadCommand, token is {}, the partition key is {}",
-                    token, ((SinglePartitionReadCommand) command).partitionKey().toString());
+            // logger.debug("[Tinoryj] touch SinglePartitionReadCommand, token is {}, the
+            // partition key is {}",
+            // token, ((SinglePartitionReadCommand)
+            // command).partitionKey().getRawKey(command.metadata()));
         } else {
             token = ((PartitionRangeReadCommand) command).dataRange().keyRange().right.getToken();
-            logger.debug("[Tinoryj] touch PartitionRangeReadCommand, token is {}",
-                    token);
+            // logger.debug("[Tinoryj] touch PartitionRangeReadCommand, token is {}",
+            // token);
         }
 
         Replica replica = Keyspace.open(command.metadata().keyspace)
diff --git a/src/java/org/apache/cassandra/db/ReadExecutionController.java b/src/java/org/apache/cassandra/db/ReadExecutionController.java
index 2fbe3ac6..717897ab 100644
--- a/src/java/org/apache/cassandra/db/ReadExecutionController.java
+++ b/src/java/org/apache/cassandra/db/ReadExecutionController.java
@@ -30,8 +30,7 @@ import org.apache.cassandra.utils.concurrent.OpOrder;
 
 import static org.apache.cassandra.utils.MonotonicClock.Global.preciseTime;
 
-public class ReadExecutionController implements AutoCloseable
-{
+public class ReadExecutionController implements AutoCloseable {
     private static final long NO_SAMPLING = Long.MIN_VALUE;
 
     // For every reads
@@ -50,15 +49,16 @@ public class ReadExecutionController implements AutoCloseable
     private int oldestUnrepairedTombstone = Integer.MAX_VALUE;
 
     ReadExecutionController(ReadCommand command,
-                            OpOrder.Group baseOp,
-                            TableMetadata baseMetadata,
-                            ReadExecutionController indexController,
-                            WriteContext writeContext,
-                            long createdAtNanos,
-                            boolean trackRepairedStatus)
-    {
-        // We can have baseOp == null, but only when empty() is called, in which case the controller will never really be used
-        // (which validForReadOn should ensure). But if it's not null, we should have the proper metadata too.
+            OpOrder.Group baseOp,
+            TableMetadata baseMetadata,
+            ReadExecutionController indexController,
+            WriteContext writeContext,
+            long createdAtNanos,
+            boolean trackRepairedStatus) {
+        // We can have baseOp == null, but only when empty() is called, in which case
+        // the controller will never really be used
+        // (which validForReadOn should ensure). But if it's not null, we should have
+        // the proper metadata too.
         assert (baseOp == null) == (baseMetadata == null);
         this.baseOp = baseOp;
         this.baseMetadata = baseMetadata;
@@ -67,97 +67,89 @@ public class ReadExecutionController implements AutoCloseable
         this.command = command;
         this.createdAtNanos = createdAtNanos;
 
-        if (trackRepairedStatus)
-        {
+        if (trackRepairedStatus) {
             DataLimits.Counter repairedReadCount = command.limits().newCounter(command.nowInSec(),
-                                                                               false,
-                                                                               command.selectsFullPartition(),
-                                                                               metadata().enforceStrictLiveness()).onlyCount();
+                    false,
+                    command.selectsFullPartition(),
+                    metadata().enforceStrictLiveness()).onlyCount();
             repairedDataInfo = new RepairedDataInfo(repairedReadCount);
-        }
-        else
-        {
+        } else {
             repairedDataInfo = RepairedDataInfo.NO_OP_REPAIRED_DATA_INFO;
         }
     }
 
-    public ReadExecutionController indexReadController()
-    {
+    public ReadExecutionController indexReadController() {
         return indexController;
     }
 
-    public WriteContext getWriteContext()
-    {
+    public WriteContext getWriteContext() {
         return writeContext;
     }
 
-    int oldestUnrepairedTombstone()
-    {
+    int oldestUnrepairedTombstone() {
         return oldestUnrepairedTombstone;
     }
-    
-    void updateMinOldestUnrepairedTombstone(int candidate)
-    {
+
+    void updateMinOldestUnrepairedTombstone(int candidate) {
         oldestUnrepairedTombstone = Math.min(oldestUnrepairedTombstone, candidate);
     }
 
-    boolean validForReadOn(ColumnFamilyStore cfs)
-    {
+    boolean validForReadOn(ColumnFamilyStore cfs) {
         return baseOp != null && cfs.metadata.id.equals(baseMetadata.id);
     }
 
-    public static ReadExecutionController empty()
-    {
+    public static ReadExecutionController empty() {
         return new ReadExecutionController(null, null, null, null, null, NO_SAMPLING, false);
     }
 
     /**
      * Creates an execution controller for the provided command.
      * <p>
-     * Note: no code should use this method outside of {@link ReadCommand#executionController} (for
-     * consistency sake) and you should use that latter method if you need an execution controller.
+     * Note: no code should use this method outside of
+     * {@link ReadCommand#executionController} (for
+     * consistency sake) and you should use that latter method if you need an
+     * execution controller.
      *
      * @param command the command for which to create a controller.
      * @return the created execution controller, which must always be closed.
      */
     @SuppressWarnings("resource") // ops closed during controller close
-    static ReadExecutionController forCommand(ReadCommand command, boolean trackRepairedStatus)
-    {
+    static ReadExecutionController forCommand(ReadCommand command, boolean trackRepairedStatus) {
         ColumnFamilyStore baseCfs = Keyspace.openAndGetStore(command.metadata());
         ColumnFamilyStore indexCfs = maybeGetIndexCfs(baseCfs, command);
 
         long createdAtNanos = baseCfs.metric.topLocalReadQueryTime.isEnabled() ? clock.now() : NO_SAMPLING;
 
         if (indexCfs == null)
-            return new ReadExecutionController(command, baseCfs.readOrdering.start(), baseCfs.metadata(), null, null, createdAtNanos, trackRepairedStatus);
+            return new ReadExecutionController(command, baseCfs.readOrdering.start(), baseCfs.metadata(), null, null,
+                    createdAtNanos, trackRepairedStatus);
 
         OpOrder.Group baseOp = null;
         WriteContext writeContext = null;
         ReadExecutionController indexController = null;
         // OpOrder.start() shouldn't fail, but better safe than sorry.
-        try
-        {
+        try {
             baseOp = baseCfs.readOrdering.start();
-            indexController = new ReadExecutionController(command, indexCfs.readOrdering.start(), indexCfs.metadata(), null, null, NO_SAMPLING, false);
+            indexController = new ReadExecutionController(command, indexCfs.readOrdering.start(), indexCfs.metadata(),
+                    null, null, NO_SAMPLING, false);
             /*
-             * TODO: this should perhaps not open and maintain a writeOp for the full duration, but instead only *try*
+             * TODO: this should perhaps not open and maintain a writeOp for the full
+             * duration, but instead only *try*
              * to delete stale entries, without blocking if there's no room
-             * as it stands, we open a writeOp and keep it open for the duration to ensure that should this CF get flushed to make room we don't block the reclamation of any room being made
+             * as it stands, we open a writeOp and keep it open for the duration to ensure
+             * that should this CF get flushed to make room we don't block the reclamation
+             * of any room being made
              */
             writeContext = baseCfs.keyspace.getWriteHandler().createContextForRead();
-            return new ReadExecutionController(command, baseOp, baseCfs.metadata(), indexController, writeContext, createdAtNanos, trackRepairedStatus);
-        }
-        catch (RuntimeException e)
-        {
+            return new ReadExecutionController(command, baseOp, baseCfs.metadata(), indexController, writeContext,
+                    createdAtNanos, trackRepairedStatus);
+        } catch (RuntimeException e) {
             // Note that must have writeContext == null since ReadOrderGroup ctor can't fail
             assert writeContext == null;
-            try
-            {
+            try {
                 if (baseOp != null)
                     baseOp.close();
-            }
-            finally
-            {
+            } finally {
                 if (indexController != null)
                     indexController.close();
             }
@@ -165,34 +157,24 @@ public class ReadExecutionController implements AutoCloseable
         }
     }
 
-    private static ColumnFamilyStore maybeGetIndexCfs(ColumnFamilyStore baseCfs, ReadCommand command)
-    {
+    private static ColumnFamilyStore maybeGetIndexCfs(ColumnFamilyStore baseCfs, ReadCommand command) {
         Index index = command.getIndex(baseCfs);
         return index == null ? null : index.getBackingTable().orElse(null);
     }
 
-    public TableMetadata metadata()
-    {
+    public TableMetadata metadata() {
         return baseMetadata;
     }
 
-    public void close()
-    {
-        try
-        {
+    public void close() {
+        try {
             if (baseOp != null)
                 baseOp.close();
-        }
-        finally
-        {
-            if (indexController != null)
-            {
-                try
-                {
+        } finally {
+            if (indexController != null) {
+                try {
                     indexController.close();
-                }
-                finally
-                {
+                } finally {
                     writeContext.close();
                 }
             }
@@ -202,30 +184,25 @@ public class ReadExecutionController implements AutoCloseable
             addSample();
     }
 
-    public boolean isTrackingRepairedStatus()
-    {
+    public boolean isTrackingRepairedStatus() {
         return repairedDataInfo != RepairedDataInfo.NO_OP_REPAIRED_DATA_INFO;
     }
 
     @VisibleForTesting
-    public ByteBuffer getRepairedDataDigest()
-    {
+    public ByteBuffer getRepairedDataDigest() {
         return repairedDataInfo.getDigest();
     }
 
     @VisibleForTesting
-    public boolean isRepairedDataDigestConclusive()
-    {
+    public boolean isRepairedDataDigestConclusive() {
         return repairedDataInfo.isConclusive();
     }
-    
-    public RepairedDataInfo getRepairedDataInfo()
-    {
+
+    public RepairedDataInfo getRepairedDataInfo() {
         return repairedDataInfo;
     }
 
-    private void addSample()
-    {
+    private void addSample() {
         String cql = command.toCQLString();
         int timeMicros = (int) Math.min(TimeUnit.NANOSECONDS.toMicros(clock.now() - createdAtNanos), Integer.MAX_VALUE);
         ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(baseMetadata.id);
diff --git a/src/java/org/apache/cassandra/db/ReadResponse.java b/src/java/org/apache/cassandra/db/ReadResponse.java
index 9ef9128a..45c90ddd 100644
--- a/src/java/org/apache/cassandra/db/ReadResponse.java
+++ b/src/java/org/apache/cassandra/db/ReadResponse.java
@@ -33,88 +33,88 @@ import org.apache.cassandra.io.util.DataOutputPlus;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.schema.TableMetadata;
 import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public abstract class ReadResponse {
+
+    private static final Logger logger = LoggerFactory.getLogger(ReadResponse.class);
 
-public abstract class ReadResponse
-{
     // Serializer for single partition read response
     public static final IVersionedSerializer<ReadResponse> serializer = new Serializer();
 
-    protected ReadResponse()
-    {
+    protected ReadResponse() {
     }
 
-    public static ReadResponse createDataResponse(UnfilteredPartitionIterator data, ReadCommand command, RepairedDataInfo rdi)
-    {
+    public static ReadResponse createDataResponse(UnfilteredPartitionIterator data, ReadCommand command,
+            RepairedDataInfo rdi) {
         return new LocalDataResponse(data, command, rdi);
     }
 
-    public static ReadResponse createSimpleDataResponse(UnfilteredPartitionIterator data, ColumnFilter selection)
-    {
+    public static ReadResponse createSimpleDataResponse(UnfilteredPartitionIterator data, ColumnFilter selection) {
         return new LocalDataResponse(data, selection);
     }
 
     @VisibleForTesting
     public static ReadResponse createRemoteDataResponse(UnfilteredPartitionIterator data,
-                                                        ByteBuffer repairedDataDigest,
-                                                        boolean isRepairedDigestConclusive,
-                                                        ReadCommand command,
-                                                        int version)
-    {
+            ByteBuffer repairedDataDigest,
+            boolean isRepairedDigestConclusive,
+            ReadCommand command,
+            int version) {
         return new RemoteDataResponse(LocalDataResponse.build(data, command.columnFilter()),
-                                      repairedDataDigest,
-                                      isRepairedDigestConclusive,
-                                      version);
+                repairedDataDigest,
+                isRepairedDigestConclusive,
+                version);
     }
 
-
-    public static ReadResponse createDigestResponse(UnfilteredPartitionIterator data, ReadCommand command)
-    {
+    public static ReadResponse createDigestResponse(UnfilteredPartitionIterator data, ReadCommand command) {
         return new DigestResponse(makeDigest(data, command));
     }
 
     public abstract UnfilteredPartitionIterator makeIterator(ReadCommand command);
+
     public abstract ByteBuffer digest(ReadCommand command);
+
     public abstract ByteBuffer repairedDataDigest();
+
     public abstract boolean isRepairedDigestConclusive();
+
     public abstract boolean mayIncludeRepairedDigest();
 
     public abstract boolean isDigestResponse();
 
     /**
-     * Creates a string of the requested partition in this read response suitable for debugging.
+     * Creates a string of the requested partition in this read response suitable
+     * for debugging.
      */
-    public String toDebugString(ReadCommand command, DecoratedKey key)
-    {
+    public String toDebugString(ReadCommand command, DecoratedKey key) {
         if (isDigestResponse())
             return "Digest:0x" + ByteBufferUtil.bytesToHex(digest(command));
 
-        try (UnfilteredPartitionIterator iter = makeIterator(command))
-        {
-            while (iter.hasNext())
-            {
-                try (UnfilteredRowIterator partition = iter.next())
-                {
+        try (UnfilteredPartitionIterator iter = makeIterator(command)) {
+            while (iter.hasNext()) {
+                try (UnfilteredRowIterator partition = iter.next()) {
                     if (partition.partitionKey().equals(key))
                         return toDebugString(partition, command.metadata());
                 }
             }
         }
         return String.format("<key %s not found (repaired_digest=%s repaired_digest_conclusive=%s)>",
-                             key, ByteBufferUtil.bytesToHex(repairedDataDigest()), isRepairedDigestConclusive());
+                key, ByteBufferUtil.bytesToHex(repairedDataDigest()), isRepairedDigestConclusive());
     }
 
-    private String toDebugString(UnfilteredRowIterator partition, TableMetadata metadata)
-    {
+    private String toDebugString(UnfilteredRowIterator partition, TableMetadata metadata) {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("[%s] key=%s partition_deletion=%s columns=%s repaired_digest=%s repaired_digest_conclusive==%s",
-                                metadata,
-                                metadata.partitionKeyType.getString(partition.partitionKey().getKey()),
-                                partition.partitionLevelDeletion(),
-                                partition.columns(),
-                                ByteBufferUtil.bytesToHex(repairedDataDigest()),
-                                isRepairedDigestConclusive()
-                                ));
+        sb.append(String.format(
+                "[%s] key=%s partition_deletion=%s columns=%s repaired_digest=%s repaired_digest_conclusive==%s",
+                metadata,
+                metadata.partitionKeyType.getString(partition.partitionKey().getKey()),
+                partition.partitionLevelDeletion(),
+                partition.columns(),
+                ByteBufferUtil.bytesToHex(repairedDataDigest()),
+                isRepairedDigestConclusive()));
 
         if (partition.staticRow() != Rows.EMPTY_STATIC_ROW)
             sb.append("\n    ").append(partition.staticRow().toString(metadata, true));
@@ -125,84 +125,78 @@ public abstract class ReadResponse
         return sb.toString();
     }
 
-    protected static ByteBuffer makeDigest(UnfilteredPartitionIterator iterator, ReadCommand command)
-    {
-        Digest digest = Digest.forReadResponse();
-        UnfilteredPartitionIterators.digest(iterator, digest, command.digestVersion());
+    protected static ByteBuffer makeDigest(UnfilteredPartitionIterator iterator, ReadCommand command) {
+        Digest digest = Digest.forReadResponse(); // Create a digest object and set the digest method as MD5.
+        UnfilteredPartitionIterators.digest(iterator, digest, command.digestVersion()); // Generate diegst based on
         return ByteBuffer.wrap(digest.digest());
     }
 
-    private static class DigestResponse extends ReadResponse
-    {
+    private static class DigestResponse extends ReadResponse {
         private final ByteBuffer digest;
 
-        private DigestResponse(ByteBuffer digest)
-        {
+        private DigestResponse(ByteBuffer digest) {
             super();
             assert digest.hasRemaining();
             this.digest = digest;
         }
 
-        public UnfilteredPartitionIterator makeIterator(ReadCommand command)
-        {
+        public UnfilteredPartitionIterator makeIterator(ReadCommand command) {
             throw new UnsupportedOperationException();
         }
 
-        public boolean mayIncludeRepairedDigest()
-        {
+        public boolean mayIncludeRepairedDigest() {
             return false;
         }
 
-        public ByteBuffer repairedDataDigest()
-        {
+        public ByteBuffer repairedDataDigest() {
             throw new UnsupportedOperationException();
         }
 
-        public boolean isRepairedDigestConclusive()
-        {
+        public boolean isRepairedDigestConclusive() {
             throw new UnsupportedOperationException();
         }
 
-        public ByteBuffer digest(ReadCommand command)
-        {
-            // We assume that the digest is in the proper version, which bug excluded should be true since this is called with
-            // ReadCommand.digestVersion() as argument and that's also what we use to produce the digest in the first place.
-            // Validating it's the proper digest in this method would require sending back the digest version along with the
+        public ByteBuffer digest(ReadCommand command) {
+            // We assume that the digest is in the proper version, which bug excluded should
+            // be true since this is called with
+            // ReadCommand.digestVersion() as argument and that's also what we use to
+            // produce the digest in the first place.
+            // Validating it's the proper digest in this method would require sending back
+            // the digest version along with the
             // digest which would waste bandwith for little gain.
             return digest;
         }
 
-        public boolean isDigestResponse()
-        {
+        public ByteBuffer digest() {
+            // Tinoryj -> remove requirement of readcommand
+            return digest;
+        }
+
+        public boolean isDigestResponse() {
             return true;
         }
     }
 
     // built on the owning node responding to a query
-    private static class LocalDataResponse extends DataResponse
-    {
-        private LocalDataResponse(UnfilteredPartitionIterator iter, ReadCommand command, RepairedDataInfo rdi)
-        {
+    private static class LocalDataResponse extends DataResponse {
+        private LocalDataResponse(UnfilteredPartitionIterator iter, ReadCommand command, RepairedDataInfo rdi) {
             super(build(iter, command.columnFilter()),
-                  rdi.getDigest(), rdi.isConclusive(),
-                  MessagingService.current_version,
-                  DeserializationHelper.Flag.LOCAL);
+                    rdi.getDigest(), rdi.isConclusive(),
+                    MessagingService.current_version,
+                    DeserializationHelper.Flag.LOCAL);
         }
 
-        private LocalDataResponse(UnfilteredPartitionIterator iter, ColumnFilter selection)
-        {
-            super(build(iter, selection), null, false, MessagingService.current_version, DeserializationHelper.Flag.LOCAL);
+        private LocalDataResponse(UnfilteredPartitionIterator iter, ColumnFilter selection) {
+            super(build(iter, selection), null, false, MessagingService.current_version,
+                    DeserializationHelper.Flag.LOCAL);
         }
 
-        private static ByteBuffer build(UnfilteredPartitionIterator iter, ColumnFilter selection)
-        {
-            try (DataOutputBuffer buffer = new DataOutputBuffer())
-            {
-                UnfilteredPartitionIterators.serializerForIntraNode().serialize(iter, selection, buffer, MessagingService.current_version);
+        private static ByteBuffer build(UnfilteredPartitionIterator iter, ColumnFilter selection) {
+            try (DataOutputBuffer buffer = new DataOutputBuffer()) {
+                UnfilteredPartitionIterators.serializerForIntraNode().serialize(iter, selection, buffer,
+                        MessagingService.current_version);
                 return buffer.buffer();
-            }
-            catch (IOException e)
-            {
+            } catch (IOException e) {
                 // We're serializing in memory so this shouldn't happen
                 throw new RuntimeException(e);
             }
@@ -210,19 +204,17 @@ public abstract class ReadResponse
     }
 
     // built on the coordinator node receiving a response
-    private static class RemoteDataResponse extends DataResponse
-    {
+    private static class RemoteDataResponse extends DataResponse {
         protected RemoteDataResponse(ByteBuffer data,
-                                     ByteBuffer repairedDataDigest,
-                                     boolean isRepairedDigestConclusive,
-                                     int version)
-        {
-            super(data, repairedDataDigest, isRepairedDigestConclusive, version, DeserializationHelper.Flag.FROM_REMOTE);
+                ByteBuffer repairedDataDigest,
+                boolean isRepairedDigestConclusive,
+                int version) {
+            super(data, repairedDataDigest, isRepairedDigestConclusive, version,
+                    DeserializationHelper.Flag.FROM_REMOTE);
         }
     }
 
-    static abstract class DataResponse extends ReadResponse
-    {
+    static abstract class DataResponse extends ReadResponse {
         // TODO: can the digest be calculated over the raw bytes now?
         // The response, serialized in the current messaging version
         private final ByteBuffer data;
@@ -232,11 +224,10 @@ public abstract class ReadResponse
         private final DeserializationHelper.Flag flag;
 
         protected DataResponse(ByteBuffer data,
-                               ByteBuffer repairedDataDigest,
-                               boolean isRepairedDigestConclusive,
-                               int dataSerializationVersion,
-                               DeserializationHelper.Flag flag)
-        {
+                ByteBuffer repairedDataDigest,
+                boolean isRepairedDigestConclusive,
+                int dataSerializationVersion,
+                DeserializationHelper.Flag flag) {
             super();
             this.data = data;
             this.repairedDataDigest = repairedDataDigest;
@@ -245,87 +236,81 @@ public abstract class ReadResponse
             this.flag = flag;
         }
 
-        public UnfilteredPartitionIterator makeIterator(ReadCommand command)
-        {
-            try (DataInputBuffer in = new DataInputBuffer(data, true))
-            {
-                // Note that the command parameter shadows the 'command' field and this is intended because
-                // the later can be null (for RemoteDataResponse as those are created in the serializers and
-                // those don't have easy access to the command). This is also why we need the command as parameter here.
+        public UnfilteredPartitionIterator makeIterator(ReadCommand command) {
+            try (DataInputBuffer in = new DataInputBuffer(data, true)) {
+                // Note that the command parameter shadows the 'command' field and this is
+                // intended because
+                // the later can be null (for RemoteDataResponse as those are created in the
+                // serializers and
+                // those don't have easy access to the command). This is also why we need the
+                // command as parameter here.
                 return UnfilteredPartitionIterators.serializerForIntraNode().deserialize(in,
-                                                                                         dataSerializationVersion,
-                                                                                         command.metadata(),
-                                                                                         command.columnFilter(),
-                                                                                         flag);
-            }
-            catch (IOException e)
-            {
+                        dataSerializationVersion,
+                        command.metadata(),
+                        command.columnFilter(),
+                        flag);
+            } catch (IOException e) {
                 // We're deserializing in memory so this shouldn't happen
                 throw new RuntimeException(e);
             }
         }
 
-        public boolean mayIncludeRepairedDigest()
-        {
+        public boolean mayIncludeRepairedDigest() {
             return dataSerializationVersion >= MessagingService.VERSION_40;
         }
 
-        public ByteBuffer repairedDataDigest()
-        {
+        public ByteBuffer repairedDataDigest() {
             return repairedDataDigest;
         }
 
-        public boolean isRepairedDigestConclusive()
-        {
+        public boolean isRepairedDigestConclusive() {
             return isRepairedDigestConclusive;
         }
 
-        public ByteBuffer digest(ReadCommand command)
-        {
-            try (UnfilteredPartitionIterator iterator = makeIterator(command))
-            {
-                return makeDigest(iterator, command);
+        public ByteBuffer digest(ReadCommand command) {
+            try (UnfilteredPartitionIterator iterator = makeIterator(command)) {
+                ByteBuffer theDigest = makeDigest(iterator, command);
+                return theDigest;
             }
         }
 
-        public boolean isDigestResponse()
-        {
+        public boolean isDigestResponse() {
             return false;
         }
     }
 
-    private static class Serializer implements IVersionedSerializer<ReadResponse>
-    {
-        public void serialize(ReadResponse response, DataOutputPlus out, int version) throws IOException
-        {
+    private static class Serializer implements IVersionedSerializer<ReadResponse> {
+        public void serialize(ReadResponse response, DataOutputPlus out, int version) throws IOException {
             boolean isDigest = response instanceof DigestResponse;
-            ByteBuffer digest = isDigest ? ((DigestResponse)response).digest : ByteBufferUtil.EMPTY_BYTE_BUFFER;
+            ByteBuffer digest = isDigest ? ((DigestResponse) response).digest : ByteBufferUtil.EMPTY_BYTE_BUFFER;
             ByteBufferUtil.writeWithVIntLength(digest, out);
-            if (!isDigest)
-            {
-                // From 4.0, a coordinator may request additional info about the repaired data that
+            if (!isDigest) {
+                // From 4.0, a coordinator may request additional info about the repaired data
+                // that
                 // makes up the response, namely a digest generated from the repaired data and a
-                // flag indicating our level of confidence in that digest. The digest may be considered
-                // inconclusive if it may have been affected by some unrepaired data during read.
+                // flag indicating our level of confidence in that digest. The digest may be
+                // considered
+                // inconclusive if it may have been affected by some unrepaired data during
+                // read.
                 // e.g. some sstables read during this read were involved in pending but not yet
-                // committed repair sessions or an unrepaired partition tombstone meant that not all
+                // committed repair sessions or an unrepaired partition tombstone meant that not
+                // all
                 // repaired sstables were read (but they might be on other replicas).
-                // If the coordinator did not request this info, the response contains an empty digest
+                // If the coordinator did not request this info, the response contains an empty
+                // digest
                 // and a true for the isConclusive flag.
                 // If the messaging version is < 4.0, these are omitted altogether.
-                if (version >= MessagingService.VERSION_40)
-                {
+                if (version >= MessagingService.VERSION_40) {
                     ByteBufferUtil.writeWithVIntLength(response.repairedDataDigest(), out);
                     out.writeBoolean(response.isRepairedDigestConclusive());
                 }
 
-                ByteBuffer data = ((DataResponse)response).data;
+                ByteBuffer data = ((DataResponse) response).data;
                 ByteBufferUtil.writeWithVIntLength(data, out);
             }
         }
 
-        public ReadResponse deserialize(DataInputPlus in, int version) throws IOException
-        {
+        public ReadResponse deserialize(DataInputPlus in, int version) throws IOException {
             ByteBuffer digest = ByteBufferUtil.readWithVIntLength(in);
             if (digest.hasRemaining())
                 return new DigestResponse(digest);
@@ -335,13 +320,10 @@ public abstract class ReadResponse
             // whether or not the digest may be influenced by unrepaired/pending
             // repaired data
             boolean repairedDigestConclusive;
-            if (version >= MessagingService.VERSION_40)
-            {
+            if (version >= MessagingService.VERSION_40) {
                 digest = ByteBufferUtil.readWithVIntLength(in);
                 repairedDigestConclusive = in.readBoolean();
-            }
-            else
-            {
+            } else {
                 digest = ByteBufferUtil.EMPTY_BYTE_BUFFER;
                 repairedDigestConclusive = true;
             }
@@ -350,27 +332,27 @@ public abstract class ReadResponse
             return new RemoteDataResponse(data, digest, repairedDigestConclusive, version);
         }
 
-        public long serializedSize(ReadResponse response, int version)
-        {
+        public long serializedSize(ReadResponse response, int version) {
             boolean isDigest = response instanceof DigestResponse;
-            ByteBuffer digest = isDigest ? ((DigestResponse)response).digest : ByteBufferUtil.EMPTY_BYTE_BUFFER;
+            ByteBuffer digest = isDigest ? ((DigestResponse) response).digest : ByteBufferUtil.EMPTY_BYTE_BUFFER;
             long size = ByteBufferUtil.serializedSizeWithVIntLength(digest);
 
-            if (!isDigest)
-            {
-                // From 4.0, a coordinator may request an additional info about the repaired data
+            if (!isDigest) {
+                // From 4.0, a coordinator may request an additional info about the repaired
+                // data
                 // that makes up the response.
-                if (version >= MessagingService.VERSION_40)
-                {
+                if (version >= MessagingService.VERSION_40) {
                     size += ByteBufferUtil.serializedSizeWithVIntLength(response.repairedDataDigest());
                     size += 1;
                 }
 
-                // In theory, we should deserialize/re-serialize if the version asked is different from the current
-                // version as the content could have a different serialization format. So far though, we haven't made
+                // In theory, we should deserialize/re-serialize if the version asked is
+                // different from the current
+                // version as the content could have a different serialization format. So far
+                // though, we haven't made
                 // change to partition iterators serialization since 3.0 so we skip this.
                 assert version >= MessagingService.VERSION_30;
-                ByteBuffer data = ((DataResponse)response).data;
+                ByteBuffer data = ((DataResponse) response).data;
                 size += ByteBufferUtil.serializedSizeWithVIntLength(data);
             }
             return size;
diff --git a/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java b/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java
index cf2b0c40..41a01212 100644
--- a/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java
+++ b/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java
@@ -349,27 +349,6 @@ public class SinglePartitionReadCommand extends ReadCommand implements SinglePar
 
     @Override
     protected SinglePartitionReadCommand copyAsDigestQuery() {
-        logger.debug("[Tinoryj] SinglePartitionReadCommand copyAsDigestQuery() for partation key: {}",
-                partitionKey().getRawKey(metadata()));
-        return create(true,
-                digestVersion(),
-                acceptsTransient(),
-                metadata(),
-                nowInSec(),
-                columnFilter(),
-                rowFilter(),
-                limits(),
-                partitionKey(),
-                clusteringIndexFilter(),
-                indexMetadata(),
-                isTrackingWarnings());
-    }
-
-    @Override
-    protected SinglePartitionReadCommand copyAsDigestQuery(int replicationIDIndicator) {
-        logger.debug(
-                "[Tinoryj] SinglePartitionReadCommand copyAsDigestQuery() for partation key: {}, the replication ID indicator is {}",
-                partitionKey().getRawKey(metadata()), replicationIDIndicator);
         return create(true,
                 digestVersion(),
                 acceptsTransient(),
diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
index 4a6e13d8..0f3367b9 100644
--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
@@ -209,10 +209,10 @@ public class CompactionManager implements CompactionManagerMBean {
     public List<Future<?>> submitBackground(final ColumnFamilyStore cfs) {
         if (cfs.isAutoCompactionDisabled()) {
             logger.trace("Autocompaction is disabled");
-            logger.debug("[Tinoryj] Autocompaction is enabled, current cfs = {}", cfs.name);
             return Collections.emptyList();
-        }else {
-            // logger.debug("[Tinoryj] Autocompaction is enabled, current cfs = {}", cfs.name);
+        } else {
+            // logger.debug("[Tinoryj] Autocompaction is enabled, current cfs = {}",
+            // cfs.name);
         }
 
         /**
@@ -317,7 +317,7 @@ public class CompactionManager implements CompactionManagerMBean {
                 if (task == null) {
                     if (DatabaseDescriptor.automaticSSTableUpgrade())
                         ranCompaction = maybeRunUpgradeTask(strategy);
-                } else if(task.isContainReplicationTransferredToErasureCoding){
+                } else if (task.isContainReplicationTransferredToErasureCoding) {
                     logger.debug("rymDebug[transferred]: this task contains transferred sstables.");
                     List<TransferredSSTableKeyRange> TransferredSSTableKeyRanges = ((LeveledCompactionTask) task).transferredSSTableKeyRanges;
                     task.execute(active, TransferredSSTableKeyRanges);
@@ -329,7 +329,7 @@ public class CompactionManager implements CompactionManagerMBean {
             } finally {
                 compactingCF.remove(cfs);
             }
-            if (ranCompaction){ // only submit background if we actually ran a compaction - otherwise we end up
+            if (ranCompaction) { // only submit background if we actually ran a compaction - otherwise we end up
                 // in an infinite loop submitting noop background tasks
                 submitBackground(cfs);
             }
@@ -429,7 +429,7 @@ public class CompactionManager implements CompactionManagerMBean {
     }
 
     // [CASSANDRAEC]
-    private AllSSTableOpStatus rewriteSSTables(final ColumnFamilyStore cfs, 
+    private AllSSTableOpStatus rewriteSSTables(final ColumnFamilyStore cfs,
             final DecoratedKey first,
             final DecoratedKey last,
             List<SSTableReader> rewriteSSTables,
@@ -439,7 +439,8 @@ public class CompactionManager implements CompactionManagerMBean {
             final OneSSTableOperation operation,
             OperationType operationType)
             throws ExecutionException, InterruptedException {
-        // logger.info("rymDebug: Starting {} for {}.{}", operationType, cfs.keyspace.getName(), cfs.getTableName());
+        // logger.info("rymDebug: Starting {} for {}.{}", operationType,
+        // cfs.keyspace.getName(), cfs.getTableName());
         List<LifecycleTransaction> transactions = new ArrayList<>();
         List<Future<?>> futures = new ArrayList<>();
         try (txn) {
@@ -449,15 +450,15 @@ public class CompactionManager implements CompactionManagerMBean {
             rewriteSSTables = Lists.newArrayList(operation.filterSSTables(txn));
             int originalRewriteSSTablesNum = rewriteSSTables.size();
             if (Iterables.isEmpty(rewriteSSTables)) {
-                // logger.info("rymDebug: No sstables to {} for {}.{}", operationType.name(), cfs.keyspace.getName(), cfs.name);
+                // logger.info("rymDebug: No sstables to {} for {}.{}", operationType.name(),
+                // cfs.keyspace.getName(), cfs.name);
                 return AllSSTableOpStatus.SUCCESSFUL;
             }
 
             transactions.add(txn);
             Callable<Object> callable = new Callable<Object>() {
                 @Override
-                public Object call() throws Exception
-                {
+                public Object call() throws Exception {
                     operation.execute(txn);
                     return this;
                 }
@@ -469,12 +470,12 @@ public class CompactionManager implements CompactionManagerMBean {
                 return AllSSTableOpStatus.ABORTED;
 
             FBUtilities.waitOnFutures(futures);
-            if(txn.originals().isEmpty()) {
+            if (txn.originals().isEmpty()) {
                 logger.info("Finished rewrite {} sstables successfully!", originalRewriteSSTablesNum);
             } else {
                 logger.warn("Still remaining {} sstables in this transaction {}, original sstables number is {}",
-                 txn.originals().size(), txn.opId(), rewriteSSTables.size());
-                txn.removeAll(rewriteSSTables);                
+                        txn.originals().size(), txn.opId(), rewriteSSTables.size());
+                txn.removeAll(rewriteSSTables);
             }
             return AllSSTableOpStatus.SUCCESSFUL;
 
@@ -583,7 +584,7 @@ public class CompactionManager implements CompactionManagerMBean {
     // rewrite sstables based source decorated keys
     public AllSSTableOpStatus performSSTableRewrite(final ColumnFamilyStore cfs,
             final DecoratedKey first,
-            final DecoratedKey last, 
+            final DecoratedKey last,
             List<SSTableReader> sstables,
             // SSTableReader ecSSTable,
             ECMetadata ecMetadata, String fileNamePrefix,
@@ -613,63 +614,65 @@ public class CompactionManager implements CompactionManagerMBean {
                             (sstable.compression && metadata.params.compression
                                     .equals(sstable.getCompressionMetadata().parameters))))
                 return false;
-            
+
             // Skip if sstable contain EC_Metadata
-            if(Files.exists(Paths.get(sstable.descriptor.filenameFor(Component.EC_METADATA))))
+            if (Files.exists(Paths.get(sstable.descriptor.filenameFor(Component.EC_METADATA))))
                 return false;
 
-            
             return true;
         }, jobs);
     }
 
     public AllSSTableOpStatus performSSTableRewrite(final ColumnFamilyStore cfs,
             final DecoratedKey first,
-            final DecoratedKey last, 
-            List<SSTableReader> sstables, 
+            final DecoratedKey last,
+            List<SSTableReader> sstables,
             // SSTableReader ecSSTable,
             ECMetadata ecMetadata, String fileNamePrefix,
             final LifecycleTransaction updateTxn,
             Predicate<SSTableReader> sstableFilter,
             int jobs) throws InterruptedException, ExecutionException {
         // return rewriteSSTables(cfs, sourceKeys, sstables, OperationType.COMPACTION);
-        return rewriteSSTables(cfs, first, last, sstables, ecMetadata, fileNamePrefix, updateTxn, new OneSSTableOperation() {
-            @Override
-            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction) {
-                List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());
-                Collections.sort(sortedSSTables, SSTableReader.sizeComparator.reversed());
-                Iterator<SSTableReader> iter = sortedSSTables.iterator();
-                // final Set<SSTableReader> compacting = cfs.getTracker().getCompacting();
-                while (iter.hasNext()) {
-                    SSTableReader sstable = iter.next();
-                    if (!sstableFilter.test(sstable)) {
-                        logger.warn(BLUE+"rymWarning: sstable {} is not qualified, cannot be rewritten!!!", sstable.getFilename()+RESET);
-                        transaction.cancel(sstable);
-                        iter.remove();
-                    } 
-                    // else if (compacting.contains(sstable)) {
-                    //     logger.warn(BLUE+"rymWarning: sstable {} is compacting, cannot be rewritten!!!", sstable.getFilename()+RESET);
-                    //     transaction.cancel(sstable);
-                    //     iter.remove();
-                    // }
-                }
-                return sortedSSTables;
-            }
+        return rewriteSSTables(cfs, first, last, sstables, ecMetadata, fileNamePrefix, updateTxn,
+                new OneSSTableOperation() {
+                    @Override
+                    public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction) {
+                        List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());
+                        Collections.sort(sortedSSTables, SSTableReader.sizeComparator.reversed());
+                        Iterator<SSTableReader> iter = sortedSSTables.iterator();
+                        // final Set<SSTableReader> compacting = cfs.getTracker().getCompacting();
+                        while (iter.hasNext()) {
+                            SSTableReader sstable = iter.next();
+                            if (!sstableFilter.test(sstable)) {
+                                logger.warn(BLUE + "rymWarning: sstable {} is not qualified, cannot be rewritten!!!",
+                                        sstable.getFilename() + RESET);
+                                transaction.cancel(sstable);
+                                iter.remove();
+                            }
+                            // else if (compacting.contains(sstable)) {
+                            // logger.warn(BLUE+"rymWarning: sstable {} is compacting, cannot be
+                            // rewritten!!!", sstable.getFilename()+RESET);
+                            // transaction.cancel(sstable);
+                            // iter.remove();
+                            // }
+                        }
+                        return sortedSSTables;
+                    }
 
-            @Override
-            public void execute(LifecycleTransaction txn) {
-                
-                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC,
-                        Long.MAX_VALUE);
-                task.setUserDefined(true);
-                task.setCompactionType(OperationType.COMPACTION);
-                // SSTableReader ecSSTable = SSTableReader.openECSSTable(ecMetadata, cfs,
-                // fileNamePrefix);
-                // logger.debug("rymDebug: open ec sstable {} successfully.",
-                // ecSSTable.descriptor);
-                task.execute(active, first, last, ecMetadata, fileNamePrefix);
-            }
-        }, OperationType.COMPACTION);
+                    @Override
+                    public void execute(LifecycleTransaction txn) {
+
+                        AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC,
+                                Long.MAX_VALUE);
+                        task.setUserDefined(true);
+                        task.setCompactionType(OperationType.COMPACTION);
+                        // SSTableReader ecSSTable = SSTableReader.openECSSTable(ecMetadata, cfs,
+                        // fileNamePrefix);
+                        // logger.debug("rymDebug: open ec sstable {} successfully.",
+                        // ecSSTable.descriptor);
+                        task.execute(active, first, last, ecMetadata, fileNamePrefix);
+                    }
+                }, OperationType.COMPACTION);
     }
 
     /**
@@ -889,7 +892,8 @@ public class CompactionManager implements CompactionManagerMBean {
             }
 
             @Override
-            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                    throws Exception {
                 // TODO Auto-generated method stub
                 throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
             }
@@ -1091,7 +1095,8 @@ public class CompactionManager implements CompactionManagerMBean {
                 }
 
                 @Override
-                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                        throws Exception {
                     // TODO Auto-generated method stub
                     throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
                 }
@@ -1150,7 +1155,8 @@ public class CompactionManager implements CompactionManagerMBean {
                 }
 
                 @Override
-                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+                protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                        throws Exception {
                     // TODO Auto-generated method stub
                     throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
                 }
@@ -1344,7 +1350,8 @@ public class CompactionManager implements CompactionManagerMBean {
             }
 
             @Override
-            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                    throws Exception {
                 // TODO Auto-generated method stub
                 throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
             }
diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java b/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java
index 54b9be8e..10c30b42 100644
--- a/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java
+++ b/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java
@@ -324,9 +324,7 @@ public class CompactionStrategyManager implements INotificationConsumer {
             fanout = (repaired.first() instanceof LeveledCompactionStrategy)
                     ? ((LeveledCompactionStrategy) repaired.first()).getLevelFanoutSize()
                     : LeveledCompactionStrategy.DEFAULT_LEVEL_FANOUT_SIZE;
-            logger.debug("[Tinoryj] Compaction strategy fanout is {}", fanout);
             maxSSTableSizeBytes = repaired.first().getMaxSSTableBytes();
-            logger.debug("[Tinoryj] Compaction strategy maxSSTableSizeBytes is {}", maxSSTableSizeBytes);
             name = repaired.first().getName();
         } finally {
             writeLock.unlock();
@@ -570,15 +568,15 @@ public class CompactionStrategyManager implements INotificationConsumer {
         maybeReloadDiskBoundaries();
         readLock.lock();
         try {
-            if(repaired.first() instanceof LeveledCompactionStrategy) {
+            if (repaired.first() instanceof LeveledCompactionStrategy) {
                 Set<SSTableReader> res = new HashSet<SSTableReader>();
                 for (AbstractCompactionStrategy strategy : getAllStrategies()) {
                     res = Sets.union(res, ((LeveledCompactionStrategy) strategy).getSStablesForLevel(sstableLevel));
                 }
-                
+
                 return res;
             }
-            
+
         } finally {
             readLock.unlock();
         }
@@ -806,8 +804,10 @@ public class CompactionStrategyManager implements INotificationConsumer {
         maybeReloadDiskBoundaries();
         List<ISSTableScanner> scanners = new ArrayList<>(sstables.size());
         readLock.lock();
-        // logger.debug("rymDebug: cfName is {}, sstable level is {}, BigTableScanner.getscanner3",
-        //              sstables.iterator().next().getColumnFamilyName(), sstables.iterator().next().getSSTableLevel());
+        // logger.debug("rymDebug: cfName is {}, sstable level is {},
+        // BigTableScanner.getscanner3",
+        // sstables.iterator().next().getColumnFamilyName(),
+        // sstables.iterator().next().getSSTableLevel());
         try {
             List<GroupedSSTableContainer> sstableGroups = groupSSTables(sstables);
             for (int i = 0; i < holders.size(); i++) {
@@ -817,11 +817,13 @@ public class CompactionStrategyManager implements INotificationConsumer {
             }
 
             // if(scanners.size() != sstables.size()) {
-            //     for (int i = 0; i < holders.size(); i++) {
-            //         if( sstableGroups.get(i).groups[0] != null ) 
-            //             logger.debug("holder is {}, group.sstable num is {}", holders.get(i).getClass(), sstableGroups.get(i).groups[0].size());
-            //     }
-            //     logger.debug("rymDebug: sstableGroups num is {}, sstables num is {}, scanner num is {}", sstableGroups.size(), sstables.size(), scanners.size());
+            // for (int i = 0; i < holders.size(); i++) {
+            // if( sstableGroups.get(i).groups[0] != null )
+            // logger.debug("holder is {}, group.sstable num is {}",
+            // holders.get(i).getClass(), sstableGroups.get(i).groups[0].size());
+            // }
+            // logger.debug("rymDebug: sstableGroups num is {}, sstables num is {}, scanner
+            // num is {}", sstableGroups.size(), sstables.size(), scanners.size());
             // }
 
         } catch (PendingRepairManager.IllegalSSTableArgumentException e) {
@@ -836,8 +838,10 @@ public class CompactionStrategyManager implements INotificationConsumer {
             Collection<Range<Token>> ranges) {
         while (true) {
             try {
-                // logger.debug("rymDebug: cfName is {}, sstable level is {}, BigTableScanner.getscanner2",
-                //      sstables.iterator().next().getColumnFamilyName(), sstables.iterator().next().getSSTableLevel());
+                // logger.debug("rymDebug: cfName is {}, sstable level is {},
+                // BigTableScanner.getscanner2",
+                // sstables.iterator().next().getColumnFamilyName(),
+                // sstables.iterator().next().getSSTableLevel());
                 return maybeGetScanners(sstables, ranges);
             } catch (ConcurrentModificationException e) {
                 logger.debug("SSTable repairedAt/pendingRepaired values changed while getting scanners");
@@ -846,8 +850,10 @@ public class CompactionStrategyManager implements INotificationConsumer {
     }
 
     public AbstractCompactionStrategy.ScannerList getScanners(Collection<SSTableReader> sstables) {
-        // logger.debug("rymDebug: cfName is {}, sstable level is {}, BigTableScanner.getscanner1",
-        //              sstables.iterator().next().getColumnFamilyName(), sstables.iterator().next().getSSTableLevel());
+        // logger.debug("rymDebug: cfName is {}, sstable level is {},
+        // BigTableScanner.getscanner1",
+        // sstables.iterator().next().getColumnFamilyName(),
+        // sstables.iterator().next().getSSTableLevel());
         return getScanners(sstables, null);
     }
 
diff --git a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java b/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
index 5a3f9d28..08edd149 100644
--- a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
+++ b/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
@@ -118,11 +118,11 @@ public class LeveledManifest {
                 long modificationTime;
                 if (ssTableReader.getFileExistFlagFor(Component.DATA)) {
                     modificationTime = ssTableReader.getCreationTimeFor(Component.DATA);
-                } else if (ssTableReader.getFileExistFlagFor(Component.EC_METADATA)){
+                } else if (ssTableReader.getFileExistFlagFor(Component.EC_METADATA)) {
                     modificationTime = ssTableReader.getCreationTimeFor(Component.EC_METADATA);
-                }else {
-                    modificationTime=0;
-                    logger.debug("[Tinoryj] could not found both EC metadata and data, modify time to 0");
+                } else {
+                    modificationTime = 0;
+                    logger.debug("[Tinoryj] could not found both EC metadata and data, set modify time to 0");
                 }
 
                 if (modificationTime >= maxModificationTime) {
@@ -181,7 +181,8 @@ public class LeveledManifest {
         if (level == 0)
             return 4L * maxSSTableSizeInBytes;
         double bytes = Math.pow(levelFanoutSize, level) * maxSSTableSizeInBytes;
-        // logger.debug("[Tinoryj] generate new level size for level = {}, new size = {}", level, bytes/1024/1024);
+        // logger.debug("[Tinoryj] generate new level size for level = {}, new size =
+        // {}", level, bytes/1024/1024);
         if (bytes > Long.MAX_VALUE)
             throw new RuntimeException("At most " + Long.MAX_VALUE
                     + " bytes may be in a compaction level; your maxSSTableSize must be absurdly high to compute "
@@ -307,11 +308,11 @@ public class LeveledManifest {
         // [CASSANDRAEC]
         // TODO: Wrong place for remove sstables of transient sstables.
         // for(SSTableReader sstable : candidates) {
-        //     if(!sstable.getColumnFamilyName().equals("usertable") && sstable.isReplicationTransferredToErasureCoding()) {
-        //         candidates.remove(sstable);
-        //     }
+        // if(!sstable.getColumnFamilyName().equals("usertable") &&
+        // sstable.isReplicationTransferredToErasureCoding()) {
+        // candidates.remove(sstable);
+        // }
         // }
-
 
         return new CompactionCandidate(candidates, getNextLevel(candidates), maxSSTableSizeInBytes);
     }
@@ -481,22 +482,24 @@ public class LeveledManifest {
         Bounds<Token> promotedBounds = new Bounds<>(start, end);
 
         for (Map.Entry<SSTableReader, Bounds<Token>> pair : sstables.entrySet()) {
-            // we cannot select the sstables that isReplicationTransferredToErasureCoding is (true) and 
+            // we cannot select the sstables that isReplicationTransferredToErasureCoding is
+            // (true) and
             // this sstable is belong to primary LSM tree
             if (pair.getValue().intersects(promotedBounds)) {
 
-                if((pair.getKey().isReplicationTransferredToErasureCoding() &&
-                    pair.getKey().getColumnFamilyName().equals("usertable"))){
+                if ((pair.getKey().isReplicationTransferredToErasureCoding() &&
+                        pair.getKey().getColumnFamilyName().equals("usertable"))) {
                     logger.debug("rymDebug: we cannot select sstable {}", pair.getKey().descriptor);
                 } else {
                     overlapped.add(pair.getKey());
-                    if(pair.getKey().isReplicationTransferredToErasureCoding()) {
-                        logger.debug("rymDebug[transferred]: select a transferred sstable {}", pair.getKey().descriptor);
+                    if (pair.getKey().isReplicationTransferredToErasureCoding()) {
+                        logger.debug("rymDebug[transferred]: select a transferred sstable {}",
+                                pair.getKey().descriptor);
                     }
                 }
-                
+
             }
-                
+
         }
         return overlapped;
     }
@@ -623,7 +626,9 @@ public class LeveledManifest {
     }
 
     /**
-     * A leveled compaction strategy implemented by the idea of RocksDB for CASSANDRAEC.
+     * A leveled compaction strategy implemented by the idea of RocksDB for
+     * CASSANDRAEC.
+     * 
      * @return highest-priority sstables to compact for the given level.
      *         If no compactions are possible (because of concurrent compactions or
      *         because some sstables are excluded
@@ -718,7 +723,8 @@ public class LeveledManifest {
         // look for a non-suspect keyspace to compact with, starting with where we left
         // off last time,
         // and wrapping back to the beginning of the generation if necessary
-        // Map<SSTableReader, Bounds<Token>> sstablesNextLevel = genBounds(generations.get(level + 1));
+        // Map<SSTableReader, Bounds<Token>> sstablesNextLevel =
+        // genBounds(generations.get(level + 1));
         // TODO: for primary node
         // [CASSANDRAEC]
         Set<SSTableReader> sstablesNextLevel = generations.get(level + 1);
@@ -726,7 +732,7 @@ public class LeveledManifest {
         Iterator<SSTableReader> levelIterator = generations.wrappingIterator(level, lastCompactedSSTables[level]);
         while (levelIterator.hasNext()) {
             SSTableReader sstable = levelIterator.next();
-            if(cfs.getColumnFamilyName().equals("usertable") && sstable.isReplicationTransferredToErasureCoding())
+            if (cfs.getColumnFamilyName().equals("usertable") && sstable.isReplicationTransferredToErasureCoding())
                 continue;
             Token startInputToken = sstable.first.getToken();
             Token endInputToken = sstable.last.getToken();
@@ -737,10 +743,12 @@ public class LeveledManifest {
             Token startOutputToken = outputLevel.iterator().next().first.getToken();
             Token endOutputToken = outputLevel.iterator().next().last.getToken();
             for (SSTableReader sst : outputLevel) {
-                startOutputToken = startOutputToken.compareTo(sst.first.getToken()) > 0 ? sst.first.getToken() : startOutputToken;
-                endOutputToken = endOutputToken.compareTo(sst.last.getToken()) < 0 ? sst.last.getToken() : endOutputToken;
+                startOutputToken = startOutputToken.compareTo(sst.first.getToken()) > 0 ? sst.first.getToken()
+                        : startOutputToken;
+                endOutputToken = endOutputToken.compareTo(sst.last.getToken()) < 0 ? sst.last.getToken()
+                        : endOutputToken;
             }
-            Set<SSTableReader> candidates = Sets.union(outputLevel, 
+            Set<SSTableReader> candidates = Sets.union(outputLevel,
                     overlapping(startOutputToken, endOutputToken, sstablesCurrentLevel));
 
             if (Iterables.any(candidates, SSTableReader::isMarkedSuspect))
@@ -753,7 +761,6 @@ public class LeveledManifest {
         return Collections.emptyList();
     }
 
-
     private Set<SSTableReader> getCompactingL0() {
         Set<SSTableReader> sstables = new HashSet<>();
         Set<SSTableReader> levelSSTables = new HashSet<>(generations.get(0));
diff --git a/src/java/org/apache/cassandra/db/partitions/UnfilteredPartitionIterators.java b/src/java/org/apache/cassandra/db/partitions/UnfilteredPartitionIterators.java
index 753d7f9c..e63bb9db 100644
--- a/src/java/org/apache/cassandra/db/partitions/UnfilteredPartitionIterators.java
+++ b/src/java/org/apache/cassandra/db/partitions/UnfilteredPartitionIterators.java
@@ -36,40 +36,43 @@ import org.apache.cassandra.utils.TimeUUID;
 /**
  * Static methods to work with partition iterators.
  */
-public abstract class UnfilteredPartitionIterators
-{
+public abstract class UnfilteredPartitionIterators {
     private static final Serializer serializer = new Serializer();
 
-    private static final Comparator<UnfilteredRowIterator> partitionComparator = (p1, p2) -> p1.partitionKey().compareTo(p2.partitionKey());
+    private static final Comparator<UnfilteredRowIterator> partitionComparator = (p1, p2) -> p1.partitionKey()
+            .compareTo(p2.partitionKey());
 
-    private UnfilteredPartitionIterators() {}
+    private UnfilteredPartitionIterators() {
+    }
+
+    public interface MergeListener {
+        public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey,
+                List<UnfilteredRowIterator> versions);
 
-    public interface MergeListener
-    {
-        public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions);
-        public default void close() {}
+        public default void close() {
+        }
 
         public static MergeListener NOOP = (partitionKey, versions) -> UnfilteredRowIterators.MergeListener.NOOP;
     }
 
     @SuppressWarnings("resource") // The created resources are returned right away
-    public static UnfilteredRowIterator getOnlyElement(final UnfilteredPartitionIterator iter, SinglePartitionReadCommand command)
-    {
+    public static UnfilteredRowIterator getOnlyElement(final UnfilteredPartitionIterator iter,
+            SinglePartitionReadCommand command) {
         // If the query has no results, we'll get an empty iterator, but we still
         // want a RowIterator out of this method, so we return an empty one.
         UnfilteredRowIterator toReturn = iter.hasNext()
-                              ? iter.next()
-                              : EmptyIterators.unfilteredRow(command.metadata(),
-                                                             command.partitionKey(),
-                                                             command.clusteringIndexFilter().isReversed());
+                ? iter.next()
+                : EmptyIterators.unfilteredRow(command.metadata(),
+                        command.partitionKey(),
+                        command.clusteringIndexFilter().isReversed());
 
-        // Note that in general, we should wrap the result so that it's close method actually
+        // Note that in general, we should wrap the result so that it's close method
+        // actually
         // close the whole UnfilteredPartitionIterator.
-        class Close extends Transformation
-        {
-            public void onPartitionClose()
-            {
-                // asserting this only now because it bothers Serializer if hasNext() is called before
+        class Close extends Transformation {
+            public void onPartitionClose() {
+                // asserting this only now because it bothers Serializer if hasNext() is called
+                // before
                 // the previously returned iterator hasn't been fully consumed.
                 boolean hadNext = iter.hasNext();
                 iter.close();
@@ -79,16 +82,14 @@ public abstract class UnfilteredPartitionIterators
         return Transformation.apply(toReturn, new Close());
     }
 
-    public static UnfilteredPartitionIterator concat(final List<UnfilteredPartitionIterator> iterators)
-    {
+    public static UnfilteredPartitionIterator concat(final List<UnfilteredPartitionIterator> iterators) {
         if (iterators.size() == 1)
             return iterators.get(0);
 
-        class Extend implements MorePartitions<UnfilteredPartitionIterator>
-        {
+        class Extend implements MorePartitions<UnfilteredPartitionIterator> {
             int i = 1;
-            public UnfilteredPartitionIterator moreContents()
-            {
+
+            public UnfilteredPartitionIterator moreContents() {
                 if (i >= iterators.size())
                     return null;
                 return iterators.get(i++);
@@ -97,88 +98,79 @@ public abstract class UnfilteredPartitionIterators
         return MorePartitions.extend(iterators.get(0), new Extend());
     }
 
-    public static PartitionIterator filter(final UnfilteredPartitionIterator iterator, final int nowInSec)
-    {
+    public static PartitionIterator filter(final UnfilteredPartitionIterator iterator, final int nowInSec) {
         return FilteredPartitions.filter(iterator, nowInSec);
     }
 
     @SuppressWarnings("resource")
-    public static UnfilteredPartitionIterator merge(final List<? extends UnfilteredPartitionIterator> iterators, final MergeListener listener, Boolean isCassandraEC, TimeUUID taskId)
-    {
+    public static UnfilteredPartitionIterator merge(final List<? extends UnfilteredPartitionIterator> iterators,
+            final MergeListener listener, Boolean isCassandraEC, TimeUUID taskId) {
         assert !iterators.isEmpty();
 
         final TableMetadata metadata = iterators.get(0).metadata();
 
-        final MergeIterator<UnfilteredRowIterator, UnfilteredRowIterator> merged = MergeIterator.get(iterators, partitionComparator, new MergeIterator.Reducer<UnfilteredRowIterator, UnfilteredRowIterator>()
-        {
-            private final List<UnfilteredRowIterator> toMerge = new ArrayList<>(iterators.size());
+        final MergeIterator<UnfilteredRowIterator, UnfilteredRowIterator> merged = MergeIterator.get(iterators,
+                partitionComparator, new MergeIterator.Reducer<UnfilteredRowIterator, UnfilteredRowIterator>() {
+                    private final List<UnfilteredRowIterator> toMerge = new ArrayList<>(iterators.size());
 
-            private DecoratedKey partitionKey;
-            private boolean isReverseOrder;
+                    private DecoratedKey partitionKey;
+                    private boolean isReverseOrder;
 
-            public void reduce(int idx, UnfilteredRowIterator current)
-            {
-                partitionKey = current.partitionKey();
-                isReverseOrder = current.isReverseOrder();
+                    public void reduce(int idx, UnfilteredRowIterator current) {
+                        partitionKey = current.partitionKey();
+                        isReverseOrder = current.isReverseOrder();
 
-                // Note that because the MergeListener cares about it, we want to preserve the index of the iterator.
-                // Non-present iterator will thus be set to empty in getReduced.
-                toMerge.set(idx, current);
-            }
-
-            @SuppressWarnings("resource")
-            protected UnfilteredRowIterator getReduced()
-            {
-                UnfilteredRowIterators.MergeListener rowListener = listener == null
-                                                                 ? null
-                                                                 : listener.getRowMergeListener(partitionKey, toMerge);
-
-                // Make a single empty iterator object to merge, we don't need toMerge.size() copiess
-                UnfilteredRowIterator empty = null;
-
-                // Replace nulls by empty iterators
-                for (int i = 0; i < toMerge.size(); i++)
-                {
-                    if (toMerge.get(i) == null)
-                    {
-                        if (null == empty)
-                            empty = EmptyIterators.unfilteredRow(metadata, partitionKey, isReverseOrder);
-                        toMerge.set(i, empty);
+                        // Note that because the MergeListener cares about it, we want to preserve the
+                        // index of the iterator.
+                        // Non-present iterator will thus be set to empty in getReduced.
+                        toMerge.set(idx, current);
                     }
-                }
 
-                return UnfilteredRowIterators.merge(toMerge, rowListener);
-            }
+                    @SuppressWarnings("resource")
+                    protected UnfilteredRowIterator getReduced() {
+                        UnfilteredRowIterators.MergeListener rowListener = listener == null
+                                ? null
+                                : listener.getRowMergeListener(partitionKey, toMerge);
+
+                        // Make a single empty iterator object to merge, we don't need toMerge.size()
+                        // copiess
+                        UnfilteredRowIterator empty = null;
+
+                        // Replace nulls by empty iterators
+                        for (int i = 0; i < toMerge.size(); i++) {
+                            if (toMerge.get(i) == null) {
+                                if (null == empty)
+                                    empty = EmptyIterators.unfilteredRow(metadata, partitionKey, isReverseOrder);
+                                toMerge.set(i, empty);
+                            }
+                        }
+
+                        return UnfilteredRowIterators.merge(toMerge, rowListener);
+                    }
 
-            protected void onKeyChange()
-            {
-                toMerge.clear();
-                for (int i = 0; i < iterators.size(); i++)
-                    toMerge.add(null);
-            }
-        },
-        isCassandraEC, taskId);
+                    protected void onKeyChange() {
+                        toMerge.clear();
+                        for (int i = 0; i < iterators.size(); i++)
+                            toMerge.add(null);
+                    }
+                },
+                isCassandraEC, taskId);
 
-        return new AbstractUnfilteredPartitionIterator()
-        {
-            public TableMetadata metadata()
-            {
+        return new AbstractUnfilteredPartitionIterator() {
+            public TableMetadata metadata() {
                 return metadata;
             }
 
-            public boolean hasNext()
-            {
+            public boolean hasNext() {
                 return merged.hasNext();
             }
 
-            public UnfilteredRowIterator next()
-            {
+            public UnfilteredRowIterator next() {
                 return merged.next();
             }
 
             @Override
-            public void close()
-            {
+            public void close() {
                 merged.close();
 
                 if (listener != null)
@@ -188,8 +180,7 @@ public abstract class UnfilteredPartitionIterators
     }
 
     @SuppressWarnings("resource")
-    public static UnfilteredPartitionIterator mergeLazily(final List<? extends UnfilteredPartitionIterator> iterators)
-    {
+    public static UnfilteredPartitionIterator mergeLazily(final List<? extends UnfilteredPartitionIterator> iterators) {
         assert !iterators.isEmpty();
 
         if (iterators.size() == 1)
@@ -197,52 +188,42 @@ public abstract class UnfilteredPartitionIterators
 
         final TableMetadata metadata = iterators.get(0).metadata();
 
-        final MergeIterator<UnfilteredRowIterator, UnfilteredRowIterator> merged = MergeIterator.get(iterators, partitionComparator, new MergeIterator.Reducer<UnfilteredRowIterator, UnfilteredRowIterator>()
-        {
-            private final List<UnfilteredRowIterator> toMerge = new ArrayList<>(iterators.size());
+        final MergeIterator<UnfilteredRowIterator, UnfilteredRowIterator> merged = MergeIterator.get(iterators,
+                partitionComparator, new MergeIterator.Reducer<UnfilteredRowIterator, UnfilteredRowIterator>() {
+                    private final List<UnfilteredRowIterator> toMerge = new ArrayList<>(iterators.size());
 
-            public void reduce(int idx, UnfilteredRowIterator current)
-            {
-                toMerge.add(current);
-            }
+                    public void reduce(int idx, UnfilteredRowIterator current) {
+                        toMerge.add(current);
+                    }
 
-            protected UnfilteredRowIterator getReduced()
-            {
-                return new LazilyInitializedUnfilteredRowIterator(toMerge.get(0).partitionKey())
-                {
-                    protected UnfilteredRowIterator initializeIterator()
-                    {
-                        return UnfilteredRowIterators.merge(toMerge);
+                    protected UnfilteredRowIterator getReduced() {
+                        return new LazilyInitializedUnfilteredRowIterator(toMerge.get(0).partitionKey()) {
+                            protected UnfilteredRowIterator initializeIterator() {
+                                return UnfilteredRowIterators.merge(toMerge);
+                            }
+                        };
                     }
-                };
-            }
 
-            protected void onKeyChange()
-            {
-                toMerge.clear();
-            }
-        });
+                    protected void onKeyChange() {
+                        toMerge.clear();
+                    }
+                });
 
-        return new AbstractUnfilteredPartitionIterator()
-        {
-            public TableMetadata metadata()
-            {
+        return new AbstractUnfilteredPartitionIterator() {
+            public TableMetadata metadata() {
                 return metadata;
             }
 
-            public boolean hasNext()
-            {
+            public boolean hasNext() {
                 return merged.hasNext();
             }
 
-            public UnfilteredRowIterator next()
-            {
+            public UnfilteredRowIterator next() {
                 return merged.next();
             }
 
             @Override
-            public void close()
-            {
+            public void close() {
                 merged.close();
             }
         };
@@ -254,37 +235,33 @@ public abstract class UnfilteredPartitionIterators
      * Caller must close the provided iterator.
      *
      * @param iterator the iterator to digest.
-     * @param digest the {@link Digest} to use.
-     * @param version the messaging protocol to use when producing the digest.
+     * @param digest   the {@link Digest} to use.
+     * @param version  the messaging protocol to use when producing the digest.
      */
-    public static void digest(UnfilteredPartitionIterator iterator, Digest digest, int version)
-    {
-        while (iterator.hasNext())
-        {
-            try (UnfilteredRowIterator partition = iterator.next())
-            {
+    public static void digest(UnfilteredPartitionIterator iterator, Digest digest, int version) {
+        while (iterator.hasNext()) {
+            try (UnfilteredRowIterator partition = iterator.next()) {
                 UnfilteredRowIterators.digest(partition, digest, version);
             }
         }
     }
 
-    public static Serializer serializerForIntraNode()
-    {
+    public static Serializer serializerForIntraNode() {
         return serializer;
     }
 
     /**
-     * Wraps the provided iterator so it logs the returned rows/RT for debugging purposes.
+     * Wraps the provided iterator so it logs the returned rows/RT for debugging
+     * purposes.
      * <p>
-     * Note that this is only meant for debugging as this can log a very large amount of
+     * Note that this is only meant for debugging as this can log a very large
+     * amount of
      * logging at INFO.
      */
-    public static UnfilteredPartitionIterator loggingIterator(UnfilteredPartitionIterator iterator, final String id, final boolean fullDetails)
-    {
-        class Logging extends Transformation<UnfilteredRowIterator>
-        {
-            public UnfilteredRowIterator applyToPartition(UnfilteredRowIterator partition)
-            {
+    public static UnfilteredPartitionIterator loggingIterator(UnfilteredPartitionIterator iterator, final String id,
+            final boolean fullDetails) {
+        class Logging extends Transformation<UnfilteredRowIterator> {
+            public UnfilteredRowIterator applyToPartition(UnfilteredRowIterator partition) {
                 return UnfilteredRowIterators.loggingIterator(partition, id, fullDetails);
             }
         }
@@ -292,96 +269,89 @@ public abstract class UnfilteredPartitionIterators
     }
 
     /**
-     * Serialize each UnfilteredSerializer one after the other, with an initial byte that indicates whether
+     * Serialize each UnfilteredSerializer one after the other, with an initial byte
+     * that indicates whether
      * we're done or not.
      */
-    public static class Serializer
-    {
-        public void serialize(UnfilteredPartitionIterator iter, ColumnFilter selection, DataOutputPlus out, int version) throws IOException
-        {
+    public static class Serializer {
+        public void serialize(UnfilteredPartitionIterator iter, ColumnFilter selection, DataOutputPlus out, int version)
+                throws IOException {
             // Previously, a boolean indicating if this was for a thrift query.
             // Unused since 4.0 but kept on wire for compatibility.
             out.writeBoolean(false);
-            while (iter.hasNext())
-            {
+            while (iter.hasNext()) {
                 out.writeBoolean(true);
-                try (UnfilteredRowIterator partition = iter.next())
-                {
+                try (UnfilteredRowIterator partition = iter.next()) {
                     UnfilteredRowIteratorSerializer.serializer.serialize(partition, selection, out, version);
                 }
             }
             out.writeBoolean(false);
         }
 
-        public UnfilteredPartitionIterator deserialize(final DataInputPlus in, final int version, final TableMetadata metadata, final ColumnFilter selection, final DeserializationHelper.Flag flag) throws IOException
-        {
+        public UnfilteredPartitionIterator deserialize(final DataInputPlus in, final int version,
+                final TableMetadata metadata, final ColumnFilter selection, final DeserializationHelper.Flag flag)
+                throws IOException {
             // Skip now unused isForThrift boolean
             in.readBoolean();
 
-            return new AbstractUnfilteredPartitionIterator()
-            {
+            return new AbstractUnfilteredPartitionIterator() {
                 private UnfilteredRowIterator next;
                 private boolean hasNext;
                 private boolean nextReturned = true;
 
-                public TableMetadata metadata()
-                {
+                public TableMetadata metadata() {
                     return metadata;
                 }
 
-                public boolean hasNext()
-                {
+                public boolean hasNext() {
                     if (!nextReturned)
                         return hasNext;
 
                     /*
-                     * We must consume the previous iterator before we start deserializing the next partition, so
+                     * We must consume the previous iterator before we start deserializing the next
+                     * partition, so
                      * that we start from the right position in the byte stream.
                      *
-                     * It's possible however that it hasn't been fully consumed by upstream consumers - for example,
-                     * if a per partition limit caused merge iterator to stop early (see CASSANDRA-13911).
+                     * It's possible however that it hasn't been fully consumed by upstream
+                     * consumers - for example,
+                     * if a per partition limit caused merge iterator to stop early (see
+                     * CASSANDRA-13911).
                      *
                      * In that case we must drain the unconsumed iterator fully ourselves, here.
                      *
-                     * NOTE: transformations of the upstream BaseRows won't be applied for these consumed elements,
+                     * NOTE: transformations of the upstream BaseRows won't be applied for these
+                     * consumed elements,
                      * so, for exmaple, they won't be counted.
                      */
                     if (null != next)
                         while (next.hasNext())
                             next.next();
 
-                    try
-                    {
+                    try {
                         hasNext = in.readBoolean();
                         nextReturned = false;
                         return hasNext;
-                    }
-                    catch (IOException e)
-                    {
+                    } catch (IOException e) {
                         throw new IOError(e);
                     }
                 }
 
-                public UnfilteredRowIterator next()
-                {
+                public UnfilteredRowIterator next() {
                     if (nextReturned && !hasNext())
                         throw new NoSuchElementException();
 
-                    try
-                    {
+                    try {
                         nextReturned = true;
-                        next = UnfilteredRowIteratorSerializer.serializer.deserialize(in, version, metadata, selection, flag);
+                        next = UnfilteredRowIteratorSerializer.serializer.deserialize(in, version, metadata, selection,
+                                flag);
                         return next;
-                    }
-                    catch (IOException e)
-                    {
+                    } catch (IOException e) {
                         throw new IOError(e);
                     }
                 }
 
                 @Override
-                public void close()
-                {
+                public void close() {
                     if (next != null)
                         next.close();
                 }
diff --git a/src/java/org/apache/cassandra/db/rows/UnfilteredRowIterators.java b/src/java/org/apache/cassandra/db/rows/UnfilteredRowIterators.java
index 2eb5d8fd..80241c9a 100644
--- a/src/java/org/apache/cassandra/db/rows/UnfilteredRowIterators.java
+++ b/src/java/org/apache/cassandra/db/rows/UnfilteredRowIterators.java
@@ -38,96 +38,122 @@ import org.apache.cassandra.utils.MergeIterator;
 /**
  * Static methods to work with atom iterators.
  */
-public abstract class UnfilteredRowIterators
-{
+public abstract class UnfilteredRowIterators {
     private static final Logger logger = LoggerFactory.getLogger(UnfilteredRowIterators.class);
 
-    private UnfilteredRowIterators() {}
+    private UnfilteredRowIterators() {
+    }
 
     /**
-     * Interface for a listener interested in the result of merging multiple versions of a given row.
+     * Interface for a listener interested in the result of merging multiple
+     * versions of a given row.
      * <p>
-     * Implementors of this interface are given enough information that they can easily reconstruct the difference
-     * between the merged result and each individual input. This is used when reconciling results on replias for
+     * Implementors of this interface are given enough information that they can
+     * easily reconstruct the difference
+     * between the merged result and each individual input. This is used when
+     * reconciling results on replias for
      * instance to figure out what to send as read-repair to each source.
      */
-    public interface MergeListener
-    {
+    public interface MergeListener {
         /**
          * Called once for the merged partition.
          *
-         * @param mergedDeletion the partition level deletion for the merged partition. Implementors can test if the
-         * merged partition actually has a partition level deletion or not by calling {@code mergedDeletion.isLive()}.
-         * @param versions the partition level deletion for the sources of the merge. Elements of the array will never
-         * be null, but be "live".
+         * @param mergedDeletion the partition level deletion for the merged partition.
+         *                       Implementors can test if the
+         *                       merged partition actually has a partition level
+         *                       deletion or not by calling
+         *                       {@code mergedDeletion.isLive()}.
+         * @param versions       the partition level deletion for the sources of the
+         *                       merge. Elements of the array will never
+         *                       be null, but be "live".
          **/
         public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions);
 
         /**
          * Called once for every row participating in the merge.
          * <p>
-         * Note that this is called for every clustering where at least one of the source merged has a row. In
-         * particular, this may be called in cases where there is no row in the merged output (if a source has a row
-         * that is shadowed by another source range tombstone or partition level deletion).
+         * Note that this is called for every clustering where at least one of the
+         * source merged has a row. In
+         * particular, this may be called in cases where there is no row in the merged
+         * output (if a source has a row
+         * that is shadowed by another source range tombstone or partition level
+         * deletion).
          *
-         * @param merged the result of the merge. This cannot be {@code null} (so that listener can always access the
-         * clustering from this safely)but can be empty, in which case this is a placeholder for when at least one
-         * source has a row, but that row is shadowed in the merged output.
-         * @param versions for each source, the row in that source corresponding to {@code merged}. This can be
-         * {@code null} for some sources if the source has not such row.
-         * @return the row to use as result of the merge (can be {@code null}). Most implementations should simply
-         * return {@code merged}, but this allows some implementations to impact the merge result if necessary. If this
-         * returns either {@code null} or an empty row, then the row is skipped from the merge result. If this returns a
-         * non {@code null} result, then the returned row <b>must</b> have the same clustering than {@code merged}.
+         * @param merged   the result of the merge. This cannot be {@code null} (so that
+         *                 listener can always access the
+         *                 clustering from this safely)but can be empty, in which case
+         *                 this is a placeholder for when at least one
+         *                 source has a row, but that row is shadowed in the merged
+         *                 output.
+         * @param versions for each source, the row in that source corresponding to
+         *                 {@code merged}. This can be
+         *                 {@code null} for some sources if the source has not such row.
+         * @return the row to use as result of the merge (can be {@code null}). Most
+         *         implementations should simply
+         *         return {@code merged}, but this allows some implementations to impact
+         *         the merge result if necessary. If this
+         *         returns either {@code null} or an empty row, then the row is skipped
+         *         from the merge result. If this returns a
+         *         non {@code null} result, then the returned row <b>must</b> have the
+         *         same clustering than {@code merged}.
          */
         public Row onMergedRows(Row merged, Row[] versions);
 
         /**
          * Called once for every range tombstone marker participating in the merge.
          * <p>
-         * Note that this is called for every "clustering position" where at least one of the source merged has a range
+         * Note that this is called for every "clustering position" where at least one
+         * of the source merged has a range
          * tombstone marker.
          *
-         * @param merged the marker in the merged output. This can be {@code null} if there is no such marker, which
-         * means that at least one source has a marker in {@code versions} but the merged out has nothing corresponding
-         * (this basically mean the merged output has a currently open deletion that shadows whatever marker the source
-         * had).
-         * @param versions the marker for each source merged. This can be {@code null} for some source if that source
-         * has not such marker.
+         * @param merged   the marker in the merged output. This can be {@code null} if
+         *                 there is no such marker, which
+         *                 means that at least one source has a marker in
+         *                 {@code versions} but the merged out has nothing corresponding
+         *                 (this basically mean the merged output has a currently open
+         *                 deletion that shadows whatever marker the source
+         *                 had).
+         * @param versions the marker for each source merged. This can be {@code null}
+         *                 for some source if that source
+         *                 has not such marker.
          */
         public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged, RangeTombstoneMarker[] versions);
 
         public void close();
 
-        public static MergeListener NOOP = new MergeListener()
-        {
-            public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions) {}
+        public static MergeListener NOOP = new MergeListener() {
+            public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions) {
+            }
 
-            public Row onMergedRows(Row merged, Row[] versions) {return merged;}
+            public Row onMergedRows(Row merged, Row[] versions) {
+                return merged;
+            }
 
-            public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged, RangeTombstoneMarker[] versions) {}
+            public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged, RangeTombstoneMarker[] versions) {
+            }
 
-            public void close() {}
+            public void close() {
+            }
         };
     }
 
     /**
      * Returns a iterator that only returns rows with only live content.
      *
-     * This is mainly used in the CQL layer when we know we don't care about deletion
-     * infos (and since an UnfilteredRowIterator cannot shadow it's own data, we know everyting
+     * This is mainly used in the CQL layer when we know we don't care about
+     * deletion
+     * infos (and since an UnfilteredRowIterator cannot shadow it's own data, we
+     * know everyting
      * returned isn't shadowed by a tombstone).
      */
-    public static RowIterator filter(UnfilteredRowIterator iter, int nowInSec)
-    {
+    public static RowIterator filter(UnfilteredRowIterator iter, int nowInSec) {
         return FilteredRows.filter(iter, nowInSec);
     }
 
     /**
      * Returns an iterator that is the result of merging other iterators.
      */
-    public static UnfilteredRowIterator merge(List<UnfilteredRowIterator> iterators)
-    {
+    public static UnfilteredRowIterator merge(List<UnfilteredRowIterator> iterators) {
         assert !iterators.isEmpty();
         if (iterators.size() == 1)
             return iterators.get(0);
@@ -136,41 +162,38 @@ public abstract class UnfilteredRowIterators
     }
 
     /**
-     * Returns an iterator that is the result of merging other iterators, and (optionally) using
+     * Returns an iterator that is the result of merging other iterators, and
+     * (optionally) using
      * specific MergeListener.
      *
      * Note that this method assumes that there is at least 2 iterators to merge.
      */
-    public static UnfilteredRowIterator merge(List<UnfilteredRowIterator> iterators, MergeListener mergeListener)
-    {
+    public static UnfilteredRowIterator merge(List<UnfilteredRowIterator> iterators, MergeListener mergeListener) {
         return UnfilteredRowMergeIterator.create(iterators, mergeListener);
     }
 
     /**
      * Returns an empty unfiltered iterator for a given partition.
      */
-    public static UnfilteredRowIterator noRowsIterator(final TableMetadata metadata, final DecoratedKey partitionKey, final Row staticRow, final DeletionTime partitionDeletion, final boolean isReverseOrder)
-    {
+    public static UnfilteredRowIterator noRowsIterator(final TableMetadata metadata, final DecoratedKey partitionKey,
+            final Row staticRow, final DeletionTime partitionDeletion, final boolean isReverseOrder) {
         return EmptyIterators.unfilteredRow(metadata, partitionKey, isReverseOrder, staticRow, partitionDeletion);
     }
 
     public static UnfilteredRowIterator singleton(Unfiltered unfiltered,
-                                                  TableMetadata metadata,
-                                                  DecoratedKey partitionKey,
-                                                  DeletionTime partitionLevelDeletion,
-                                                  RegularAndStaticColumns columns,
-                                                  Row staticRow,
-                                                  boolean isReverseOrder,
-                                                  EncodingStats encodingStats)
-    {
-        return new AbstractUnfilteredRowIterator(metadata, partitionKey, partitionLevelDeletion, columns, staticRow, isReverseOrder, encodingStats)
-        {
+            TableMetadata metadata,
+            DecoratedKey partitionKey,
+            DeletionTime partitionLevelDeletion,
+            RegularAndStaticColumns columns,
+            Row staticRow,
+            boolean isReverseOrder,
+            EncodingStats encodingStats) {
+        return new AbstractUnfilteredRowIterator(metadata, partitionKey, partitionLevelDeletion, columns, staticRow,
+                isReverseOrder, encodingStats) {
             boolean isDone = false;
 
-            protected Unfiltered computeNext()
-            {
-                if (!isDone)
-                {
+            protected Unfiltered computeNext() {
+                if (!isDone) {
                     isDone = true;
                     return unfiltered;
                 }
@@ -184,47 +207,64 @@ public abstract class UnfilteredRowIterators
      * Digests the partition represented by the provided iterator.
      *
      * @param iterator the iterator to digest.
-     * @param digest the {@link Digest} to use.
-     * @param version the messaging protocol to use when producing the digest.
+     * @param digest   the {@link Digest} to use.
+     * @param version  the messaging protocol to use when producing the digest.
      */
-    public static void digest(UnfilteredRowIterator iterator, Digest digest, int version)
-    {
+    public static void digest(UnfilteredRowIterator iterator, Digest digest, int version) {
         digest.update(iterator.partitionKey().getKey());
-        iterator.partitionLevelDeletion().digest(digest);
-        iterator.columns().regulars.digest(digest);
-        // When serializing an iterator, we skip the static columns if the iterator has not static row, even if the
-        // columns() object itself has some (the columns() is a superset of what the iterator actually contains, and
-        // will correspond to the queried columns pre-serialization). So we must avoid taking the satic column names
-        // into account if there is no static row or we'd have a digest mismatch between depending on whether the digest
-        // is computed on an iterator that has been serialized or not (see CASSANDRA-12090)
-        // TODO: in practice we could completely skip digesting the columns since they are more informative of what the
-        // iterator may contain, and digesting the actual content is enough. And in fact, that would be more correct
-        // (since again, the columns could be different without the information represented by the iterator being
-        // different), but removing them entirely is stricly speaking a breaking change (it would create mismatches on
+        // iterator.partitionLevelDeletion().digest(digest); // Tinoryj: Remove
+        // DeletionTime from digest.
+        // iterator.columns().regulars.digest(digest); // Tinoryj: Remove columns name
+        // from the digest.
+        // When serializing an iterator, we skip the static columns if the iterator has
+        // not static row, even if the
+        // columns() object itself has some (the columns() is a superset of what the
+        // iterator actually contains, and
+        // will correspond to the queried columns pre-serialization). So we must avoid
+        // taking the satic column names
+        // into account if there is no static row or we'd have a digest mismatch between
+        // depending on whether the digest
+        // is computed on an iterator that has been serialized or not (see
+        // CASSANDRA-12090)
+        // TODO: in practice we could completely skip digesting the columns since they
+        // are more informative of what the
+        // iterator may contain, and digesting the actual content is enough. And in
+        // fact, that would be more correct
+        // (since again, the columns could be different without the information
+        // represented by the iterator being
+        // different), but removing them entirely is stricly speaking a breaking change
+        // (it would create mismatches on
         // upgrade) so we can only do on the next protocol version bump.
-        if (iterator.staticRow() != Rows.EMPTY_STATIC_ROW)
-            iterator.columns().statics.digest(digest);
-        digest.updateWithBoolean(iterator.isReverseOrder());
-        iterator.staticRow().digest(digest);
+        // Tinoryj: remove static columns (Include column family name) from digest.
+        // if (iterator.staticRow() != Rows.EMPTY_STATIC_ROW) {
+        // iterator.columns().statics.digest(digest);
+        // }
 
-        while (iterator.hasNext())
-        {
+        // digest.updateWithBoolean(iterator.isReverseOrder());
+        // iterator.staticRow().digest(digest);
+
+        while (iterator.hasNext()) {
             Unfiltered unfiltered = iterator.next();
             unfiltered.digest(digest);
         }
+        // logger.debug("[Tinoryj] cauclate digest for iterator token = {}, on node {}",
+        // iterator.partitionKey().getToken(),
+        // FBUtilities.getBroadcastAddressAndPort());
     }
 
     /**
-     * Filter the provided iterator to exclude cells that have been fetched but are not queried by the user
+     * Filter the provided iterator to exclude cells that have been fetched but are
+     * not queried by the user
      * (see ColumnFilter for detailes).
      *
      * @param iterator the iterator to filter.
-     * @param filter the {@code ColumnFilter} to use when deciding which columns are the one queried by the
-     * user. This should be the filter that was used when querying {@code iterator}.
+     * @param filter   the {@code ColumnFilter} to use when deciding which columns
+     *                 are the one queried by the
+     *                 user. This should be the filter that was used when querying
+     *                 {@code iterator}.
      * @return the filtered iterator..
      */
-    public static UnfilteredRowIterator withOnlyQueriedData(UnfilteredRowIterator iterator, ColumnFilter filter)
-    {
+    public static UnfilteredRowIterator withOnlyQueriedData(UnfilteredRowIterator iterator, ColumnFilter filter) {
         if (filter.allFetchedColumnsAreQueried())
             return iterator;
 
@@ -233,23 +273,23 @@ public abstract class UnfilteredRowIterators
 
     /**
      * Returns an iterator that concatenate two atom iterators.
-     * This method assumes that both iterator are from the same partition and that the atom from
-     * {@code iter2} come after the ones of {@code iter1} (that is, that concatenating the iterator
+     * This method assumes that both iterator are from the same partition and that
+     * the atom from
+     * {@code iter2} come after the ones of {@code iter1} (that is, that
+     * concatenating the iterator
      * make sense).
      */
-    public static UnfilteredRowIterator concat(final UnfilteredRowIterator iter1, final UnfilteredRowIterator iter2)
-    {
+    public static UnfilteredRowIterator concat(final UnfilteredRowIterator iter1, final UnfilteredRowIterator iter2) {
         assert iter1.metadata().id.equals(iter2.metadata().id)
-            && iter1.partitionKey().equals(iter2.partitionKey())
-            && iter1.partitionLevelDeletion().equals(iter2.partitionLevelDeletion())
-            && iter1.isReverseOrder() == iter2.isReverseOrder()
-            && iter1.staticRow().equals(iter2.staticRow());
+                && iter1.partitionKey().equals(iter2.partitionKey())
+                && iter1.partitionLevelDeletion().equals(iter2.partitionLevelDeletion())
+                && iter1.isReverseOrder() == iter2.isReverseOrder()
+                && iter1.staticRow().equals(iter2.staticRow());
 
-        class Extend implements MoreRows<UnfilteredRowIterator>
-        {
+        class Extend implements MoreRows<UnfilteredRowIterator> {
             boolean returned = false;
-            public UnfilteredRowIterator moreContents()
-            {
+
+            public UnfilteredRowIterator moreContents() {
                 if (returned)
                     return null;
                 returned = true;
@@ -263,23 +303,18 @@ public abstract class UnfilteredRowIterators
     /**
      * Returns an iterator that concatenate the specified atom with the iterator.
      */
-    public static UnfilteredRowIterator concat(final Unfiltered first, final UnfilteredRowIterator rest)
-    {
-        return new WrappingUnfilteredRowIterator(rest)
-        {
+    public static UnfilteredRowIterator concat(final Unfiltered first, final UnfilteredRowIterator rest) {
+        return new WrappingUnfilteredRowIterator(rest) {
             private boolean hasReturnedFirst;
 
             @Override
-            public boolean hasNext()
-            {
+            public boolean hasNext() {
                 return hasReturnedFirst ? super.hasNext() : true;
             }
 
             @Override
-            public Unfiltered next()
-            {
-                if (!hasReturnedFirst)
-                {
+            public Unfiltered next() {
+                if (!hasReturnedFirst) {
                     hasReturnedFirst = true;
                     return first;
                 }
@@ -289,8 +324,10 @@ public abstract class UnfilteredRowIterators
     }
 
     /**
-     * Validate that the data of the provided iterator is valid, that is that the values
-     * it contains are valid for the type they represent, and more generally that the
+     * Validate that the data of the provided iterator is valid, that is that the
+     * values
+     * it contains are valid for the type they represent, and more generally that
+     * the
      * infos stored are sensible.
      *
      * This is mainly used by scrubber to detect problems in sstables.
@@ -298,42 +335,34 @@ public abstract class UnfilteredRowIterators
      * @param iterator the partition to check.
      * @param filename the name of the file the data is comming from.
      * @return an iterator that returns the same data than {@code iterator} but that
-     * checks said data and throws a {@code CorruptedSSTableException} if it detects
-     * invalid data.
+     *         checks said data and throws a {@code CorruptedSSTableException} if it
+     *         detects
+     *         invalid data.
      */
-    public static UnfilteredRowIterator withValidation(UnfilteredRowIterator iterator, final String filename)
-    {
-        class Validator extends Transformation
-        {
+    public static UnfilteredRowIterator withValidation(UnfilteredRowIterator iterator, final String filename) {
+        class Validator extends Transformation {
             @Override
-            public Row applyToStatic(Row row)
-            {
+            public Row applyToStatic(Row row) {
                 validate(row);
                 return row;
             }
 
             @Override
-            public Row applyToRow(Row row)
-            {
+            public Row applyToRow(Row row) {
                 validate(row);
                 return row;
             }
 
             @Override
-            public RangeTombstoneMarker applyToMarker(RangeTombstoneMarker marker)
-            {
+            public RangeTombstoneMarker applyToMarker(RangeTombstoneMarker marker) {
                 validate(marker);
                 return marker;
             }
 
-            private void validate(Unfiltered unfiltered)
-            {
-                try
-                {
+            private void validate(Unfiltered unfiltered) {
+                try {
                     unfiltered.validateData(iterator.metadata());
-                }
-                catch (MarshalException me)
-                {
+                } catch (MarshalException me) {
                     throw new CorruptSSTableException(me, filename);
                 }
             }
@@ -342,42 +371,40 @@ public abstract class UnfilteredRowIterators
     }
 
     /**
-     * Wraps the provided iterator so it logs the returned atoms for debugging purposes.
+     * Wraps the provided iterator so it logs the returned atoms for debugging
+     * purposes.
      * <p>
-     * Note that this is only meant for debugging as this can log a very large amount of
+     * Note that this is only meant for debugging as this can log a very large
+     * amount of
      * logging at INFO.
      */
-    public static UnfilteredRowIterator loggingIterator(UnfilteredRowIterator iterator, final String id, final boolean fullDetails)
-    {
+    public static UnfilteredRowIterator loggingIterator(UnfilteredRowIterator iterator, final String id,
+            final boolean fullDetails) {
         TableMetadata metadata = iterator.metadata();
         logger.info("[{}] Logging iterator on {}.{}, partition key={}, reversed={}, deletion={}",
-                    id,
-                    metadata.keyspace,
-                    metadata.name,
-                    metadata.partitionKeyType.getString(iterator.partitionKey().getKey()),
-                    iterator.isReverseOrder(),
-                    iterator.partitionLevelDeletion().markedForDeleteAt());
-
-        class Logger extends Transformation
-        {
+                id,
+                metadata.keyspace,
+                metadata.name,
+                metadata.partitionKeyType.getString(iterator.partitionKey().getKey()),
+                iterator.isReverseOrder(),
+                iterator.partitionLevelDeletion().markedForDeleteAt());
+
+        class Logger extends Transformation {
             @Override
-            public Row applyToStatic(Row row)
-            {
+            public Row applyToStatic(Row row) {
                 if (!row.isEmpty())
                     logger.info("[{}] {}", id, row.toString(metadata, fullDetails));
                 return row;
             }
 
             @Override
-            public Row applyToRow(Row row)
-            {
+            public Row applyToRow(Row row) {
                 logger.info("[{}] {}", id, row.toString(metadata, fullDetails));
                 return row;
             }
 
             @Override
-            public RangeTombstoneMarker applyToMarker(RangeTombstoneMarker marker)
-            {
+            public RangeTombstoneMarker applyToMarker(RangeTombstoneMarker marker) {
                 logger.info("[{}] {}", id, marker.toString(metadata));
                 return marker;
             }
@@ -386,54 +413,47 @@ public abstract class UnfilteredRowIterators
     }
 
     /**
-     * A wrapper over MergeIterator to implement the UnfilteredRowIterator interface.
+     * A wrapper over MergeIterator to implement the UnfilteredRowIterator
+     * interface.
      */
-    private static class UnfilteredRowMergeIterator extends AbstractUnfilteredRowIterator
-    {
+    private static class UnfilteredRowMergeIterator extends AbstractUnfilteredRowIterator {
         private final IMergeIterator<Unfiltered, Unfiltered> mergeIterator;
         private final MergeListener listener;
 
         private UnfilteredRowMergeIterator(TableMetadata metadata,
-                                           List<UnfilteredRowIterator> iterators,
-                                           RegularAndStaticColumns columns,
-                                           DeletionTime partitionDeletion,
-                                           boolean reversed,
-                                           MergeListener listener)
-        {
+                List<UnfilteredRowIterator> iterators,
+                RegularAndStaticColumns columns,
+                DeletionTime partitionDeletion,
+                boolean reversed,
+                MergeListener listener) {
             super(metadata,
-                  iterators.get(0).partitionKey(),
-                  partitionDeletion,
-                  columns,
-                  mergeStaticRows(iterators, columns.statics, listener, partitionDeletion),
-                  reversed,
-                  EncodingStats.merge(iterators, UnfilteredRowIterator::stats));
+                    iterators.get(0).partitionKey(),
+                    partitionDeletion,
+                    columns,
+                    mergeStaticRows(iterators, columns.statics, listener, partitionDeletion),
+                    reversed,
+                    EncodingStats.merge(iterators, UnfilteredRowIterator::stats));
 
             this.mergeIterator = MergeIterator.get(iterators,
-                                                   reversed ? metadata.comparator.reversed() : metadata.comparator,
-                                                   new MergeReducer(iterators.size(), reversed, listener));
+                    reversed ? metadata.comparator.reversed() : metadata.comparator,
+                    new MergeReducer(iterators.size(), reversed, listener));
             this.listener = listener;
         }
 
-        private static UnfilteredRowMergeIterator create(List<UnfilteredRowIterator> iterators, MergeListener listener)
-        {
-            try
-            {
+        private static UnfilteredRowMergeIterator create(List<UnfilteredRowIterator> iterators,
+                MergeListener listener) {
+            try {
                 checkForInvalidInput(iterators);
                 return new UnfilteredRowMergeIterator(iterators.get(0).metadata(),
-                                                      iterators,
-                                                      collectColumns(iterators),
-                                                      collectPartitionLevelDeletion(iterators, listener),
-                                                      iterators.get(0).isReverseOrder(),
-                                                      listener);
-            }
-            catch (RuntimeException | Error e)
-            {
-                try
-                {
+                        iterators,
+                        collectColumns(iterators),
+                        collectPartitionLevelDeletion(iterators, listener),
+                        iterators.get(0).isReverseOrder(),
+                        listener);
+            } catch (RuntimeException | Error e) {
+                try {
                     FBUtilities.closeAll(iterators);
-                }
-                catch (Exception suppressed)
-                {
+                } catch (Exception suppressed) {
                     e.addSuppressed(suppressed);
                 }
                 throw e;
@@ -441,14 +461,12 @@ public abstract class UnfilteredRowIterators
         }
 
         @SuppressWarnings("resource") // We're not really creating any resource here
-        private static void checkForInvalidInput(List<UnfilteredRowIterator> iterators)
-        {
+        private static void checkForInvalidInput(List<UnfilteredRowIterator> iterators) {
             if (iterators.isEmpty())
                 return;
 
             UnfilteredRowIterator first = iterators.get(0);
-            for (int i = 1; i < iterators.size(); i++)
-            {
+            for (int i = 1; i < iterators.size(); i++) {
                 UnfilteredRowIterator iter = iterators.get(i);
                 assert first.metadata().id.equals(iter.metadata().id);
                 assert first.partitionKey().equals(iter.partitionKey());
@@ -457,13 +475,12 @@ public abstract class UnfilteredRowIterators
         }
 
         @SuppressWarnings("resource") // We're not really creating any resource here
-        private static DeletionTime collectPartitionLevelDeletion(List<UnfilteredRowIterator> iterators, MergeListener listener)
-        {
+        private static DeletionTime collectPartitionLevelDeletion(List<UnfilteredRowIterator> iterators,
+                MergeListener listener) {
             DeletionTime[] versions = listener == null ? null : new DeletionTime[iterators.size()];
 
             DeletionTime delTime = DeletionTime.LIVE;
-            for (int i = 0; i < iterators.size(); i++)
-            {
+            for (int i = 0; i < iterators.size(); i++) {
                 UnfilteredRowIterator iter = iterators.get(i);
                 DeletionTime iterDeletion = iter.partitionLevelDeletion();
                 if (listener != null)
@@ -477,10 +494,9 @@ public abstract class UnfilteredRowIterators
         }
 
         private static Row mergeStaticRows(List<UnfilteredRowIterator> iterators,
-                                           Columns columns,
-                                           MergeListener listener,
-                                           DeletionTime partitionDeletion)
-        {
+                Columns columns,
+                MergeListener listener,
+                DeletionTime partitionDeletion) {
             if (columns.isEmpty())
                 return Rows.EMPTY_STATIC_ROW;
 
@@ -498,30 +514,27 @@ public abstract class UnfilteredRowIterators
                 return merged;
 
             merged = listener.onMergedRows(merged, merger.mergedRows());
-            // Note that onMergedRows can have returned null even though his input wasn't null
+            // Note that onMergedRows can have returned null even though his input wasn't
+            // null
             return merged == null ? Rows.EMPTY_STATIC_ROW : merged;
         }
 
-        private static RegularAndStaticColumns collectColumns(List<UnfilteredRowIterator> iterators)
-        {
+        private static RegularAndStaticColumns collectColumns(List<UnfilteredRowIterator> iterators) {
             RegularAndStaticColumns first = iterators.get(0).columns();
             Columns statics = first.statics;
             Columns regulars = first.regulars;
-            for (int i = 1; i < iterators.size(); i++)
-            {
+            for (int i = 1; i < iterators.size(); i++) {
                 RegularAndStaticColumns cols = iterators.get(i).columns();
                 statics = statics.mergeTo(cols.statics);
                 regulars = regulars.mergeTo(cols.regulars);
             }
             return statics == first.statics && regulars == first.regulars
-                 ? first
-                 : new RegularAndStaticColumns(statics, regulars);
+                    ? first
+                    : new RegularAndStaticColumns(statics, regulars);
         }
 
-        protected Unfiltered computeNext()
-        {
-            while (mergeIterator.hasNext())
-            {
+        protected Unfiltered computeNext() {
+            while (mergeIterator.hasNext()) {
                 Unfiltered merged = mergeIterator.next();
                 if (merged != null)
                     return merged;
@@ -529,8 +542,7 @@ public abstract class UnfilteredRowIterators
             return endOfData();
         }
 
-        public void close()
-        {
+        public void close() {
             // This will close the input iterators
             FileUtils.closeQuietly(mergeIterator);
 
@@ -538,8 +550,7 @@ public abstract class UnfilteredRowIterators
                 listener.close();
         }
 
-        private class MergeReducer extends MergeIterator.Reducer<Unfiltered, Unfiltered>
-        {
+        private class MergeReducer extends MergeIterator.Reducer<Unfiltered, Unfiltered> {
             private final MergeListener listener;
 
             private Unfiltered.Kind nextKind;
@@ -547,46 +558,39 @@ public abstract class UnfilteredRowIterators
             private final Row.Merger rowMerger;
             private final RangeTombstoneMarker.Merger markerMerger;
 
-            private MergeReducer(int size, boolean reversed, MergeListener listener)
-            {
+            private MergeReducer(int size, boolean reversed, MergeListener listener) {
                 this.rowMerger = new Row.Merger(size, columns().regulars.hasComplex());
                 this.markerMerger = new RangeTombstoneMarker.Merger(size, partitionLevelDeletion(), reversed);
                 this.listener = listener;
             }
 
             @Override
-            public boolean trivialReduceIsTrivial()
-            {
+            public boolean trivialReduceIsTrivial() {
                 // If we have a listener, we must signal it even when we have a single version
                 return listener == null;
             }
 
-            public void reduce(int idx, Unfiltered current)
-            {
+            public void reduce(int idx, Unfiltered current) {
                 nextKind = current.kind();
                 if (nextKind == Unfiltered.Kind.ROW)
-                    rowMerger.add(idx, (Row)current);
+                    rowMerger.add(idx, (Row) current);
                 else
-                    markerMerger.add(idx, (RangeTombstoneMarker)current);
+                    markerMerger.add(idx, (RangeTombstoneMarker) current);
             }
 
-            protected Unfiltered getReduced()
-            {
-                if (nextKind == Unfiltered.Kind.ROW)
-                {
+            protected Unfiltered getReduced() {
+                if (nextKind == Unfiltered.Kind.ROW) {
                     Row merged = rowMerger.merge(markerMerger.activeDeletion());
                     if (listener == null)
                         return merged;
 
                     merged = listener.onMergedRows(merged == null
-                                                   ? BTreeRow.emptyRow(rowMerger.mergedClustering())
-                                                   : merged,
-                                                   rowMerger.mergedRows());
+                            ? BTreeRow.emptyRow(rowMerger.mergedClustering())
+                            : merged,
+                            rowMerger.mergedRows());
 
                     return merged == null || merged.isEmpty() ? null : merged;
-                }
-                else
-                {
+                } else {
                     RangeTombstoneMarker merged = markerMerger.merge();
                     if (listener != null)
                         listener.onMergedRangeTombstoneMarkers(merged, markerMerger.mergedMarkers());
@@ -594,8 +598,7 @@ public abstract class UnfilteredRowIterators
                 }
             }
 
-            protected void onKeyChange()
-            {
+            protected void onKeyChange() {
                 if (nextKind == Unfiltered.Kind.ROW)
                     rowMerger.clear();
                 else
diff --git a/src/java/org/apache/cassandra/gms/GossipDigestAck.java b/src/java/org/apache/cassandra/gms/GossipDigestAck.java
index 26494eab..a37913eb 100644
--- a/src/java/org/apache/cassandra/gms/GossipDigestAck.java
+++ b/src/java/org/apache/cassandra/gms/GossipDigestAck.java
@@ -31,55 +31,47 @@ import org.apache.cassandra.locator.InetAddressAndPort;
 import static org.apache.cassandra.locator.InetAddressAndPort.Serializer.inetAddressAndPortSerializer;
 
 /**
- * This ack gets sent out as a result of the receipt of a GossipDigestSynMessage by an
+ * This ack gets sent out as a result of the receipt of a GossipDigestSynMessage
+ * by an
  * endpoint. This is the 2 stage of the 3 way messaging in the Gossip protocol.
  */
-public class GossipDigestAck
-{
+public class GossipDigestAck {
     public static final IVersionedSerializer<GossipDigestAck> serializer = new GossipDigestAckSerializer();
 
     final List<GossipDigest> gDigestList;
     final Map<InetAddressAndPort, EndpointState> epStateMap;
 
-    GossipDigestAck(List<GossipDigest> gDigestList, Map<InetAddressAndPort, EndpointState> epStateMap)
-    {
+    GossipDigestAck(List<GossipDigest> gDigestList, Map<InetAddressAndPort, EndpointState> epStateMap) {
         this.gDigestList = gDigestList;
         this.epStateMap = epStateMap;
     }
 
-    List<GossipDigest> getGossipDigestList()
-    {
+    List<GossipDigest> getGossipDigestList() {
         return gDigestList;
     }
 
-    Map<InetAddressAndPort, EndpointState> getEndpointStateMap()
-    {
+    Map<InetAddressAndPort, EndpointState> getEndpointStateMap() {
         return epStateMap;
     }
 }
 
-class GossipDigestAckSerializer implements IVersionedSerializer<GossipDigestAck>
-{
-    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException
-    {
+class GossipDigestAckSerializer implements IVersionedSerializer<GossipDigestAck> {
+    public void serialize(GossipDigestAck gDigestAckMessage, DataOutputPlus out, int version) throws IOException {
         GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList, out, version);
         out.writeInt(gDigestAckMessage.epStateMap.size());
-        for (Map.Entry<InetAddressAndPort, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet())
-        {
+        for (Map.Entry<InetAddressAndPort, EndpointState> entry : gDigestAckMessage.epStateMap.entrySet()) {
             InetAddressAndPort ep = entry.getKey();
             inetAddressAndPortSerializer.serialize(ep, out, version);
             EndpointState.serializer.serialize(entry.getValue(), out, version);
         }
     }
 
-    public GossipDigestAck deserialize(DataInputPlus in, int version) throws IOException
-    {
+    public GossipDigestAck deserialize(DataInputPlus in, int version) throws IOException {
         List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(in, version);
         int size = in.readInt();
         Map<InetAddressAndPort, EndpointState> epStateMap = new HashMap<InetAddressAndPort, EndpointState>(size);
 
-        for (int i = 0; i < size; ++i)
-        {
+        for (int i = 0; i < size; ++i) {
             InetAddressAndPort ep = inetAddressAndPortSerializer.deserialize(in, version);
             EndpointState epState = EndpointState.serializer.deserialize(in, version);
             epStateMap.put(ep, epState);
@@ -87,8 +79,7 @@ class GossipDigestAckSerializer implements IVersionedSerializer<GossipDigestAck>
         return new GossipDigestAck(gDigestList, epStateMap);
     }
 
-    public long serializedSize(GossipDigestAck ack, int version)
-    {
+    public long serializedSize(GossipDigestAck ack, int version) {
         int size = GossipDigestSerializationHelper.serializedSize(ack.gDigestList, version);
         size += TypeSizes.sizeof(ack.epStateMap.size());
         for (Map.Entry<InetAddressAndPort, EndpointState> entry : ack.epStateMap.entrySet())
diff --git a/src/java/org/apache/cassandra/locator/ReplicaLayout.java b/src/java/org/apache/cassandra/locator/ReplicaLayout.java
index 351e837a..e86d9f23 100644
--- a/src/java/org/apache/cassandra/locator/ReplicaLayout.java
+++ b/src/java/org/apache/cassandra/locator/ReplicaLayout.java
@@ -35,101 +35,95 @@ import java.util.function.Predicate;
  *
  * @param <E>
  */
-public abstract class ReplicaLayout<E extends Endpoints<E>>
-{
+public abstract class ReplicaLayout<E extends Endpoints<E>> {
     private final E natural;
-    // the snapshot of the replication strategy that corresponds to the replica layout
+    // the snapshot of the replication strategy that corresponds to the replica
+    // layout
     private final AbstractReplicationStrategy replicationStrategy;
 
-    ReplicaLayout(AbstractReplicationStrategy replicationStrategy, E natural)
-    {
+    ReplicaLayout(AbstractReplicationStrategy replicationStrategy, E natural) {
         this.replicationStrategy = replicationStrategy;
         this.natural = natural;
     }
 
     /**
-     * The 'natural' owners of the ring position(s), as implied by the current ring layout.
-     * This excludes any pending owners, i.e. those that are in the process of taking ownership of a range, but
+     * The 'natural' owners of the ring position(s), as implied by the current ring
+     * layout.
+     * This excludes any pending owners, i.e. those that are in the process of
+     * taking ownership of a range, but
      * have not yet finished obtaining their view of the range.
      */
-    public final E natural()
-    {
+    public final E natural() {
         return natural;
     }
 
-    public final AbstractReplicationStrategy replicationStrategy()
-    {
+    public final AbstractReplicationStrategy replicationStrategy() {
         return replicationStrategy;
     }
 
     /**
-     * All relevant owners of the ring position(s) for this operation, as implied by the current ring layout.
-     * For writes, this will include pending owners, and for reads it will be equivalent to natural()
+     * All relevant owners of the ring position(s) for this operation, as implied by
+     * the current ring layout.
+     * For writes, this will include pending owners, and for reads it will be
+     * equivalent to natural()
      */
-    public E all()
-    {
+    public E all() {
         return natural;
     }
 
-    public String toString()
-    {
+    public String toString() {
         return "ReplicaLayout [ natural: " + natural + " ]";
     }
 
-    public static class ForTokenRead extends ReplicaLayout<EndpointsForToken> implements ForToken
-    {
-        public ForTokenRead(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural)
-        {
+    public static class ForTokenRead extends ReplicaLayout<EndpointsForToken> implements ForToken {
+        public ForTokenRead(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural) {
             super(replicationStrategy, natural);
         }
 
         @Override
-        public Token token()
-        {
+        public Token token() {
             return natural().token();
         }
 
-        public ReplicaLayout.ForTokenRead filter(Predicate<Replica> filter)
-        {
+        public ReplicaLayout.ForTokenRead filter(Predicate<Replica> filter) {
             EndpointsForToken filtered = natural().filter(filter);
-            // AbstractReplicaCollection.filter returns itself if all elements match the filter
-            if (filtered == natural()) return this;
+            // AbstractReplicaCollection.filter returns itself if all elements match the
+            // filter
+            if (filtered == natural())
+                return this;
             return new ReplicaLayout.ForTokenRead(replicationStrategy(), filtered);
         }
     }
 
-    public static class ForRangeRead extends ReplicaLayout<EndpointsForRange> implements ForRange
-    {
+    public static class ForRangeRead extends ReplicaLayout<EndpointsForRange> implements ForRange {
         final AbstractBounds<PartitionPosition> range;
 
-        public ForRangeRead(AbstractReplicationStrategy replicationStrategy, AbstractBounds<PartitionPosition> range, EndpointsForRange natural)
-        {
+        public ForRangeRead(AbstractReplicationStrategy replicationStrategy, AbstractBounds<PartitionPosition> range,
+                EndpointsForRange natural) {
             super(replicationStrategy, natural);
             this.range = range;
         }
 
         @Override
-        public AbstractBounds<PartitionPosition> range()
-        {
+        public AbstractBounds<PartitionPosition> range() {
             return range;
         }
 
-        public ReplicaLayout.ForRangeRead filter(Predicate<Replica> filter)
-        {
+        public ReplicaLayout.ForRangeRead filter(Predicate<Replica> filter) {
             EndpointsForRange filtered = natural().filter(filter);
-            // AbstractReplicaCollection.filter returns itself if all elements match the filter
-            if (filtered == natural()) return this;
+            // AbstractReplicaCollection.filter returns itself if all elements match the
+            // filter
+            if (filtered == natural())
+                return this;
             return new ReplicaLayout.ForRangeRead(replicationStrategy(), range(), filtered);
         }
     }
 
-    public static class ForWrite<E extends Endpoints<E>> extends ReplicaLayout<E>
-    {
+    public static class ForWrite<E extends Endpoints<E>> extends ReplicaLayout<E> {
         final E all;
         final E pending;
 
-        ForWrite(AbstractReplicationStrategy replicationStrategy, E natural, E pending, E all)
-        {
+        ForWrite(AbstractReplicationStrategy replicationStrategy, E natural, E pending, E all) {
             super(replicationStrategy, natural);
             assert pending != null && !haveWriteConflicts(natural, pending);
             if (all == null)
@@ -138,71 +132,71 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
             this.pending = pending;
         }
 
-        public final E all()
-        {
+        public final E all() {
             return all;
         }
 
-        public final E pending()
-        {
+        public final E pending() {
             return pending;
         }
 
-        public String toString()
-        {
+        public String toString() {
             return "ReplicaLayout [ natural: " + natural() + ", pending: " + pending + " ]";
         }
     }
 
-    public static class ForTokenWrite extends ForWrite<EndpointsForToken> implements ForToken
-    {
-        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending)
-        {
+    public static class ForTokenWrite extends ForWrite<EndpointsForToken> implements ForToken {
+        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural,
+                EndpointsForToken pending) {
             this(replicationStrategy, natural, pending, null);
         }
-        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending, EndpointsForToken all)
-        {
+
+        public ForTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural,
+                EndpointsForToken pending, EndpointsForToken all) {
             super(replicationStrategy, natural, pending, all);
         }
 
         @Override
-        public Token token() { return natural().token(); }
+        public Token token() {
+            return natural().token();
+        }
 
-        public ForTokenWrite filter(Predicate<Replica> filter)
-        {
+        public ForTokenWrite filter(Predicate<Replica> filter) {
             EndpointsForToken filtered = all().filter(filter);
-            // AbstractReplicaCollection.filter returns itself if all elements match the filter
-            if (filtered == all()) return this;
-            if (pending().isEmpty()) return new ForTokenWrite(replicationStrategy(), filtered, pending(), filtered);
+            // AbstractReplicaCollection.filter returns itself if all elements match the
+            // filter
+            if (filtered == all())
+                return this;
+            if (pending().isEmpty())
+                return new ForTokenWrite(replicationStrategy(), filtered, pending(), filtered);
             // unique by endpoint, so can for efficiency filter only on endpoint
             return new ForTokenWrite(
                     replicationStrategy(),
                     natural().keep(filtered.endpoints()),
                     pending().keep(filtered.endpoints()),
-                    filtered
-            );
+                    filtered);
         }
     }
 
-    public interface ForRange
-    {
+    public interface ForRange {
         public AbstractBounds<PartitionPosition> range();
     }
 
-    public interface ForToken
-    {
+    public interface ForToken {
         public Token token();
     }
 
     /**
-     * Gets the 'natural' and 'pending' replicas that own a given token, with no filtering or processing.
+     * Gets the 'natural' and 'pending' replicas that own a given token, with no
+     * filtering or processing.
      *
-     * Since a write is intended for all nodes (except, unless necessary, transient replicas), this method's
-     * only responsibility is to fetch the 'natural' and 'pending' replicas, then resolve any conflicts
+     * Since a write is intended for all nodes (except, unless necessary, transient
+     * replicas), this method's
+     * only responsibility is to fetch the 'natural' and 'pending' replicas, then
+     * resolve any conflicts
      * {@link ReplicaLayout#haveWriteConflicts(Endpoints, Endpoints)}
      */
-    public static ReplicaLayout.ForTokenWrite forTokenWriteLiveAndDown(Keyspace keyspace, Token token)
-    {
+    public static ReplicaLayout.ForTokenWrite forTokenWriteLiveAndDown(Keyspace keyspace, Token token) {
         // TODO: these should be cached, not the natural replicas
         // TODO: race condition to fetch these. implications??
         AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();
@@ -211,10 +205,9 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
         return forTokenWrite(replicationStrategy, natural, pending);
     }
 
-    public static ReplicaLayout.ForTokenWrite forTokenWrite(AbstractReplicationStrategy replicationStrategy, EndpointsForToken natural, EndpointsForToken pending)
-    {
-        if (haveWriteConflicts(natural, pending))
-        {
+    public static ReplicaLayout.ForTokenWrite forTokenWrite(AbstractReplicationStrategy replicationStrategy,
+            EndpointsForToken natural, EndpointsForToken pending) {
+        if (haveWriteConflicts(natural, pending)) {
             natural = resolveWriteConflictsInNatural(natural, pending);
             pending = resolveWriteConflictsInPending(natural, pending);
         }
@@ -222,46 +215,69 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
     }
 
     /**
-     * Detect if we have any endpoint in both pending and full; this can occur either due to races (there is no isolation)
-     * or because an endpoint is transitioning between full and transient replication status.
+     * Detect if we have any endpoint in both pending and full; this can occur
+     * either due to races (there is no isolation)
+     * or because an endpoint is transitioning between full and transient
+     * replication status.
      *
-     * We essentially always prefer the full version for writes, because this is stricter.
+     * We essentially always prefer the full version for writes, because this is
+     * stricter.
      *
      * For transient->full transitions:
      *
-     *   Since we always write to any pending transient replica, effectively upgrading it to full for the transition duration,
-     *   it might at first seem to be OK to continue treating the conflict replica as its 'natural' transient form,
-     *   as there is always a quorum of nodes receiving the write.  However, ring ownership changes are not atomic or
-     *   consistent across the cluster, and it is possible for writers to see different ring states.
+     * Since we always write to any pending transient replica, effectively upgrading
+     * it to full for the transition duration,
+     * it might at first seem to be OK to continue treating the conflict replica as
+     * its 'natural' transient form,
+     * as there is always a quorum of nodes receiving the write. However, ring
+     * ownership changes are not atomic or
+     * consistent across the cluster, and it is possible for writers to see
+     * different ring states.
      *
-     *   Furthermore, an operator would expect that the full node has received all writes, with no extra need for repair
-     *   (as the normal contract dictates) when it completes its transition.
+     * Furthermore, an operator would expect that the full node has received all
+     * writes, with no extra need for repair
+     * (as the normal contract dictates) when it completes its transition.
      *
-     *   While we cannot completely eliminate risks due to ring inconsistencies, this approach is the most conservative
-     *   available to us today to mitigate, and (we think) the easiest to reason about.
+     * While we cannot completely eliminate risks due to ring inconsistencies, this
+     * approach is the most conservative
+     * available to us today to mitigate, and (we think) the easiest to reason
+     * about.
      *
      * For full->transient transitions:
      *
-     *   In this case, things are dicier, because in theory we can trigger this change instantly.  All we need to do is
-     *   drop some data, surely?
+     * In this case, things are dicier, because in theory we can trigger this change
+     * instantly. All we need to do is
+     * drop some data, surely?
      *
-     *   Ring movements can put us in a pickle; any other node could believe us to be full when we have become transient,
-     *   and perform a full data request to us that we believe ourselves capable of answering, but that we are not.
-     *   If the ring is inconsistent, it's even feasible that a transient request would be made to the node that is losing
-     *   its transient status, that also does not know it has yet done so, resulting in all involved nodes being unaware
-     *   of the data inconsistency.
+     * Ring movements can put us in a pickle; any other node could believe us to be
+     * full when we have become transient,
+     * and perform a full data request to us that we believe ourselves capable of
+     * answering, but that we are not.
+     * If the ring is inconsistent, it's even feasible that a transient request
+     * would be made to the node that is losing
+     * its transient status, that also does not know it has yet done so, resulting
+     * in all involved nodes being unaware
+     * of the data inconsistency.
      *
-     *   This happens because ring ownership changes are implied by a single node; not all owning nodes get a say in when
-     *   the transition takes effect.  As such, a node can hold an incorrect belief about its own ownership ranges.
+     * This happens because ring ownership changes are implied by a single node; not
+     * all owning nodes get a say in when
+     * the transition takes effect. As such, a node can hold an incorrect belief
+     * about its own ownership ranges.
      *
-     *   This race condition is somewhat inherent in present day Cassandra, and there's actually a limit to what we can do about it.
-     *   It is a little more dangerous with transient replication, however, because we can completely answer a request without
-     *   ever touching a digest, meaning we are less likely to attempt to repair any inconsistency.
+     * This race condition is somewhat inherent in present day Cassandra, and
+     * there's actually a limit to what we can do about it.
+     * It is a little more dangerous with transient replication, however, because we
+     * can completely answer a request without
+     * ever touching a digest, meaning we are less likely to attempt to repair any
+     * inconsistency.
      *
-     *   We aren't guaranteed to contact any different nodes for the data requests, of course, though we at least have a chance.
+     * We aren't guaranteed to contact any different nodes for the data requests, of
+     * course, though we at least have a chance.
      *
-     * Note: If we have any pending transient->full movement, we need to move the full replica to our 'natural' bucket
-     * to avoid corrupting our count.  This is fine for writes, all we're doing is ensuring we always write to the node,
+     * Note: If we have any pending transient->full movement, we need to move the
+     * full replica to our 'natural' bucket
+     * to avoid corrupting our count. This is fine for writes, all we're doing is
+     * ensuring we always write to the node,
      * instead of selectively.
      *
      * @param natural
@@ -269,11 +285,9 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
      * @param <E>
      * @return
      */
-    static <E extends Endpoints<E>> boolean haveWriteConflicts(E natural, E pending)
-    {
+    static <E extends Endpoints<E>> boolean haveWriteConflicts(E natural, E pending) {
         Set<InetAddressAndPort> naturalEndpoints = natural.endpoints();
-        for (InetAddressAndPort pendingEndpoint : pending.endpoints())
-        {
+        for (InetAddressAndPort pendingEndpoint : pending.endpoints()) {
             if (naturalEndpoints.contains(pendingEndpoint))
                 return true;
         }
@@ -283,23 +297,23 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
     /**
      * MUST APPLY FIRST
      * See {@link ReplicaLayout#haveWriteConflicts}
-     * @return a 'natural' replica collection, that has had its conflicts with pending repaired
+     * 
+     * @return a 'natural' replica collection, that has had its conflicts with
+     *         pending repaired
      */
     @VisibleForTesting
-    static EndpointsForToken resolveWriteConflictsInNatural(EndpointsForToken natural, EndpointsForToken pending)
-    {
+    static EndpointsForToken resolveWriteConflictsInNatural(EndpointsForToken natural, EndpointsForToken pending) {
         EndpointsForToken.Builder resolved = natural.newBuilder(natural.size());
-        for (Replica replica : natural)
-        {
+        for (Replica replica : natural) {
             // always prefer the full natural replica, if there is a conflict
-            if (replica.isTransient())
-            {
+            if (replica.isTransient()) {
                 Replica conflict = pending.byEndpoint().get(replica.endpoint());
-                if (conflict != null)
-                {
-                    // it should not be possible to have conflicts of the same replication type for the same range
+                if (conflict != null) {
+                    // it should not be possible to have conflicts of the same replication type for
+                    // the same range
                     assert conflict.isFull();
-                    // If we have any pending transient->full movement, we need to move the full replica to our 'natural' bucket
+                    // If we have any pending transient->full movement, we need to move the full
+                    // replica to our 'natural' bucket
                     // to avoid corrupting our count
                     resolved.add(conflict);
                     continue;
@@ -313,35 +327,45 @@ public abstract class ReplicaLayout<E extends Endpoints<E>>
     /**
      * MUST APPLY SECOND
      * See {@link ReplicaLayout#haveWriteConflicts}
-     * @return a 'pending' replica collection, that has had its conflicts with natural repaired
+     * 
+     * @return a 'pending' replica collection, that has had its conflicts with
+     *         natural repaired
      */
     @VisibleForTesting
-    static EndpointsForToken resolveWriteConflictsInPending(EndpointsForToken natural, EndpointsForToken pending)
-    {
+    static EndpointsForToken resolveWriteConflictsInPending(EndpointsForToken natural, EndpointsForToken pending) {
         return pending.without(natural.endpoints());
     }
 
     /**
-     * @return the read layout for a token - this includes only live natural replicas, i.e. those that are not pending
-     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
+     * @return the read layout for a token - this includes only live natural
+     *         replicas, i.e. those that are not pending
+     *         and not marked down by the failure detector. these are reverse sorted
+     *         by the badness score of the configured snitch
      */
-    public static ReplicaLayout.ForTokenRead forTokenReadLiveSorted(AbstractReplicationStrategy replicationStrategy, Token token)
-    {
+    public static ReplicaLayout.ForTokenRead forTokenReadLiveSorted(AbstractReplicationStrategy replicationStrategy,
+            Token token) {
         EndpointsForToken replicas = replicationStrategy.getNaturalReplicasForToken(token);
-        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);
+        // replicas =
+        // DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(),
+        // replicas);
         replicas = replicas.filter(FailureDetector.isReplicaAlive);
         return new ReplicaLayout.ForTokenRead(replicationStrategy, replicas);
     }
 
     /**
-     * TODO: we should really double check that the provided range does not overlap multiple token ring regions
-     * @return the read layout for a range - this includes only live natural replicas, i.e. those that are not pending
-     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
+     * TODO: we should really double check that the provided range does not overlap
+     * multiple token ring regions
+     * 
+     * @return the read layout for a range - this includes only live natural
+     *         replicas, i.e. those that are not pending
+     *         and not marked down by the failure detector. these are reverse sorted
+     *         by the badness score of the configured snitch
      */
-    static ReplicaLayout.ForRangeRead forRangeReadLiveSorted(AbstractReplicationStrategy replicationStrategy, AbstractBounds<PartitionPosition> range)
-    {
+    static ReplicaLayout.ForRangeRead forRangeReadLiveSorted(AbstractReplicationStrategy replicationStrategy,
+            AbstractBounds<PartitionPosition> range) {
         EndpointsForRange replicas = replicationStrategy.getNaturalReplicas(range.right);
-        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);
+        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(),
+                replicas);
         replicas = replicas.filter(FailureDetector.isReplicaAlive);
         return new ReplicaLayout.ForRangeRead(replicationStrategy, range, replicas);
     }
diff --git a/src/java/org/apache/cassandra/schema/TableMetadata.java b/src/java/org/apache/cassandra/schema/TableMetadata.java
index 2e9d507b..b1aa3b77 100644
--- a/src/java/org/apache/cassandra/schema/TableMetadata.java
+++ b/src/java/org/apache/cassandra/schema/TableMetadata.java
@@ -52,80 +52,84 @@ import static java.util.stream.Collectors.toSet;
 import static org.apache.cassandra.schema.IndexMetadata.isNameValid;
 
 @Unmetered
-public class TableMetadata implements SchemaElement
-{
+public class TableMetadata implements SchemaElement {
     private static final Logger logger = LoggerFactory.getLogger(TableMetadata.class);
 
-    // Please note that currently the only one truly useful flag is COUNTER, as the rest of the flags were about
-    // differencing between CQL tables and the various types of COMPACT STORAGE tables (pre-4.0). As those "compact"
-    // tables are not supported anymore, no tables should be either SUPER or DENSE, and they should all be COMPOUND.
-    public enum Flag
-    {
-        // As mentioned above, all tables on 4.0+ will have the COMPOUND flag, making the flag of little value. However,
-        // on upgrade from pre-4.0, we want to detect if a tables does _not_ have this flag, in which case this would
-        // be a compact table on which DROP COMPACT STORAGE has _not_ be used and fail startup. This is also why we
-        // still write this flag for all tables. Once we drop support for upgrading from pre-4.0 versions (and so are
-        // sure all tables do have the flag), we can stop writing this flag and ignore it when present (deprecate it).
+    // Please note that currently the only one truly useful flag is COUNTER, as the
+    // rest of the flags were about
+    // differencing between CQL tables and the various types of COMPACT STORAGE
+    // tables (pre-4.0). As those "compact"
+    // tables are not supported anymore, no tables should be either SUPER or DENSE,
+    // and they should all be COMPOUND.
+    public enum Flag {
+        // As mentioned above, all tables on 4.0+ will have the COMPOUND flag, making
+        // the flag of little value. However,
+        // on upgrade from pre-4.0, we want to detect if a tables does _not_ have this
+        // flag, in which case this would
+        // be a compact table on which DROP COMPACT STORAGE has _not_ be used and fail
+        // startup. This is also why we
+        // still write this flag for all tables. Once we drop support for upgrading from
+        // pre-4.0 versions (and so are
+        // sure all tables do have the flag), we can stop writing this flag and ignore
+        // it when present (deprecate it).
         // Later, we'll be able to drop the flag from this enum completely.
         COMPOUND,
         DENSE,
         COUNTER,
-        // The only reason we still have those is that on the first startup after an upgrade from pre-4.0, we cannot
-        // guarantee some tables won't have those flags (users having forgotten to use DROP COMPACT STORAGE before
-        // upgrading). So we still "deserialize" those flags correctly, but otherwise prevent startup if any table
-        // have them. Once we drop support for upgrading from pre-4.0, we can remove those values.
-        @Deprecated SUPER;
+        // The only reason we still have those is that on the first startup after an
+        // upgrade from pre-4.0, we cannot
+        // guarantee some tables won't have those flags (users having forgotten to use
+        // DROP COMPACT STORAGE before
+        // upgrading). So we still "deserialize" those flags correctly, but otherwise
+        // prevent startup if any table
+        // have them. Once we drop support for upgrading from pre-4.0, we can remove
+        // those values.
+        @Deprecated
+        SUPER;
 
         /*
-         *  We call dense a CF for which each component of the comparator is a clustering column, i.e. no
-         * component is used to store a regular column names. In other words, non-composite static "thrift"
+         * We call dense a CF for which each component of the comparator is a clustering
+         * column, i.e. no
+         * component is used to store a regular column names. In other words,
+         * non-composite static "thrift"
          * and CQL3 CF are *not* dense.
          */
-        public static boolean isDense(Set<TableMetadata.Flag> flags)
-        {
+        public static boolean isDense(Set<TableMetadata.Flag> flags) {
             return flags.contains(TableMetadata.Flag.DENSE);
         }
 
-        public static boolean isCompound(Set<TableMetadata.Flag> flags)
-        {
+        public static boolean isCompound(Set<TableMetadata.Flag> flags) {
             return flags.contains(TableMetadata.Flag.COMPOUND);
         }
 
-
-        public static boolean isSuper(Set<TableMetadata.Flag> flags)
-        {
+        public static boolean isSuper(Set<TableMetadata.Flag> flags) {
             return flags.contains(TableMetadata.Flag.SUPER);
         }
 
-        public static boolean isCQLTable(Set<TableMetadata.Flag> flags)
-        {
+        public static boolean isCQLTable(Set<TableMetadata.Flag> flags) {
             return !isSuper(flags) && !isDense(flags) && isCompound(flags);
         }
 
-        public static boolean isStaticCompactTable(Set<TableMetadata.Flag> flags)
-        {
+        public static boolean isStaticCompactTable(Set<TableMetadata.Flag> flags) {
             return !Flag.isSuper(flags) && !Flag.isDense(flags) && !Flag.isCompound(flags);
         }
 
-        public static Set<Flag> fromStringSet(Set<String> strings)
-        {
+        public static Set<Flag> fromStringSet(Set<String> strings) {
             return strings.stream().map(String::toUpperCase).map(Flag::valueOf).collect(toSet());
         }
 
-        public static Set<String> toStringSet(Set<Flag> flags)
-        {
+        public static Set<String> toStringSet(Set<Flag> flags) {
             return flags.stream().map(Flag::toString).map(String::toLowerCase).collect(toSet());
         }
     }
 
-    public enum Kind
-    {
+    public enum Kind {
         REGULAR, INDEX, VIEW, VIRTUAL
     }
 
     public final String keyspace;
-    public final String name;
-    public final TableId id;
+    public String name;
+    public TableId id;
 
     public final IPartitioner partitioner;
     public final Kind kind;
@@ -139,7 +143,8 @@ public class TableMetadata implements SchemaElement
      * All CQL3 columns definition are stored in the columns map.
      * On top of that, we keep separated collection of each kind of definition, to
      * 1) allow easy access to each kind and
-     * 2) for the partition key and clustering key ones, those list are ordered by the "component index" of the elements.
+     * 2) for the partition key and clustering key ones, those list are ordered by
+     * the "component index" of the elements.
      */
     public final ImmutableMap<ByteBuffer, DroppedColumn> droppedColumns;
     final ImmutableMap<ByteBuffer, ColumnMetadata> columns;
@@ -158,8 +163,7 @@ public class TableMetadata implements SchemaElement
     // performance hacks; TODO see if all are really necessary
     public final DataResource resource;
 
-    protected TableMetadata(Builder builder)
-    {
+    protected TableMetadata(Builder builder) {
         flags = Sets.immutableEnumSet(builder.flags);
         keyspace = builder.keyspace;
         name = builder.name;
@@ -183,132 +187,111 @@ public class TableMetadata implements SchemaElement
         triggers = builder.triggers;
 
         partitionKeyType = partitionKeyColumns.size() == 1
-                         ? partitionKeyColumns.get(0).type
-                         : CompositeType.getInstance(transform(partitionKeyColumns, t -> t.type));
+                ? partitionKeyColumns.get(0).type
+                : CompositeType.getInstance(transform(partitionKeyColumns, t -> t.type));
 
         comparator = new ClusteringComparator(transform(clusteringColumns, c -> c.type));
 
         resource = DataResource.table(keyspace, name);
     }
 
-    public static Builder builder(String keyspace, String table)
-    {
+    public static Builder builder(String keyspace, String table) {
         return new Builder(keyspace, table);
     }
 
-    public static Builder builder(String keyspace, String table, TableId id)
-    {
+    public static Builder builder(String keyspace, String table, TableId id) {
         return new Builder(keyspace, table, id);
     }
 
-    public Builder unbuild()
-    {
+    public Builder unbuild() {
         return builder(keyspace, name, id)
-               .partitioner(partitioner)
-               .kind(kind)
-               .params(params)
-               .flags(flags)
-               .addColumns(columns())
-               .droppedColumns(droppedColumns)
-               .indexes(indexes)
-               .triggers(triggers);
-    }
-
-    public boolean isIndex()
-    {
+                .partitioner(partitioner)
+                .kind(kind)
+                .params(params)
+                .flags(flags)
+                .addColumns(columns())
+                .droppedColumns(droppedColumns)
+                .indexes(indexes)
+                .triggers(triggers);
+    }
+
+    public boolean isIndex() {
         return kind == Kind.INDEX;
     }
 
-    public TableMetadata withSwapped(TableParams params)
-    {
+    public TableMetadata withSwapped(TableParams params) {
         return unbuild().params(params).build();
     }
 
-    public TableMetadata withSwapped(Set<Flag> flags)
-    {
+    public TableMetadata withSwapped(Set<Flag> flags) {
         return unbuild().flags(flags).build();
     }
 
-    public TableMetadata withSwapped(Triggers triggers)
-    {
+    public TableMetadata withSwapped(Triggers triggers) {
         return unbuild().triggers(triggers).build();
     }
 
-    public TableMetadata withSwapped(Indexes indexes)
-    {
+    public TableMetadata withSwapped(Indexes indexes) {
         return unbuild().indexes(indexes).build();
     }
 
-    public boolean isView()
-    {
+    public boolean isView() {
         return kind == Kind.VIEW;
     }
 
-    public boolean isVirtual()
-    {
+    public boolean isVirtual() {
         return kind == Kind.VIRTUAL;
     }
 
-    public Optional<String> indexName()
-    {
+    public Optional<String> indexName() {
         return Optional.ofNullable(indexName);
     }
 
-    public boolean isCounter()
-    {
+    public boolean isCounter() {
         return flags.contains(Flag.COUNTER);
     }
 
-    public boolean isCompactTable()
-    {
+    public boolean isCompactTable() {
         return false;
     }
 
-    public boolean isStaticCompactTable()
-    {
+    public boolean isStaticCompactTable() {
         return false;
     }
 
-    public ImmutableCollection<ColumnMetadata> columns()
-    {
+    public ImmutableCollection<ColumnMetadata> columns() {
         return columns.values();
     }
 
-    public Iterable<ColumnMetadata> primaryKeyColumns()
-    {
+    public Iterable<ColumnMetadata> primaryKeyColumns() {
         return Iterables.concat(partitionKeyColumns, clusteringColumns);
     }
 
-    public ImmutableList<ColumnMetadata> partitionKeyColumns()
-    {
+    public ImmutableList<ColumnMetadata> partitionKeyColumns() {
         return partitionKeyColumns;
     }
 
-    public ImmutableList<ColumnMetadata> clusteringColumns()
-    {
+    public ImmutableList<ColumnMetadata> clusteringColumns() {
         return clusteringColumns;
     }
 
-    public RegularAndStaticColumns regularAndStaticColumns()
-    {
+    public RegularAndStaticColumns regularAndStaticColumns() {
         return regularAndStaticColumns;
     }
 
-    public Columns regularColumns()
-    {
+    public Columns regularColumns() {
         return regularAndStaticColumns.regulars;
     }
 
-    public Columns staticColumns()
-    {
+    public Columns staticColumns() {
         return regularAndStaticColumns.statics;
     }
 
     /*
-     * An iterator over all column definitions but that respect the order of a SELECT *.
+     * An iterator over all column definitions but that respect the order of a
+     * SELECT *.
      */
-    public Iterator<ColumnMetadata> allColumnsInSelectOrder()
-    {
+    public Iterator<ColumnMetadata> allColumnsInSelectOrder() {
         Iterator<ColumnMetadata> partitionKeyIter = partitionKeyColumns.iterator();
         Iterator<ColumnMetadata> clusteringIter = clusteringColumns.iterator();
         Iterator<ColumnMetadata> otherColumns = regularAndStaticColumns.selectOrderIterator();
@@ -317,10 +300,10 @@ public class TableMetadata implements SchemaElement
     }
 
     /**
-     * Returns an iterator over all column definitions that respect the order of the CREATE statement.
+     * Returns an iterator over all column definitions that respect the order of the
+     * CREATE statement.
      */
-    public Iterator<ColumnMetadata> allColumnsInCreateOrder()
-    {
+    public Iterator<ColumnMetadata> allColumnsInCreateOrder() {
         Iterator<ColumnMetadata> partitionKeyIter = partitionKeyColumns.iterator();
         Iterator<ColumnMetadata> clusteringIter = clusteringColumns.iterator();
         Iterator<ColumnMetadata> otherColumns = regularAndStaticColumns.iterator();
@@ -329,13 +312,10 @@ public class TableMetadata implements SchemaElement
     }
 
     private static Iterator<ColumnMetadata> columnsIterator(Iterator<ColumnMetadata> partitionKeys,
-                                                            Iterator<ColumnMetadata> clusteringColumns,
-                                                            Iterator<ColumnMetadata> otherColumns)
-    {
-        return new AbstractIterator<ColumnMetadata>()
-        {
-            protected ColumnMetadata computeNext()
-            {
+            Iterator<ColumnMetadata> clusteringColumns,
+            Iterator<ColumnMetadata> otherColumns) {
+        return new AbstractIterator<ColumnMetadata>() {
+            protected ColumnMetadata computeNext() {
                 if (partitionKeys.hasNext())
                     return partitionKeys.next();
 
@@ -350,55 +330,58 @@ public class TableMetadata implements SchemaElement
     /**
      * Returns the ColumnMetadata for {@code name}.
      */
-    public ColumnMetadata getColumn(ColumnIdentifier name)
-    {
+    public ColumnMetadata getColumn(ColumnIdentifier name) {
         return columns.get(name.bytes);
     }
+
     /**
-     * Returns the column of the provided name if it exists, but throws a user-visible exception if that column doesn't
+     * Returns the column of the provided name if it exists, but throws a
+     * user-visible exception if that column doesn't
      * exist.
      *
-     * <p>This method is for finding columns from a name provided by the user, and as such it does _not_ returne hidden
+     * <p>
+     * This method is for finding columns from a name provided by the user, and as
+     * such it does _not_ returne hidden
      * columns (throwing that the column is unknown instead).
      *
      * @param name the name of an existing non-hidden column of this table.
      * @return the column metadata corresponding to {@code name}.
      *
-     * @throws InvalidRequestException if there is no non-hidden column named {@code name} in this table.
+     * @throws InvalidRequestException if there is no non-hidden column named
+     *                                 {@code name} in this table.
      */
-    public ColumnMetadata getExistingColumn(ColumnIdentifier name)
-    {
+    public ColumnMetadata getExistingColumn(ColumnIdentifier name) {
         ColumnMetadata def = getColumn(name);
         if (def == null)
             throw new InvalidRequestException(format("Undefined column name %s in table %s", name.toCQLString(), this));
         return def;
     }
+
     /*
      * In general it is preferable to work with ColumnIdentifier to make it
      * clear that we are talking about a CQL column, not a cell name, but there
-     * is a few cases where all we have is a ByteBuffer (when dealing with IndexExpression
+     * is a few cases where all we have is a ByteBuffer (when dealing with
+     * IndexExpression
      * for instance) so...
      */
-    public ColumnMetadata getColumn(ByteBuffer name)
-    {
+    public ColumnMetadata getColumn(ByteBuffer name) {
         return columns.get(name);
     }
 
-    public ColumnMetadata getDroppedColumn(ByteBuffer name)
-    {
+    public ColumnMetadata getDroppedColumn(ByteBuffer name) {
         DroppedColumn dropped = droppedColumns.get(name);
         return dropped == null ? null : dropped.column;
     }
 
     /**
-     * Returns a "fake" ColumnMetadata corresponding to the dropped column {@code name}
+     * Returns a "fake" ColumnMetadata corresponding to the dropped column
+     * {@code name}
      * of {@code null} if there is no such dropped column.
      *
-     * @param name - the column name
+     * @param name     - the column name
      * @param isStatic - whether the column was a static column, if known
      */
-    public ColumnMetadata getDroppedColumn(ByteBuffer name, boolean isStatic)
-    {
+    public ColumnMetadata getDroppedColumn(ByteBuffer name, boolean isStatic) {
         DroppedColumn dropped = droppedColumns.get(name);
         if (dropped == null)
             return null;
@@ -409,18 +392,18 @@ public class TableMetadata implements SchemaElement
         return dropped.column;
     }
 
-    public boolean hasStaticColumns()
-    {
+    public boolean hasStaticColumns() {
         return !staticColumns().isEmpty();
     }
 
-    public void validate()
-    {
+    public void validate() {
         if (!isNameValid(keyspace))
-            except("Keyspace name must not be empty, more than %s characters long, or contain non-alphanumeric-underscore characters (got \"%s\")", SchemaConstants.NAME_LENGTH, keyspace);
+            except("Keyspace name must not be empty, more than %s characters long, or contain non-alphanumeric-underscore characters (got \"%s\")",
+                    SchemaConstants.NAME_LENGTH, keyspace);
 
         if (!isNameValid(name))
-            except("Table name must not be empty, more than %s characters long, or contain non-alphanumeric-underscore characters (got \"%s\")", SchemaConstants.NAME_LENGTH, name);
+            except("Table name must not be empty, more than %s characters long, or contain non-alphanumeric-underscore characters (got \"%s\")",
+                    SchemaConstants.NAME_LENGTH, name);
 
         params.validate();
 
@@ -428,14 +411,11 @@ public class TableMetadata implements SchemaElement
             except("PRIMARY KEY columns cannot contain counters");
 
         // Mixing counter with non counter columns is not supported (#2614)
-        if (isCounter())
-        {
+        if (isCounter()) {
             for (ColumnMetadata column : regularAndStaticColumns)
                 if (!(column.type.isCounter()) && !isSuperColumnMapColumnName(column.name))
                     except("Cannot have a non counter column (\"%s\") in a counter table", column.name);
-        }
-        else
-        {
+        } else {
             for (ColumnMetadata column : regularAndStaticColumns)
                 if (column.type.isCounter())
                     except("Cannot have a counter column (\"%s\") in a non counter table", column.name);
@@ -449,28 +429,37 @@ public class TableMetadata implements SchemaElement
     }
 
     /**
-     * To support backward compatibility with thrift super columns in the C* 3.0+ storage engine, we encode said super
-     * columns as a CQL {@code map<blob, blob>}. To ensure the name of this map did not conflict with any other user
-     * defined columns, we used the empty name (which is otherwise not allowed for user created columns).
+     * To support backward compatibility with thrift super columns in the C* 3.0+
+     * storage engine, we encode said super
+     * columns as a CQL {@code map<blob, blob>}. To ensure the name of this map did
+     * not conflict with any other user
+     * defined columns, we used the empty name (which is otherwise not allowed for
+     * user created columns).
      * <p>
-     * While all thrift-based tables must have been converted to "CQL" ones with "DROP COMPACT STORAGE" (before
-     * upgrading to C* 4.0, which stop supporting non-CQL tables completely), a converted super-column table will still
-     * have this map with an empty name. And the reason we need to recognize it still, is that for backward
-     * compatibility we need to support counters in values of this map while it's not supported in any other map.
+     * While all thrift-based tables must have been converted to "CQL" ones with
+     * "DROP COMPACT STORAGE" (before
+     * upgrading to C* 4.0, which stop supporting non-CQL tables completely), a
+     * converted super-column table will still
+     * have this map with an empty name. And the reason we need to recognize it
+     * still, is that for backward
+     * compatibility we need to support counters in values of this map while it's
+     * not supported in any other map.
      *
-     * TODO: it's probably worth lifting the limitation of not allowing counters as map values. It works fully
-     *   internally (since we had to support it for this special map) and doesn't feel particularly dangerous to
-     *   support. Doing so would remove this special case, but would also let user that do have an upgraded super-column
-     *   table with counters to rename that weirdly name map to something more meaningful (it's not possible today
-     *   as after renaming the validation in {@link #validate)} would trigger).
+     * TODO: it's probably worth lifting the limitation of not allowing counters as
+     * map values. It works fully
+     * internally (since we had to support it for this special map) and doesn't feel
+     * particularly dangerous to
+     * support. Doing so would remove this special case, but would also let user
+     * that do have an upgraded super-column
+     * table with counters to rename that weirdly name map to something more
+     * meaningful (it's not possible today
+     * as after renaming the validation in {@link #validate)} would trigger).
      */
-    private static boolean isSuperColumnMapColumnName(ColumnIdentifier columnName)
-    {
+    private static boolean isSuperColumnMapColumnName(ColumnIdentifier columnName) {
         return !columnName.bytes.hasRemaining();
     }
 
-    void validateCompatibility(TableMetadata previous)
-    {
+    void validateCompatibility(TableMetadata previous) {
         if (isIndex())
             return;
 
@@ -486,97 +475,90 @@ public class TableMetadata implements SchemaElement
         if (!previous.flags.equals(flags) && (!Flag.isCQLTable(flags) || Flag.isCQLTable(previous.flags)))
             except("Table type mismatch (found %s; expected %s)", flags, previous.flags);
 
-        if (previous.partitionKeyColumns.size() != partitionKeyColumns.size())
-        {
+        if (previous.partitionKeyColumns.size() != partitionKeyColumns.size()) {
             except("Partition keys of different length (found %s; expected %s)",
-                   partitionKeyColumns.size(),
-                   previous.partitionKeyColumns.size());
+                    partitionKeyColumns.size(),
+                    previous.partitionKeyColumns.size());
         }
 
-        for (int i = 0; i < partitionKeyColumns.size(); i++)
-        {
-            if (!partitionKeyColumns.get(i).type.isCompatibleWith(previous.partitionKeyColumns.get(i).type))
-            {
+        for (int i = 0; i < partitionKeyColumns.size(); i++) {
+            if (!partitionKeyColumns.get(i).type.isCompatibleWith(previous.partitionKeyColumns.get(i).type)) {
                 except("Partition key column mismatch (found %s; expected %s)",
-                       partitionKeyColumns.get(i).type,
-                       previous.partitionKeyColumns.get(i).type);
+                        partitionKeyColumns.get(i).type,
+                        previous.partitionKeyColumns.get(i).type);
             }
         }
 
-        if (previous.clusteringColumns.size() != clusteringColumns.size())
-        {
+        if (previous.clusteringColumns.size() != clusteringColumns.size()) {
             except("Clustering columns of different length (found %s; expected %s)",
-                   clusteringColumns.size(),
-                   previous.clusteringColumns.size());
+                    clusteringColumns.size(),
+                    previous.clusteringColumns.size());
         }
 
-        for (int i = 0; i < clusteringColumns.size(); i++)
-        {
-            if (!clusteringColumns.get(i).type.isCompatibleWith(previous.clusteringColumns.get(i).type))
-            {
+        for (int i = 0; i < clusteringColumns.size(); i++) {
+            if (!clusteringColumns.get(i).type.isCompatibleWith(previous.clusteringColumns.get(i).type)) {
                 except("Clustering column mismatch (found %s; expected %s)",
-                       clusteringColumns.get(i).type,
-                       previous.clusteringColumns.get(i).type);
+                        clusteringColumns.get(i).type,
+                        previous.clusteringColumns.get(i).type);
             }
         }
 
-        for (ColumnMetadata previousColumn : previous.regularAndStaticColumns)
-        {
+        for (ColumnMetadata previousColumn : previous.regularAndStaticColumns) {
             ColumnMetadata column = getColumn(previousColumn.name);
             if (column != null && !column.type.isCompatibleWith(previousColumn.type))
                 except("Column mismatch (found %s; expected %s)", column, previousColumn);
         }
     }
 
-    public ClusteringComparator partitionKeyAsClusteringComparator()
-    {
+    public ClusteringComparator partitionKeyAsClusteringComparator() {
         return new ClusteringComparator(partitionKeyColumns.stream().map(c -> c.type).collect(toList()));
     }
 
     /**
      * Generate a table name for an index corresponding to the given column.
-     * This is NOT the same as the index's name! This is only used in sstable filenames and is not exposed to users.
+     * This is NOT the same as the index's name! This is only used in sstable
+     * filenames and is not exposed to users.
      *
      * @param info A definition of the column with index
      *
      * @return name of the index table
      */
-    public String indexTableName(IndexMetadata info)
-    {
+    public String indexTableName(IndexMetadata info) {
         // TODO simplify this when info.index_name is guaranteed to be set
         return name + Directories.SECONDARY_INDEX_NAME_SEPARATOR + info.name;
     }
 
     /**
      * @return true if the change as made impacts queries/updates on the table,
-     *         e.g. any columns or indexes were added, removed, or altered; otherwise, false is returned.
-     *         Used to determine whether prepared statements against this table need to be re-prepared.
+     *         e.g. any columns or indexes were added, removed, or altered;
+     *         otherwise, false is returned.
+     *         Used to determine whether prepared statements against this table need
+     *         to be re-prepared.
      */
-    boolean changeAffectsPreparedStatements(TableMetadata updated)
-    {
+    boolean changeAffectsPreparedStatements(TableMetadata updated) {
         return !partitionKeyColumns.equals(updated.partitionKeyColumns)
-            || !clusteringColumns.equals(updated.clusteringColumns)
-            || !regularAndStaticColumns.equals(updated.regularAndStaticColumns)
-            || !indexes.equals(updated.indexes)
-            || params.defaultTimeToLive != updated.params.defaultTimeToLive
-            || params.gcGraceSeconds != updated.params.gcGraceSeconds
-            || ( !Flag.isCQLTable(flags) && Flag.isCQLTable(updated.flags) );
+                || !clusteringColumns.equals(updated.clusteringColumns)
+                || !regularAndStaticColumns.equals(updated.regularAndStaticColumns)
+                || !indexes.equals(updated.indexes)
+                || params.defaultTimeToLive != updated.params.defaultTimeToLive
+                || params.gcGraceSeconds != updated.params.gcGraceSeconds
+                || (!Flag.isCQLTable(flags) && Flag.isCQLTable(updated.flags));
     }
 
     /**
-     * There is a couple of places in the code where we need a TableMetadata object and don't have one readily available
-     * and know that only the keyspace and name matter. This creates such "fake" metadata. Use only if you know what
+     * There is a couple of places in the code where we need a TableMetadata object
+     * and don't have one readily available
+     * and know that only the keyspace and name matter. This creates such "fake"
+     * metadata. Use only if you know what
      * you're doing.
      */
-    public static TableMetadata minimal(String keyspace, String name)
-    {
+    public static TableMetadata minimal(String keyspace, String name) {
         return TableMetadata.builder(keyspace, name)
-                            .addPartitionKeyColumn("key", BytesType.instance)
-                            .build();
+                .addPartitionKeyColumn("key", BytesType.instance)
+                .build();
     }
 
-    public TableMetadata updateIndexTableMetadata(TableParams baseTableParams)
-    {
+    public TableMetadata updateIndexTableMetadata(TableParams baseTableParams) {
         TableParams.Builder builder = baseTableParams.unbuild().gcGraceSeconds(0);
 
         // Depends on parent's cache setting, turn on its index table's cache.
@@ -586,13 +568,11 @@ public class TableMetadata implements SchemaElement
         return unbuild().params(builder.build()).build();
     }
 
-    boolean referencesUserType(ByteBuffer name)
-    {
+    boolean referencesUserType(ByteBuffer name) {
         return any(columns(), c -> c.type.referencesUserType(name));
     }
 
-    public TableMetadata withUpdatedUserType(UserType udt)
-    {
+    public TableMetadata withUpdatedUserType(UserType udt) {
         if (!referencesUserType(udt.name))
             return this;
 
@@ -602,14 +582,12 @@ public class TableMetadata implements SchemaElement
         return builder.build();
     }
 
-    protected void except(String format, Object... args)
-    {
+    protected void except(String format, Object... args) {
         throw new ConfigurationException(keyspace + "." + name + ": " + format(format, args));
     }
 
     @Override
-    public boolean equals(Object o)
-    {
+    public boolean equals(Object o) {
         if (this == o)
             return true;
 
@@ -621,44 +599,38 @@ public class TableMetadata implements SchemaElement
         return equalsWithoutColumns(tm) && columns.equals(tm.columns);
     }
 
-    private boolean equalsWithoutColumns(TableMetadata tm)
-    {
+    private boolean equalsWithoutColumns(TableMetadata tm) {
         return keyspace.equals(tm.keyspace)
-            && name.equals(tm.name)
-            && id.equals(tm.id)
-            && partitioner.equals(tm.partitioner)
-            && kind == tm.kind
-            && params.equals(tm.params)
-            && flags.equals(tm.flags)
-            && droppedColumns.equals(tm.droppedColumns)
-            && indexes.equals(tm.indexes)
-            && triggers.equals(tm.triggers);
-    }
-
-    Optional<Difference> compare(TableMetadata other)
-    {
+                && name.equals(tm.name)
+                && id.equals(tm.id)
+                && partitioner.equals(tm.partitioner)
+                && kind == tm.kind
+                && params.equals(tm.params)
+                && flags.equals(tm.flags)
+                && droppedColumns.equals(tm.droppedColumns)
+                && indexes.equals(tm.indexes)
+                && triggers.equals(tm.triggers);
+    }
+
+    Optional<Difference> compare(TableMetadata other) {
         return equalsWithoutColumns(other)
-             ? compareColumns(other.columns)
-             : Optional.of(Difference.SHALLOW);
+                ? compareColumns(other.columns)
+                : Optional.of(Difference.SHALLOW);
     }
 
-    private Optional<Difference> compareColumns(Map<ByteBuffer, ColumnMetadata> other)
-    {
+    private Optional<Difference> compareColumns(Map<ByteBuffer, ColumnMetadata> other) {
         if (!columns.keySet().equals(other.keySet()))
             return Optional.of(Difference.SHALLOW);
 
         boolean differsDeeply = false;
 
-        for (Map.Entry<ByteBuffer, ColumnMetadata> entry : columns.entrySet())
-        {
+        for (Map.Entry<ByteBuffer, ColumnMetadata> entry : columns.entrySet()) {
             ColumnMetadata thisColumn = entry.getValue();
             ColumnMetadata thatColumn = other.get(entry.getKey());
 
             Optional<Difference> difference = thisColumn.compare(thatColumn);
-            if (difference.isPresent())
-            {
-                switch (difference.get())
-                {
+            if (difference.isPresent()) {
+                switch (difference.get()) {
                     case SHALLOW:
                         return difference;
                     case DEEP:
@@ -671,36 +643,33 @@ public class TableMetadata implements SchemaElement
     }
 
     @Override
-    public int hashCode()
-    {
-        return Objects.hash(keyspace, name, id, partitioner, kind, params, flags, columns, droppedColumns, indexes, triggers);
+    public int hashCode() {
+        return Objects.hash(keyspace, name, id, partitioner, kind, params, flags, columns, droppedColumns, indexes,
+                triggers);
     }
 
     @Override
-    public String toString()
-    {
+    public String toString() {
         return format("%s.%s", ColumnIdentifier.maybeQuote(keyspace), ColumnIdentifier.maybeQuote(name));
     }
 
-    public String toDebugString()
-    {
+    public String toDebugString() {
         return MoreObjects.toStringHelper(this)
-                          .add("keyspace", keyspace)
-                          .add("table", name)
-                          .add("id", id)
-                          .add("partitioner", partitioner)
-                          .add("kind", kind)
-                          .add("params", params)
-                          .add("flags", flags)
-                          .add("columns", columns())
-                          .add("droppedColumns", droppedColumns.values())
-                          .add("indexes", indexes)
-                          .add("triggers", triggers)
-                          .toString();
-    }
-
-    public static final class Builder
-    {
+                .add("keyspace", keyspace)
+                .add("table", name)
+                .add("id", id)
+                .add("partitioner", partitioner)
+                .add("kind", kind)
+                .add("params", params)
+                .add("flags", flags)
+                .add("columns", columns())
+                .add("droppedColumns", droppedColumns.values())
+                .add("indexes", indexes)
+                .add("triggers", triggers)
+                .toString();
+    }
+
+    public static final class Builder {
         final String keyspace;
         final String name;
 
@@ -710,7 +679,8 @@ public class TableMetadata implements SchemaElement
         private Kind kind = Kind.REGULAR;
         private TableParams.Builder params = TableParams.builder();
 
-        // See the comment on Flag.COMPOUND definition for why we (still) inconditionally add this flag.
+        // See the comment on Flag.COMPOUND definition for why we (still)
+        // inconditionally add this flag.
         private Set<Flag> flags = EnumSet.of(Flag.COMPOUND);
         private Triggers triggers = Triggers.none();
         private Indexes indexes = Indexes.none();
@@ -721,30 +691,29 @@ public class TableMetadata implements SchemaElement
         private final List<ColumnMetadata> clusteringColumns = new ArrayList<>();
         private final List<ColumnMetadata> regularAndStaticColumns = new ArrayList<>();
 
-        private Builder(String keyspace, String name, TableId id)
-        {
+        private Builder(String keyspace, String name, TableId id) {
             this.keyspace = keyspace;
             this.name = name;
             this.id = id;
         }
 
-        private Builder(String keyspace, String name)
-        {
+        private Builder(String keyspace, String name) {
             this.keyspace = keyspace;
             this.name = name;
         }
 
-        public TableMetadata build()
-        {
+        public TableMetadata build() {
             if (partitioner == null)
                 partitioner = DatabaseDescriptor.getPartitioner();
 
-            if (id == null)
-            {
-                // make sure vtables use determiniestic ids so they can be referenced in calls cross-nodes
+            if (id == null) {
+                // make sure vtables use determiniestic ids so they can be referenced in calls
+                // cross-nodes
                 // see CASSANDRA-17295
-                if (DatabaseDescriptor.useDeterministicTableID() || kind == Kind.VIRTUAL) id = TableId.unsafeDeterministic(keyspace, name);
-                else id = TableId.generate();
+                if (DatabaseDescriptor.useDeterministicTableID() || kind == Kind.VIRTUAL)
+                    id = TableId.unsafeDeterministic(keyspace, name);
+                else
+                    id = TableId.generate();
             }
 
             if (Flag.isCQLTable(flags))
@@ -753,197 +722,169 @@ public class TableMetadata implements SchemaElement
                 return new CompactTableMetadata(this);
         }
 
-        public Builder id(TableId val)
-        {
+        public Builder id(TableId val) {
             id = val;
             return this;
         }
 
-        public Builder partitioner(IPartitioner val)
-        {
+        public Builder partitioner(IPartitioner val) {
             partitioner = val;
             return this;
         }
 
-        public Builder kind(Kind val)
-        {
+        public Builder kind(Kind val) {
             kind = val;
             return this;
         }
 
-        public Builder params(TableParams val)
-        {
+        public Builder params(TableParams val) {
             params = val.unbuild();
             return this;
         }
 
-        public Builder bloomFilterFpChance(double val)
-        {
+        public Builder bloomFilterFpChance(double val) {
             params.bloomFilterFpChance(val);
             return this;
         }
 
-        public Builder caching(CachingParams val)
-        {
+        public Builder caching(CachingParams val) {
             params.caching(val);
             return this;
         }
 
-        public Builder comment(String val)
-        {
+        public Builder comment(String val) {
             params.comment(val);
             return this;
         }
 
-        public Builder compaction(CompactionParams val)
-        {
+        public Builder compaction(CompactionParams val) {
             params.compaction(val);
             return this;
         }
 
-        public Builder compression(CompressionParams val)
-        {
+        public Builder compression(CompressionParams val) {
             params.compression(val);
             return this;
         }
 
-        public Builder defaultTimeToLive(int val)
-        {
+        public Builder defaultTimeToLive(int val) {
             params.defaultTimeToLive(val);
             return this;
         }
 
-        public Builder gcGraceSeconds(int val)
-        {
+        public Builder gcGraceSeconds(int val) {
             params.gcGraceSeconds(val);
             return this;
         }
 
-        public Builder maxIndexInterval(int val)
-        {
+        public Builder maxIndexInterval(int val) {
             params.maxIndexInterval(val);
             return this;
         }
 
-        public Builder memtableFlushPeriod(int val)
-        {
+        public Builder memtableFlushPeriod(int val) {
             params.memtableFlushPeriodInMs(val);
             return this;
         }
 
-        public Builder minIndexInterval(int val)
-        {
+        public Builder minIndexInterval(int val) {
             params.minIndexInterval(val);
             return this;
         }
 
-        public Builder crcCheckChance(double val)
-        {
+        public Builder crcCheckChance(double val) {
             params.crcCheckChance(val);
             return this;
         }
 
-        public Builder speculativeRetry(SpeculativeRetryPolicy val)
-        {
+        public Builder speculativeRetry(SpeculativeRetryPolicy val) {
             params.speculativeRetry(val);
             return this;
         }
 
-        public Builder additionalWritePolicy(SpeculativeRetryPolicy val)
-        {
+        public Builder additionalWritePolicy(SpeculativeRetryPolicy val) {
             params.additionalWritePolicy(val);
             return this;
         }
 
-        public Builder extensions(Map<String, ByteBuffer> val)
-        {
+        public Builder extensions(Map<String, ByteBuffer> val) {
             params.extensions(val);
             return this;
         }
 
-        public Builder flags(Set<Flag> val)
-        {
+        public Builder flags(Set<Flag> val) {
             flags = val;
             return this;
         }
 
-        public Builder memtable(MemtableParams val)
-        {
+        public Builder memtable(MemtableParams val) {
             params.memtable(val);
             return this;
         }
 
-
-        public Builder isCounter(boolean val)
-        {
+        public Builder isCounter(boolean val) {
             return flag(Flag.COUNTER, val);
         }
 
-        private Builder flag(Flag flag, boolean set)
-        {
-            if (set) flags.add(flag); else flags.remove(flag);
+        private Builder flag(Flag flag, boolean set) {
+            if (set)
+                flags.add(flag);
+            else
+                flags.remove(flag);
             return this;
         }
 
-        public Builder triggers(Triggers val)
-        {
+        public Builder triggers(Triggers val) {
             triggers = val;
             return this;
         }
 
-        public Builder indexes(Indexes val)
-        {
+        public Builder indexes(Indexes val) {
             indexes = val;
             return this;
         }
 
-        public Builder addPartitionKeyColumn(String name, AbstractType type)
-        {
+        public Builder addPartitionKeyColumn(String name, AbstractType type) {
             return addPartitionKeyColumn(ColumnIdentifier.getInterned(name, false), type);
         }
 
-        public Builder addPartitionKeyColumn(ColumnIdentifier name, AbstractType type)
-        {
-            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, partitionKeyColumns.size(), ColumnMetadata.Kind.PARTITION_KEY));
+        public Builder addPartitionKeyColumn(ColumnIdentifier name, AbstractType type) {
+            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, partitionKeyColumns.size(),
+                    ColumnMetadata.Kind.PARTITION_KEY));
         }
 
-        public Builder addClusteringColumn(String name, AbstractType type)
-        {
+        public Builder addClusteringColumn(String name, AbstractType type) {
             return addClusteringColumn(ColumnIdentifier.getInterned(name, false), type);
         }
 
-        public Builder addClusteringColumn(ColumnIdentifier name, AbstractType type)
-        {
-            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, clusteringColumns.size(), ColumnMetadata.Kind.CLUSTERING));
+        public Builder addClusteringColumn(ColumnIdentifier name, AbstractType type) {
+            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, clusteringColumns.size(),
+                    ColumnMetadata.Kind.CLUSTERING));
         }
 
-        public Builder addRegularColumn(String name, AbstractType type)
-        {
+        public Builder addRegularColumn(String name, AbstractType type) {
             return addRegularColumn(ColumnIdentifier.getInterned(name, false), type);
         }
 
-        public Builder addRegularColumn(ColumnIdentifier name, AbstractType type)
-        {
-            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, ColumnMetadata.NO_POSITION, ColumnMetadata.Kind.REGULAR));
+        public Builder addRegularColumn(ColumnIdentifier name, AbstractType type) {
+            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, ColumnMetadata.NO_POSITION,
+                    ColumnMetadata.Kind.REGULAR));
         }
 
-        public Builder addStaticColumn(String name, AbstractType type)
-        {
+        public Builder addStaticColumn(String name, AbstractType type) {
             return addStaticColumn(ColumnIdentifier.getInterned(name, false), type);
         }
 
-        public Builder addStaticColumn(ColumnIdentifier name, AbstractType type)
-        {
-            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, ColumnMetadata.NO_POSITION, ColumnMetadata.Kind.STATIC));
+        public Builder addStaticColumn(ColumnIdentifier name, AbstractType type) {
+            return addColumn(new ColumnMetadata(keyspace, this.name, name, type, ColumnMetadata.NO_POSITION,
+                    ColumnMetadata.Kind.STATIC));
         }
 
-        public Builder addColumn(ColumnMetadata column)
-        {
+        public Builder addColumn(ColumnMetadata column) {
             if (columns.containsKey(column.name.bytes))
                 throw new IllegalArgumentException();
 
-            switch (column.kind)
-            {
+            switch (column.kind) {
                 case PARTITION_KEY:
                     partitionKeyColumns.add(column);
                     Collections.sort(partitionKeyColumns);
@@ -962,14 +903,12 @@ public class TableMetadata implements SchemaElement
             return this;
         }
 
-        public Builder addColumns(Iterable<ColumnMetadata> columns)
-        {
+        public Builder addColumns(Iterable<ColumnMetadata> columns) {
             columns.forEach(this::addColumn);
             return this;
         }
 
-        public Builder droppedColumns(Map<ByteBuffer, DroppedColumn> droppedColumns)
-        {
+        public Builder droppedColumns(Map<ByteBuffer, DroppedColumn> droppedColumns) {
             this.droppedColumns.clear();
             this.droppedColumns.putAll(droppedColumns);
             return this;
@@ -978,56 +917,50 @@ public class TableMetadata implements SchemaElement
         /**
          * Records a deprecated column for a system table.
          */
-        public Builder recordDeprecatedSystemColumn(String name, AbstractType<?> type)
-        {
-            // As we play fast and loose with the removal timestamp, make sure this is misued for a non system table.
+        public Builder recordDeprecatedSystemColumn(String name, AbstractType<?> type) {
+            // As we play fast and loose with the removal timestamp, make sure this is
+            // misued for a non system table.
             assert SchemaConstants.isLocalSystemKeyspace(keyspace);
             recordColumnDrop(ColumnMetadata.regularColumn(keyspace, this.name, name, type), Long.MAX_VALUE);
             return this;
         }
 
-        public Builder recordColumnDrop(ColumnMetadata column, long timeMicros)
-        {
-            droppedColumns.put(column.name.bytes, new DroppedColumn(column.withNewType(column.type.expandUserTypes()), timeMicros));
+        public Builder recordColumnDrop(ColumnMetadata column, long timeMicros) {
+            droppedColumns.put(column.name.bytes,
+                    new DroppedColumn(column.withNewType(column.type.expandUserTypes()), timeMicros));
             return this;
         }
 
-        public Iterable<ColumnMetadata> columns()
-        {
+        public Iterable<ColumnMetadata> columns() {
             return columns.values();
         }
 
-        public int numColumns()
-        {
+        public int numColumns() {
             return columns.size();
         }
 
-        public Set<String> columnNames()
-        {
+        public Set<String> columnNames() {
             return columns.values().stream().map(c -> c.name.toString()).collect(toSet());
         }
 
-        public ColumnMetadata getColumn(ColumnIdentifier identifier)
-        {
+        public ColumnMetadata getColumn(ColumnIdentifier identifier) {
             return columns.get(identifier.bytes);
         }
 
-        public ColumnMetadata getColumn(ByteBuffer name)
-        {
+        public ColumnMetadata getColumn(ByteBuffer name) {
             return columns.get(name);
         }
 
-        public boolean hasRegularColumns()
-        {
+        public boolean hasRegularColumns() {
             return regularAndStaticColumns.stream().anyMatch(ColumnMetadata::isRegular);
         }
 
         /*
-         * The following methods all assume a Builder with valid set of partition key, clustering, regular and static columns.
+         * The following methods all assume a Builder with valid set of partition key,
+         * clustering, regular and static columns.
          */
 
-        public Builder removeRegularOrStaticColumn(ColumnIdentifier identifier)
-        {
+        public Builder removeRegularOrStaticColumn(ColumnIdentifier identifier) {
             ColumnMetadata column = columns.get(identifier.bytes);
             if (column == null || column.isPrimaryKeyColumn())
                 throw new IllegalArgumentException();
@@ -1038,8 +971,7 @@ public class TableMetadata implements SchemaElement
             return this;
         }
 
-        public Builder renamePrimaryKeyColumn(ColumnIdentifier from, ColumnIdentifier to)
-        {
+        public Builder renamePrimaryKeyColumn(ColumnIdentifier from, ColumnIdentifier to) {
             if (columns.containsKey(to.bytes))
                 throw new IllegalArgumentException();
 
@@ -1059,16 +991,14 @@ public class TableMetadata implements SchemaElement
             return this;
         }
 
-        Builder alterColumnType(ColumnIdentifier name, AbstractType<?> type)
-        {
+        Builder alterColumnType(ColumnIdentifier name, AbstractType<?> type) {
             ColumnMetadata column = columns.get(name.bytes);
             if (column == null)
                 throw new IllegalArgumentException();
 
             ColumnMetadata newColumn = column.withNewType(type);
 
-            switch (column.kind)
-            {
+            switch (column.kind) {
                 case PARTITION_KEY:
                     partitionKeyColumns.set(column.position(), newColumn);
                     break;
@@ -1087,7 +1017,7 @@ public class TableMetadata implements SchemaElement
             return this;
         }
     }
-    
+
     /**
      * A table with strict liveness filters/ignores rows without PK liveness info,
      * effectively tying the row liveness to its primary key liveness.
@@ -1098,8 +1028,7 @@ public class TableMetadata implements SchemaElement
      *
      * TODO: does not belong here, should be gone
      */
-    public boolean enforceStrictLiveness()
-    {
+    public boolean enforceStrictLiveness() {
         return isView() && Keyspace.open(keyspace).viewManager.getByName(name).enforceStrictLiveness();
     }
 
@@ -1108,8 +1037,7 @@ public class TableMetadata implements SchemaElement
      *
      * @return the names of all the user types referenced by this table.
      */
-    public Set<ByteBuffer> getReferencedUserTypes()
-    {
+    public Set<ByteBuffer> getReferencedUserTypes() {
         Set<ByteBuffer> types = new LinkedHashSet<>();
         columns().forEach(c -> addUserTypes(c.type, types));
         return types;
@@ -1118,82 +1046,77 @@ public class TableMetadata implements SchemaElement
     /**
      * Find all user types used by the specified type and add them to the set.
      *
-     * @param type the type to check for user types.
-     * @param types the set of UDT names to which to add new user types found in {@code type}. Note that the
-     * insertion ordering is important and ensures that if a user type A uses another user type B, then B will appear
-     * before A in iteration order.
+     * @param type  the type to check for user types.
+     * @param types the set of UDT names to which to add new user types found in
+     *              {@code type}. Note that the
+     *              insertion ordering is important and ensures that if a user type
+     *              A uses another user type B, then B will appear
+     *              before A in iteration order.
      */
-    private static void addUserTypes(AbstractType<?> type, Set<ByteBuffer> types)
-    {
-        // Reach into subtypes first, so that if the type is a UDT, it's dependencies are recreated first.
+    private static void addUserTypes(AbstractType<?> type, Set<ByteBuffer> types) {
+        // Reach into subtypes first, so that if the type is a UDT, it's dependencies
+        // are recreated first.
         type.subTypes().forEach(t -> addUserTypes(t, types));
 
         if (type.isUDT())
-            types.add(((UserType)type).name);
+            types.add(((UserType) type).name);
     }
 
     @Override
-    public SchemaElementType elementType()
-    {
+    public SchemaElementType elementType() {
         return SchemaElementType.TABLE;
     }
 
     @Override
-    public String elementKeyspace()
-    {
+    public String elementKeyspace() {
         return keyspace;
     }
 
     @Override
-    public String elementName()
-    {
+    public String elementName() {
         return name;
     }
 
     @Override
-    public String toCqlString(boolean withInternals, boolean ifNotExists)
-    {
+    public String toCqlString(boolean withInternals, boolean ifNotExists) {
         CqlBuilder builder = new CqlBuilder(2048);
         appendCqlTo(builder, withInternals, withInternals, ifNotExists);
         return builder.toString();
     }
 
     public String toCqlString(boolean includeDroppedColumns,
-                              boolean withInternals,
-                              boolean ifNotExists)
-    {
+            boolean withInternals,
+            boolean ifNotExists) {
         CqlBuilder builder = new CqlBuilder(2048);
         appendCqlTo(builder, includeDroppedColumns, withInternals, ifNotExists);
         return builder.toString();
     }
 
     public void appendCqlTo(CqlBuilder builder,
-                            boolean includeDroppedColumns,
-                            boolean withInternals,
-                            boolean ifNotExists)
-    {
+            boolean includeDroppedColumns,
+            boolean withInternals,
+            boolean ifNotExists) {
         assert !isView();
 
         String createKeyword = "CREATE";
-        if (isVirtual())
-        {
+        if (isVirtual()) {
             builder.append(String.format("/*\n" +
                     "Warning: Table %s is a virtual table and cannot be recreated with CQL.\n" +
                     "Structure, for reference:\n",
-                                         toString()));
+                    toString()));
             createKeyword = "VIRTUAL";
         }
 
         builder.append(createKeyword)
-               .append(" TABLE ");
+                .append(" TABLE ");
 
         if (ifNotExists)
             builder.append("IF NOT EXISTS ");
 
         builder.append(toString())
-               .append(" (")
-               .newLine()
-               .increaseIndent();
+                .append(" (")
+                .newLine()
+                .increaseIndent();
 
         boolean hasSingleColumnPrimaryKey = partitionKeyColumns.size() == 1 && clusteringColumns.isEmpty();
 
@@ -1203,19 +1126,18 @@ public class TableMetadata implements SchemaElement
             appendPrimaryKey(builder);
 
         builder.decreaseIndent()
-               .append(')');
+                .append(')');
 
         builder.append(" WITH ")
-               .increaseIndent();
+                .increaseIndent();
 
         appendTableOptions(builder, withInternals);
 
         builder.decreaseIndent();
 
-        if (isVirtual())
-        {
+        if (isVirtual()) {
             builder.newLine()
-                   .append("*/");
+                    .append("*/");
         }
 
         if (includeDroppedColumns)
@@ -1223,15 +1145,15 @@ public class TableMetadata implements SchemaElement
     }
 
     private void appendColumnDefinitions(CqlBuilder builder,
-                                         boolean includeDroppedColumns,
-                                         boolean hasSingleColumnPrimaryKey)
-    {
+            boolean includeDroppedColumns,
+            boolean hasSingleColumnPrimaryKey) {
         Iterator<ColumnMetadata> iter = allColumnsInCreateOrder();
-        while (iter.hasNext())
-        {
+        while (iter.hasNext()) {
             ColumnMetadata column = iter.next();
-            // If the column has been re-added after a drop, we don't include it right away. Instead, we'll add the
-            // dropped one first below, then we'll issue the DROP and then the actual ADD for this column, thus
+            // If the column has been re-added after a drop, we don't include it right away.
+            // Instead, we'll add the
+            // dropped one first below, then we'll issue the DROP and then the actual ADD
+            // for this column, thus
             // simulating the proper sequence of events.
             if (includeDroppedColumns && droppedColumns.containsKey(column.name.bytes))
                 continue;
@@ -1247,11 +1169,9 @@ public class TableMetadata implements SchemaElement
             builder.newLine();
         }
 
-        if (includeDroppedColumns)
-        {
+        if (includeDroppedColumns) {
             Iterator<DroppedColumn> iterDropped = droppedColumns.values().iterator();
-            while (iterDropped.hasNext())
-            {
+            while (iterDropped.hasNext()) {
                 DroppedColumn dropped = iterDropped.next();
                 dropped.column.appendCqlTo(builder);
 
@@ -1263,8 +1183,7 @@ public class TableMetadata implements SchemaElement
         }
     }
 
-    void appendPrimaryKey(CqlBuilder builder)
-    {
+    void appendPrimaryKey(CqlBuilder builder) {
         List<ColumnMetadata> partitionKeyColumns = partitionKeyColumns();
         List<ColumnMetadata> clusteringColumns = clusteringColumns();
 
@@ -1272,76 +1191,65 @@ public class TableMetadata implements SchemaElement
             clusteringColumns = Collections.emptyList();
 
         builder.append("PRIMARY KEY (");
-        if (partitionKeyColumns.size() > 1)
-        {
+        if (partitionKeyColumns.size() > 1) {
             builder.append('(')
-                   .appendWithSeparators(partitionKeyColumns, (b, c) -> b.append(c.name), ", ")
-                   .append(')');
-        }
-        else
-        {
+                    .appendWithSeparators(partitionKeyColumns, (b, c) -> b.append(c.name), ", ")
+                    .append(')');
+        } else {
             builder.append(partitionKeyColumns.get(0).name);
         }
 
         if (!clusteringColumns.isEmpty())
             builder.append(", ")
-                   .appendWithSeparators(clusteringColumns, (b, c) -> b.append(c.name), ", ");
+                    .appendWithSeparators(clusteringColumns, (b, c) -> b.append(c.name), ", ");
 
         builder.append(')')
-               .newLine();
+                .newLine();
     }
 
-    void appendTableOptions(CqlBuilder builder, boolean withInternals)
-    {
+    void appendTableOptions(CqlBuilder builder, boolean withInternals) {
         if (withInternals)
             builder.append("ID = ")
-                   .append(id.toString())
-                   .newLine()
-                   .append("AND ");
+                    .append(id.toString())
+                    .newLine()
+                    .append("AND ");
 
         List<ColumnMetadata> clusteringColumns = clusteringColumns();
-        if (!clusteringColumns.isEmpty())
-        {
+        if (!clusteringColumns.isEmpty()) {
             builder.append("CLUSTERING ORDER BY (")
-                   .appendWithSeparators(clusteringColumns, (b, c) -> c.appendNameAndOrderTo(b), ", ")
-                   .append(')')
-                   .newLine()
-                   .append("AND ");
+                    .appendWithSeparators(clusteringColumns, (b, c) -> c.appendNameAndOrderTo(b), ", ")
+                    .append(')')
+                    .newLine()
+                    .append("AND ");
         }
 
-        if (isVirtual())
-        {
+        if (isVirtual()) {
             builder.append("comment = ").appendWithSingleQuotes(params.comment);
-        }
-        else
-        {
+        } else {
             params.appendCqlTo(builder, isView());
         }
         builder.append(";");
     }
 
-    private void appendDropColumns(CqlBuilder builder)
-    {
-        for (Entry<ByteBuffer, DroppedColumn> entry : droppedColumns.entrySet())
-        {
+    private void appendDropColumns(CqlBuilder builder) {
+        for (Entry<ByteBuffer, DroppedColumn> entry : droppedColumns.entrySet()) {
             DroppedColumn dropped = entry.getValue();
 
             builder.newLine()
-                   .append("ALTER TABLE ")
-                   .append(toString())
-                   .append(" DROP ")
-                   .append(dropped.column.name)
-                   .append(" USING TIMESTAMP ")
-                   .append(dropped.droppedTime)
-                   .append(';');
+                    .append("ALTER TABLE ")
+                    .append(toString())
+                    .append(" DROP ")
+                    .append(dropped.column.name)
+                    .append(" USING TIMESTAMP ")
+                    .append(dropped.droppedTime)
+                    .append(';');
 
             ColumnMetadata column = getColumn(entry.getKey());
-            if (column != null)
-            {
+            if (column != null) {
                 builder.newLine()
-                       .append("ALTER TABLE ")
-                       .append(toString())
-                       .append(" ADD ");
+                        .append("ALTER TABLE ")
+                        .append(toString())
+                        .append(" ADD ");
 
                 column.appendCqlTo(builder);
 
@@ -1353,16 +1261,17 @@ public class TableMetadata implements SchemaElement
     /**
      * Returns a string representation of a partition in a CQL-friendly format.
      *
-     * For non-composite types it returns the result of {@link org.apache.cassandra.cql3.CQL3Type#toCQLLiteral}
+     * For non-composite types it returns the result of
+     * {@link org.apache.cassandra.cql3.CQL3Type#toCQLLiteral}
      * applied to the partition key.
-     * For composite types it applies {@link org.apache.cassandra.cql3.CQL3Type#toCQLLiteral} to each subkey and
+     * For composite types it applies
+     * {@link org.apache.cassandra.cql3.CQL3Type#toCQLLiteral} to each subkey and
      * combines the results into a tuple.
      *
      * @param partitionKey a partition key
      * @return CQL-like string representation of a partition key
      */
-    public String partitionKeyAsCQLLiteral(ByteBuffer partitionKey)
-    {
+    public String partitionKeyAsCQLLiteral(ByteBuffer partitionKey) {
         return primaryKeyAsCQLLiteral(partitionKey, Clustering.EMPTY);
     }
 
@@ -1370,122 +1279,106 @@ public class TableMetadata implements SchemaElement
      * Returns a string representation of a primary key in a CQL-friendly format.
      *
      * @param partitionKey the partition key part of the primary key
-     * @param clustering the clustering key part of the primary key
+     * @param clustering   the clustering key part of the primary key
      * @return a CQL-like string representation of the specified primary key
      */
-    public String primaryKeyAsCQLLiteral(ByteBuffer partitionKey, Clustering<?> clustering)
-    {
+    public String primaryKeyAsCQLLiteral(ByteBuffer partitionKey, Clustering<?> clustering) {
         int clusteringSize = clustering.size();
 
         String[] literals;
         int i = 0;
 
-        if (partitionKeyType instanceof CompositeType)
-        {
+        if (partitionKeyType instanceof CompositeType) {
             List<AbstractType<?>> components = partitionKeyType.getComponents();
             int size = components.size();
             literals = new String[size + clusteringSize];
             ByteBuffer[] values = ((CompositeType) partitionKeyType).split(partitionKey);
-            for (i = 0; i < size; i++)
-            {
+            for (i = 0; i < size; i++) {
                 literals[i] = asCQLLiteral(components.get(i), values[i]);
             }
-        }
-        else
-        {
+        } else {
             literals = new String[1 + clusteringSize];
             literals[i++] = asCQLLiteral(partitionKeyType, partitionKey);
         }
 
-        for (int j = 0; j < clusteringSize; j++)
-        {
+        for (int j = 0; j < clusteringSize; j++) {
             literals[i++] = asCQLLiteral(clusteringColumns().get(j).type, clustering.bufferAt(j));
         }
 
         return i == 1 ? literals[0] : "(" + String.join(", ", literals) + ")";
     }
 
-    private static String asCQLLiteral(AbstractType<?> type, ByteBuffer value)
-    {
+    private static String asCQLLiteral(AbstractType<?> type, ByteBuffer value) {
         return type.asCQL3Type().toCQLLiteral(value, ProtocolVersion.CURRENT);
     }
 
-    public static class CompactTableMetadata extends TableMetadata
-    {
+    public static class CompactTableMetadata extends TableMetadata {
 
         /*
-         * For dense tables, this alias the single non-PK column the table contains (since it can only have one). We keep
-         * that as convenience to access that column more easily (but we could replace calls by regularAndStaticColumns().iterator().next()
+         * For dense tables, this alias the single non-PK column the table contains
+         * (since it can only have one). We keep
+         * that as convenience to access that column more easily (but we could replace
+         * calls by regularAndStaticColumns().iterator().next()
          * for those tables in practice).
          */
         public final ColumnMetadata compactValueColumn;
 
         private final Set<ColumnMetadata> hiddenColumns;
-        protected CompactTableMetadata(Builder builder)
-        {
+
+        protected CompactTableMetadata(Builder builder) {
             super(builder);
 
             compactValueColumn = getCompactValueColumn(regularAndStaticColumns);
 
-            if (isCompactTable() && Flag.isDense(this.flags) && hasEmptyCompactValue())
-            {
+            if (isCompactTable() && Flag.isDense(this.flags) && hasEmptyCompactValue()) {
                 hiddenColumns = Collections.singleton(compactValueColumn);
-            }
-            else if (isCompactTable() && !Flag.isDense(this.flags))
-            {
+            } else if (isCompactTable() && !Flag.isDense(this.flags)) {
                 hiddenColumns = Sets.newHashSetWithExpectedSize(clusteringColumns.size() + 1);
                 hiddenColumns.add(compactValueColumn);
                 hiddenColumns.addAll(clusteringColumns);
 
-            }
-            else
-            {
+            } else {
                 hiddenColumns = Collections.emptySet();
             }
         }
 
         @Override
-        public boolean isCompactTable()
-        {
+        public boolean isCompactTable() {
             return true;
         }
 
-        public ColumnMetadata getExistingColumn(ColumnIdentifier name)
-        {
+        public ColumnMetadata getExistingColumn(ColumnIdentifier name) {
             ColumnMetadata def = getColumn(name);
             if (def == null || isHiddenColumn(def))
-                throw new InvalidRequestException(format("Undefined column name %s in table %s", name.toCQLString(), this));
+                throw new InvalidRequestException(
+                        format("Undefined column name %s in table %s", name.toCQLString(), this));
             return def;
         }
 
-        public boolean isHiddenColumn(ColumnMetadata def)
-        {
+        public boolean isHiddenColumn(ColumnMetadata def) {
             return hiddenColumns.contains(def);
         }
 
         @Override
-        public Iterator<ColumnMetadata> allColumnsInSelectOrder()
-        {
+        public Iterator<ColumnMetadata> allColumnsInSelectOrder() {
             boolean isStaticCompactTable = isStaticCompactTable();
             boolean noNonPkColumns = hasEmptyCompactValue();
 
             Iterator<ColumnMetadata> partitionKeyIter = partitionKeyColumns.iterator();
-            Iterator<ColumnMetadata> clusteringIter =
-            isStaticCompactTable ? Collections.emptyIterator() : clusteringColumns.iterator();
+            Iterator<ColumnMetadata> clusteringIter = isStaticCompactTable ? Collections.emptyIterator()
+                    : clusteringColumns.iterator();
             Iterator<ColumnMetadata> otherColumns = noNonPkColumns ? Collections.emptyIterator()
-                                                                   : (isStaticCompactTable ? staticColumns().selectOrderIterator()
-                                                                                           : regularAndStaticColumns.selectOrderIterator());
+                    : (isStaticCompactTable ? staticColumns().selectOrderIterator()
+                            : regularAndStaticColumns.selectOrderIterator());
 
             return columnsIterator(partitionKeyIter, clusteringIter, otherColumns);
         }
 
-        public ImmutableList<ColumnMetadata> createStatementClusteringColumns()
-        {
+        public ImmutableList<ColumnMetadata> createStatementClusteringColumns() {
             return isStaticCompactTable() ? ImmutableList.of() : clusteringColumns;
         }
 
-        public Iterator<ColumnMetadata> allColumnsInCreateOrder()
-        {
+        public Iterator<ColumnMetadata> allColumnsInCreateOrder() {
             boolean isStaticCompactTable = isStaticCompactTable();
             boolean noNonPkColumns = !Flag.isCQLTable(flags) && hasEmptyCompactValue();
 
@@ -1499,97 +1392,84 @@ public class TableMetadata implements SchemaElement
 
             Iterator<ColumnMetadata> otherColumns;
 
-            if (noNonPkColumns)
-            {
+            if (noNonPkColumns) {
                 otherColumns = Collections.emptyIterator();
-            }
-            else if (isStaticCompactTable)
-            {
+            } else if (isStaticCompactTable) {
                 List<ColumnMetadata> columns = new ArrayList<>();
-                for (ColumnMetadata c : regularAndStaticColumns)
-                {
+                for (ColumnMetadata c : regularAndStaticColumns) {
                     if (c.isStatic())
-                        columns.add(new ColumnMetadata(c.ksName, c.cfName, c.name, c.type, -1, ColumnMetadata.Kind.REGULAR));
+                        columns.add(new ColumnMetadata(c.ksName, c.cfName, c.name, c.type, -1,
+                                ColumnMetadata.Kind.REGULAR));
                 }
                 otherColumns = columns.iterator();
-            }
-            else
-            {
+            } else {
                 otherColumns = regularAndStaticColumns.iterator();
             }
 
             return columnsIterator(partitionKeyIter, clusteringIter, otherColumns);
         }
 
-        public boolean hasEmptyCompactValue()
-        {
+        public boolean hasEmptyCompactValue() {
             return compactValueColumn.type instanceof EmptyType;
         }
 
-        public void validate()
-        {
+        public void validate() {
             super.validate();
 
             // A compact table should always have a clustering
             if (!Flag.isCQLTable(flags) && clusteringColumns.isEmpty())
                 except("For table %s, isDense=%b, isCompound=%b, clustering=%s", toString(),
-                       Flag.isDense(flags), Flag.isCompound(flags), clusteringColumns);
+                        Flag.isDense(flags), Flag.isCompound(flags), clusteringColumns);
         }
 
-        AbstractType<?> staticCompactOrSuperTableColumnNameType()
-        {
+        AbstractType<?> staticCompactOrSuperTableColumnNameType() {
             assert isStaticCompactTable();
             return clusteringColumns.get(0).type;
         }
 
-        public AbstractType<?> columnDefinitionNameComparator(ColumnMetadata.Kind kind)
-        {
+        public AbstractType<?> columnDefinitionNameComparator(ColumnMetadata.Kind kind) {
             return (Flag.isSuper(this.flags) && kind == ColumnMetadata.Kind.REGULAR) ||
-                   (isStaticCompactTable() && kind == ColumnMetadata.Kind.STATIC)
-                   ? staticCompactOrSuperTableColumnNameType()
-                   : UTF8Type.instance;
+                    (isStaticCompactTable() && kind == ColumnMetadata.Kind.STATIC)
+                            ? staticCompactOrSuperTableColumnNameType()
+                            : UTF8Type.instance;
         }
 
         @Override
-        public boolean isStaticCompactTable()
-        {
+        public boolean isStaticCompactTable() {
             return !Flag.isSuper(flags) && !Flag.isDense(flags) && !Flag.isCompound(flags);
         }
 
         public void appendCqlTo(CqlBuilder builder,
-                                boolean includeDroppedColumns,
-                                boolean internals,
-                                boolean ifNotExists)
-        {
+                boolean includeDroppedColumns,
+                boolean internals,
+                boolean ifNotExists) {
             builder.append("/*")
-                   .newLine()
-                   .append("Warning: Table ")
-                   .append(toString())
-                   .append(" omitted because it has constructs not compatible with CQL (was created via legacy API).")
-                   .newLine()
-                   .append("Approximate structure, for reference:")
-                   .newLine()
-                   .append("(this should not be used to reproduce this schema)")
-                   .newLine()
-                   .newLine();
+                    .newLine()
+                    .append("Warning: Table ")
+                    .append(toString())
+                    .append(" omitted because it has constructs not compatible with CQL (was created via legacy API).")
+                    .newLine()
+                    .append("Approximate structure, for reference:")
+                    .newLine()
+                    .append("(this should not be used to reproduce this schema)")
+                    .newLine()
+                    .newLine();
 
             super.appendCqlTo(builder, includeDroppedColumns, internals, ifNotExists);
 
             builder.newLine()
-                   .append("*/");
+                    .append("*/");
         }
 
-        void appendTableOptions(CqlBuilder builder, boolean internals)
-        {
+        void appendTableOptions(CqlBuilder builder, boolean internals) {
             builder.append("COMPACT STORAGE")
-                   .newLine()
-                   .append("AND ");
+                    .newLine()
+                    .append("AND ");
 
             super.appendTableOptions(builder, internals);
         }
 
-        public static ColumnMetadata getCompactValueColumn(RegularAndStaticColumns columns)
-        {
+        public static ColumnMetadata getCompactValueColumn(RegularAndStaticColumns columns) {
             assert columns.regulars.simpleColumnCount() == 1 && columns.regulars.complexColumnCount() == 0;
             return columns.regulars.getSimple(0);
         }
diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java
index 03c89c8d..9fd6ca61 100644
--- a/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -85,6 +85,7 @@ import org.apache.cassandra.db.partitions.PartitionIterators;
 import org.apache.cassandra.db.partitions.PartitionUpdate;
 import org.apache.cassandra.db.partitions.UnfilteredPartitionIterator;
 import org.apache.cassandra.db.rows.RowIterator;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
 import org.apache.cassandra.db.view.ViewUtils;
 import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.exceptions.CasWriteTimeoutException;
@@ -138,6 +139,7 @@ import org.apache.cassandra.service.reads.range.RangeCommands;
 import org.apache.cassandra.service.reads.repair.ReadRepair;
 import org.apache.cassandra.tracing.Tracing;
 import org.apache.cassandra.triggers.TriggerExecutor;
+import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.Clock;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.MBeanWrapper;
@@ -371,6 +373,7 @@ public class StorageProxy implements StorageProxyMBean {
             long queryStartNanoTime)
             throws UnavailableException, IsBootstrappingException, RequestFailureException, RequestTimeoutException,
             InvalidRequestException {
+        // logger.debug("[Tinoryj] legacyCas read command to column family {}", cfName);
         final long startTimeForMetrics = nanoTime();
         try {
             TableMetadata metadata = Schema.instance.validateTable(keyspaceName, cfName);
@@ -2102,11 +2105,15 @@ public class StorageProxy implements StorageProxyMBean {
      * 4. If the digests (if any) match the data return the data
      * 5. else carry out read repair by getting data from all the nodes.
      */
+    public static void printStackTace(String msg) {
+        logger.debug("stack trace {}", new Exception(msg));
+    }
+
     private static PartitionIterator fetchRows(List<SinglePartitionReadCommand> commands,
             ConsistencyLevel consistencyLevel, long queryStartNanoTime)
             throws UnavailableException, ReadFailureException, ReadTimeoutException {
         int cmdCount = commands.size();
-        logger.debug("[Tinoryj] total read command count: {}", cmdCount);
+        // logger.debug("[Tinoryj] total read command count: {}", cmdCount);
         AbstractReadExecutor[] reads = new AbstractReadExecutor[cmdCount];
 
         // Get the replica locations, sorted by response time according to the snitch,
@@ -2188,17 +2195,9 @@ public class StorageProxy implements StorageProxyMBean {
             this.command = command;
             this.handler = handler;
             this.trackRepairedStatus = trackRepairedStatus;
-            // if (command instanceof SinglePartitionReadCommand) {
-            // logger.debug("[Tinoryj] touch SinglePartitionReadCommand in storage proxy");
-            // } else {
-            // logger.debug("[Tinoryj] touch PartitionRangeReadCommand in storage proxy");
-            // }
         }
 
         protected void runMayThrow() {
-            logger.debug(
-                    "[Tinoryj] touch read run may throw in storage proxy, local read runnable, try to read from {} in keyspace {}",
-                    command.metadata().name, command.metadata().keyspace);
             try {
                 MessageParams.reset();
 
@@ -2209,8 +2208,21 @@ public class StorageProxy implements StorageProxyMBean {
                 ReadResponse response;
                 try (ReadExecutionController controller = command.executionController(trackRepairedStatus);
                         UnfilteredPartitionIterator iterator = command.executeLocally(controller)) {
-                    response = command.createResponse(iterator, controller.getRepairedDataInfo());
-                    logger.debug("[Tinoryj] get read response in storage proxy: {}", response);
+                    if (iterator == null) {
+                        logger.debug(
+                                "[Tinoryj] Could not get {} response from table {}",
+                                command.isDigestQuery() ? "digest" : "data",
+                                command.metadata().name, FBUtilities.getBroadcastAddressAndPort());
+                        response = command.createEmptyResponse();
+                    } else {
+                        response = command.createResponse(iterator, controller.getRepairedDataInfo());
+                        ByteBuffer newDigest = response.digest(command);
+                        logger.debug(
+                                "[Tinoryj] Get {} response from table {}, {}",
+                                command.isDigestQuery() ? "digest" : "data",
+                                command.metadata().name, FBUtilities.getBroadcastAddressAndPort(),
+                                "Digest:0x" + ByteBufferUtil.bytesToHex(newDigest));
+                    }
                 } catch (RejectException e) {
                     if (!command.isTrackingWarnings())
                         throw e;
diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java
index 7b6647f5..81102531 100644
--- a/src/java/org/apache/cassandra/service/StorageService.java
+++ b/src/java/org/apache/cassandra/service/StorageService.java
@@ -188,16 +188,20 @@ public class StorageService extends NotificationBroadcasterSupport
     // [CASSANDRAEC] The following properties belong to CassandraEC.
     // [In parity node] This queue is used to receive ECMessages for erasure coding.
     public ConcurrentHashMap<InetAddressAndPort, Queue<ECMessage>> globalRecvQueues = new ConcurrentHashMap<InetAddressAndPort, Queue<ECMessage>>();
-    // [In secondary node] This map is used to read EC SSTables generate after perform ECSyncSSTable, use During erasure coding.
+    // [In secondary node] This map is used to read EC SSTables generate after
+    // perform ECSyncSSTable, use During erasure coding.
     public Map<String, DataForRewrite> globalSSTMap = new HashMap<String, DataForRewrite>();
-    // [In parity node] This map is used to store <stripID, ECMetadataContent>, generate after erasure coding, use during parity update.
-    // TODO: could be optimize 
+    // [In parity node] This map is used to store <stripID, ECMetadataContent>,
+    // generate after erasure coding, use during parity update.
+    // TODO: could be optimize
     public Map<String, ECMetadataContent> globalECMetadataMap = new HashMap<String, ECMetadataContent>();
-    // [In parity node] Generate after ResponseParity, use during real parity update.
+    // [In parity node] Generate after ResponseParity, use during real parity
+    // update.
     public ConcurrentHashMap<String, ByteBuffer[]> globalSSTHashToParityCodeMap = new ConcurrentHashMap<String, ByteBuffer[]>();
-    // [In primary node] Generate when sendSSTableToParity, use during send parity update signal.
+    // [In primary node] Generate when sendSSTableToParity, use during send parity
+    // update signal.
     public Map<String, List<InetAddressAndPort>> globalSSTHashToParityNodesMap = new HashMap<String, List<InetAddressAndPort>>();
-    // [In parity node] Generate after erasure coding, use during parity update. 
+    // [In parity node] Generate after erasure coding, use during parity update.
     public Map<String, String> globalSSTHashToStripID = new HashMap<String, String>();
     // [In every node] Record the sstHash to SSTableReader map
     public Map<String, SSTableReader> globalSSTHashToECSSTable = new HashMap<String, SSTableReader>();
@@ -797,7 +801,8 @@ public class StorageService extends NotificationBroadcasterSupport
             }
 
             @Override
-            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable) throws Exception {
+            protected void runMayThrow(DecoratedKey first, DecoratedKey last, SSTableReader ecSSTable)
+                    throws Exception {
                 // TODO Auto-generated method stub
                 throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
             }
@@ -808,7 +813,7 @@ public class StorageService extends NotificationBroadcasterSupport
                 // TODO Auto-generated method stub
                 throw new UnsupportedOperationException("Unimplemented method 'runMayThrow'");
             }
-            
+
             @Override
             protected void runMayThrow(DecoratedKey first, DecoratedKey last, ECMetadata ecMetadata,
                     String fileNamePrefix) throws Exception {
@@ -4043,9 +4048,7 @@ public class StorageService extends NotificationBroadcasterSupport
      * @return collection of ranges that match ring layout in TokenMetadata
      */
     @VisibleForTesting
-    public
-    Collection<Range<Token>> createRepairRangeFrom(String beginToken, String endToken)
-    {
+    public Collection<Range<Token>> createRepairRangeFrom(String beginToken, String endToken) {
         Token parsedBeginToken = getTokenFactory().fromString(beginToken);
         Token parsedEndToken = getTokenFactory().fromString(endToken);
 
@@ -4333,6 +4336,14 @@ public class StorageService extends NotificationBroadcasterSupport
         return inetList;
     }
 
+    // [CASSANDRAEC]
+    public List<InetAddressAndPort> getNaturalEndpointsForCassandraEC(String keyspaceName, ByteBuffer key) {
+        EndpointsForToken replicas = getNaturalReplicasForToken(keyspaceName, key);
+        List<InetAddressAndPort> inetList = new ArrayList<>(replicas.size());
+        replicas.forEach(r -> inetList.add(r.endpoint()));
+        return inetList;
+    }
+
     public List<String> getNaturalEndpointsWithPort(String keyspaceName, ByteBuffer key) {
         EndpointsForToken replicas = getNaturalReplicasForToken(keyspaceName, key);
         return Replicas.stringify(replicas, true);
@@ -4355,17 +4366,18 @@ public class StorageService extends NotificationBroadcasterSupport
     }
 
     // [CASSANDRAEC]
-    public List<InetAddressAndPort> getReplicaNodesWithPortFromPrimaryNode(InetAddressAndPort primaryNode, String keyspaceName) {
+    public List<InetAddressAndPort> getReplicaNodesWithPortFromPrimaryNode(InetAddressAndPort primaryNode,
+            String keyspaceName) {
         List<InetAddressAndPort> liveEndpoints = new ArrayList<>(Gossiper.instance.getLiveMembers());
         List<InetAddressAndPort> replicaNodes = new ArrayList<>();
         int rf = Keyspace.open(keyspaceName).getAllReplicationFactor();
 
         int startIndex = liveEndpoints.indexOf(primaryNode);
         int endIndex = startIndex + rf;
-        
-        if(endIndex > liveEndpoints.size()) {
+
+        if (endIndex > liveEndpoints.size()) {
             replicaNodes.addAll(liveEndpoints.subList(startIndex, liveEndpoints.size()));
-            replicaNodes.addAll(liveEndpoints.subList(0,endIndex % liveEndpoints.size()));
+            replicaNodes.addAll(liveEndpoints.subList(0, endIndex % liveEndpoints.size()));
         } else {
             replicaNodes.addAll(liveEndpoints.subList(startIndex, endIndex));
         }
@@ -4376,7 +4388,8 @@ public class StorageService extends NotificationBroadcasterSupport
     public List<InetAddress> getNaturalEndpointsForToken(String keyspaceName, String tokenStr) {
         List<InetAddress> inetList = new ArrayList<>();
         Token token = getTokenFactory().fromString(tokenStr);
-        EndpointsForToken replicas = Keyspace.open(keyspaceName).getReplicationStrategy().getNaturalReplicasForToken(token);
+        EndpointsForToken replicas = Keyspace.open(keyspaceName).getReplicationStrategy()
+                .getNaturalReplicasForToken(token);
         replicas.forEach(r -> inetList.add(r.endpoint().getAddress()));
         return inetList;
     }
@@ -4396,12 +4409,12 @@ public class StorageService extends NotificationBroadcasterSupport
 
     public ByteBuffer partitionKeyToBytes(String keyspaceName, String cf, String key) {
         KeyspaceMetadata ksMetaData = Schema.instance.getKeyspaceMetadata(keyspaceName);
-        
+
         if (ksMetaData == null)
             throw new IllegalArgumentException("Unknown keyspace '" + keyspaceName + "'");
 
         TableMetadata metadata = ksMetaData.getTableOrViewNullable(cf);
-        
+
         if (metadata == null)
             throw new IllegalArgumentException("Unknown table '" + cf + "' in keyspace '" + keyspaceName + "'");
 
diff --git a/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java b/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java
index fe79f9fe..1c598303 100644
--- a/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java
+++ b/src/java/org/apache/cassandra/service/reads/AbstractReadExecutor.java
@@ -29,9 +29,12 @@ import org.apache.cassandra.db.ConsistencyLevel;
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.Keyspace;
 import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.RegularAndStaticColumns;
 import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.filter.ColumnFilter;
 import org.apache.cassandra.db.partitions.PartitionIterator;
 import org.apache.cassandra.db.transform.DuplicateRowChecker;
+import org.apache.cassandra.dht.Token;
 import org.apache.cassandra.exceptions.ReadFailureException;
 import org.apache.cassandra.exceptions.ReadTimeoutException;
 import org.apache.cassandra.exceptions.UnavailableException;
@@ -51,6 +54,7 @@ import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.service.StorageService;
 import static com.google.common.collect.Iterables.all;
 import static java.util.concurrent.TimeUnit.MICROSECONDS;
+import java.lang.reflect.Field;
 
 /**
  * Sends a read request to the replicas needed to satisfy a given
@@ -67,16 +71,22 @@ import static java.util.concurrent.TimeUnit.MICROSECONDS;
 public abstract class AbstractReadExecutor {
     private static final Logger logger = LoggerFactory.getLogger(AbstractReadExecutor.class);
 
-    protected final ReadCommand command;
+    protected ReadCommand command;
     private final ReplicaPlan.SharedForTokenRead replicaPlan;
     protected final ReadRepair<EndpointsForToken, ReplicaPlan.ForTokenRead> readRepair;
     protected final DigestResolver<EndpointsForToken, ReplicaPlan.ForTokenRead> digestResolver;
     protected final ReadCallback<EndpointsForToken, ReplicaPlan.ForTokenRead> handler;
     protected final TraceState traceState;
-    protected final ColumnFamilyStore cfs;
+    protected ColumnFamilyStore cfs;
     protected final long queryStartNanoTime;
     private final int initialDataRequestCount;
     protected volatile PartitionIterator result = null;
+    public static List<InetAddressAndPort> sendRequestAddressesAndPorts;
+    public static Token targetReadToken;
+
+    public final String primaryLSMTreeName = "usertable";
+    public final String secondaryLSMTreeName1 = "usertable1";
+    public final String secondaryLSMTreeName2 = "usertable2";
 
     AbstractReadExecutor(ColumnFamilyStore cfs, ReadCommand command, ReplicaPlan.ForTokenRead replicaPlan,
             int initialDataRequestCount, long queryStartNanoTime) {
@@ -132,43 +142,117 @@ public abstract class AbstractReadExecutor {
     private void makeRequests(ReadCommand readCommand, Iterable<Replica> replicas) {
         boolean hasLocalEndpoint = false;
         Message<ReadCommand> message = null;
-        logger.debug("[Tinoryj] Read make request replicas: {}", replicas);
-        int replicationIDIndicatorForSendRequest = 0;
         for (Replica replica : replicas) {
             assert replica.isFull() || readCommand.acceptsTransient();
             InetAddressAndPort endpoint = replica.endpoint();
-            if (replica.isSelf()) {
-                hasLocalEndpoint = true;
-                continue;
-            }
 
             if (traceState != null)
                 traceState.trace("reading {} from {}", readCommand.isDigestQuery() ? "digest" : "data", endpoint);
 
-            if (replicationIDIndicatorForSendRequest != 0) {
-                logger.debug(
-                        "[Tinoryj] Send read request to replica ID: {}, reading {} from {}, the target key space is {}, column family is {}",
-                        replicationIDIndicatorForSendRequest,
-                        readCommand.isDigestQuery() ? "digest" : "data", endpoint, readCommand.metadata().keyspace,
-                        readCommand.metadata().name);
+            if (replica.isSelf()) {
+                hasLocalEndpoint = true;
+                continue;
             }
+
             if (null == message)
                 message = readCommand.createMessage(false);
 
             MessagingService.instance().sendWithCallback(message, endpoint, handler);
-            logger.debug("[Tinoryj] Read make request from replica ID: {}, reading {} from {}", replica,
-                    readCommand.isDigestQuery() ? "digest" : "data", endpoint);
-            replicationIDIndicatorForSendRequest++;
         }
 
         // We delay the local (potentially blocking) read till the end to avoid stalling
         // remote requests.
         if (hasLocalEndpoint) {
-            logger.trace("reading {} locally", readCommand.isDigestQuery() ? "digest" : "data");
             Stage.READ.maybeExecuteImmediately(new LocalReadRunnable(readCommand, handler));
         }
     }
 
+    public static void printStackTace(String msg) {
+        logger.debug("stack trace {}", new Exception(msg));
+    }
+
+    private void makeRequestsCassandraEC(ReadCommand readCommand, Iterable<Replica> replicas) {
+        // printStackTace("makeRequestsCassandraEC");
+        int sendRequestNumberAccordingToConsistencyLevel = 3;
+        switch (replicaPlan().consistencyLevel()) {
+            case ONE:
+                sendRequestNumberAccordingToConsistencyLevel = 1;
+                break;
+            case TWO:
+                sendRequestNumberAccordingToConsistencyLevel = 2;
+                break;
+            case ALL:
+                sendRequestNumberAccordingToConsistencyLevel = sendRequestAddressesAndPorts.size();
+                break;
+            default:
+                logger.error(
+                        "[Tinoryj] Not support such consistency in CassandraEC, using default consistency read number = {}",
+                        replicaPlan().consistencyLevel(), sendRequestNumberAccordingToConsistencyLevel);
+        }
+
+        for (int replicationIDIndicatorForSendRequest = 0; replicationIDIndicatorForSendRequest < 3; replicationIDIndicatorForSendRequest++) {
+            if (replicationIDIndicatorForSendRequest == sendRequestNumberAccordingToConsistencyLevel) {
+                break;
+            }
+            InetAddressAndPort endpoint = sendRequestAddressesAndPorts.get(replicationIDIndicatorForSendRequest);
+
+            if (traceState != null)
+                traceState.trace("reading {} from {}", readCommand.isDigestQuery() ? "digest" : "data", endpoint);
+
+            ReadCommand readCommandCopy;
+            switch (replicationIDIndicatorForSendRequest) {
+                case 0:
+                    readCommand.updateTableMetadata(
+                            Keyspace.open("ycsb").getColumnFamilyStore(primaryLSMTreeName).metadata());
+                    ColumnFilter newColumnFilter = ColumnFilter.allRegularColumnsBuilder(readCommand.metadata(), false)
+                            .build();
+                    readCommand.updateColumnFilter(newColumnFilter);
+                    readCommand.setIsDigestQuery(false);
+                    readCommandCopy = readCommand.copy();
+                    break;
+                case 1:
+                    readCommand.updateTableMetadata(
+                            Keyspace.open("ycsb").getColumnFamilyStore(secondaryLSMTreeName1).metadata());
+                    ColumnFilter newColumnFilter1 = ColumnFilter.allRegularColumnsBuilder(readCommand.metadata(), false)
+                            .build();
+                    readCommand.updateColumnFilter(newColumnFilter1);
+                    readCommand.setIsDigestQuery(true);
+                    readCommandCopy = readCommand.copy();
+                    break;
+                case 2:
+                    readCommand.updateTableMetadata(
+                            Keyspace.open("ycsb").getColumnFamilyStore(secondaryLSMTreeName2).metadata());
+                    ColumnFilter newColumnFilter2 = ColumnFilter.allRegularColumnsBuilder(readCommand.metadata(), false)
+                            .build();
+                    readCommand.updateColumnFilter(newColumnFilter2);
+                    readCommand.setIsDigestQuery(true);
+                    readCommandCopy = readCommand.copy();
+                    break;
+                default:
+                    logger.debug("[Tinoryj] Not support replication number more than 3!!!");
+                    readCommandCopy = readCommand.copy();
+            }
+
+            if (endpoint.equals(FBUtilities.getBroadcastAddressAndPort())) {
+                logger.debug(
+                        "[Tinoryj] Make {} read [Locally] request for key token = {}, replica address = {}, target column name = {}",
+                        readCommand.isDigestQuery() ? "digest" : "data", targetReadToken,
+                        endpoint, readCommand.metadata().name);
+                Stage.READ.maybeExecuteImmediately(new LocalReadRunnable(readCommandCopy, handler));
+                this.command = readCommandCopy;
+                this.cfs = Keyspace.open("ycsb").getColumnFamilyStore(readCommandCopy.metadata().name);
+                ;
+            } else {
+                logger.debug(
+                        "[Tinoryj] Make {} read [Remote] request for key token = {}, replica address = {}, target column name = {}",
+                        readCommand.isDigestQuery() ? "digest" : "data", targetReadToken,
+                        endpoint, readCommand.metadata().name);
+                Message<ReadCommand> message = readCommandCopy.createMessage(false);
+                MessagingService.instance().sendWithCallback(message, endpoint, handler);
+            }
+        }
+    }
+
     /**
      * Perform additional requests if it looks like the original will time out. May
      * block while it waits
@@ -180,16 +264,22 @@ public abstract class AbstractReadExecutor {
      * send the initial set of requests
      */
     public void executeAsync() {
-        logger.debug(
-                "[Tinoryj] Read executeAsync replicas: {}, initialDataRequestCount: {}, target primary and secondary nodes info = {}",
-                replicaPlan().contacts(),
-                initialDataRequestCount);
+        // logger.debug(
+        // "[Tinoryj] Read executeAsync replicas: {}, initialDataRequestCount: {}",
+        // replicaPlan().contacts(),
+        // initialDataRequestCount);
         EndpointsForToken selected = replicaPlan().contacts();
+        // if (command.metadata().keyspace.equals("ycsb")) {
+        // // Tinoryj-> the read path for CassandraEC test with "YCSB".
+        // makeRequestsCassandraEC(command, selected);
+        // } else {
+        // Normal read path for Cassandra system tables.
         EndpointsForToken fullDataRequests = selected.filter(Replica::isFull, initialDataRequestCount);
         makeFullDataRequests(fullDataRequests); // Tinoryj-> to read the primary replica.
         makeTransientDataRequests(selected.filterLazily(Replica::isTransient));
         // Tinoryj-> to read the possible secondary replica.
         makeDigestRequests(selected.filterLazily(r -> r.isFull() && !fullDataRequests.contains(r)));
+        // }
     }
 
     /**
@@ -204,23 +294,49 @@ public abstract class AbstractReadExecutor {
         ReplicaPlan.ForTokenRead replicaPlan = ReplicaPlans.forRead(keyspace, command.partitionKey().getToken(),
                 consistencyLevel, retry);
 
-        List<InetAddress> sendRequestAddresses = StorageService.instance.getNaturalEndpoints(command
-                .metadata().keyspace,
-                command.partitionKey().getKey());
-        if (replicaPlan.contacts().endpointList().get(0).getAddress().equals(sendRequestAddresses.get(0))) {
-            logger.debug("[Tinoryj] the primary node is the first node in the natural storage node list");
-        } else {
-            logger.debug(
-                    "[Tinoryj-ERROR] the primary node is not the first node in the natural storage node list ++ the replication plan for read is {}, natural storage node list = {}",
-                    replicaPlan,
-                    sendRequestAddresses);
+        if (keyspace.getName().equals("ycsb")) {
+            targetReadToken = command.partitionKey().getToken();
+            sendRequestAddressesAndPorts = StorageService.instance.getNaturalEndpointsForCassandraEC(command
+                    .metadata().keyspace,
+                    command.partitionKey().getKey());
+            // List<InetAddress> sendRequestAddresses =
+            // StorageService.instance.getNaturalEndpoints(command
+            // .metadata().keyspace,
+            // command.partitionKey().getKey());
+            // if (sendRequestAddressesAndPorts.size() != 3) {
+            // logger.debug("[Tinoryj-ERROR] sendRequestAddressesAndPorts.size() != 3");
+            // }
+            // if
+            // (replicaPlan.contacts().endpointList().get(0).getAddress().equals(sendRequestAddresses.get(0))
+            // &&
+            // replicaPlan.contacts().endpointList().get(1).getAddress().equals(sendRequestAddresses.get(1))
+            // &&
+            // replicaPlan.contacts().endpointList().get(2).getAddress().equals(sendRequestAddresses.get(2)))
+            // {
+            // } else {
+            // logger.debug(
+            // "[Tinoryj-ERROR] for key token = {}, the primary node is not the first node
+            // in the natural storage node list. The replication plan for read is {},
+            // natural storage node list = {}",
+            // command.partitionKey().getToken(),
+            // replicaPlan.contacts().endpointList(),
+            // sendRequestAddresses);
+            // }
         }
 
+        // if (command.metadata().keyspace.equals("ycsb")) {
+        // logger.debug("[Tinoryj] NeverSpeculatingReadExecutor is in use");
+        // return new NeverSpeculatingReadExecutor(cfs, command, replicaPlan,
+        // queryStartNanoTime, false);
+        // }
+
         // Speculative retry is disabled *OR*
         // 11980: Disable speculative retry if using EACH_QUORUM in order to prevent
         // miscounting DC responses
-        if (retry.equals(NeverSpeculativeRetryPolicy.INSTANCE) || consistencyLevel == ConsistencyLevel.EACH_QUORUM)
+        if (retry.equals(NeverSpeculativeRetryPolicy.INSTANCE) || consistencyLevel == ConsistencyLevel.EACH_QUORUM) {
+            logger.debug("[Tinoryj] NeverSpeculatingReadExecutor is in use");
             return new NeverSpeculatingReadExecutor(cfs, command, replicaPlan, queryStartNanoTime, false);
+        }
 
         // There are simply no extra replicas to speculate.
         // Handle this separately so it can record failed attempts to speculate due to
@@ -229,16 +345,22 @@ public abstract class AbstractReadExecutor {
 
         {
             boolean recordFailedSpeculation = consistencyLevel != ConsistencyLevel.ALL;
+            logger.debug("[Tinoryj] NeverSpeculatingReadExecutor is in use");
             return new NeverSpeculatingReadExecutor(cfs, command, replicaPlan, queryStartNanoTime,
                     recordFailedSpeculation);
         }
 
-        if (retry.equals(AlwaysSpeculativeRetryPolicy.INSTANCE))
+        if (retry.equals(AlwaysSpeculativeRetryPolicy.INSTANCE)) {
+            logger.debug("[Tinoryj] AlwaysSpeculatingReadExecutor is in use");
             return new AlwaysSpeculatingReadExecutor(cfs, command, replicaPlan, queryStartNanoTime);
-        else // PERCENTILE
-             // or
-             // CUSTOM.
+        } else {
+            // PERCENTILE
+            // or
+            logger.debug("[Tinoryj] SpeculatingReadExecutor is in use");
+            // CUSTOM.
             return new SpeculatingReadExecutor(cfs, command, replicaPlan, queryStartNanoTime);
+        }
+
     }
 
     public boolean hasLocalRead() {
@@ -347,8 +469,9 @@ public abstract class AbstractReadExecutor {
                 // insufficient
                 super.replicaPlan.addToContacts(extraReplica);
 
-                if (traceState != null)
+                if (traceState != null) {
                     traceState.trace("speculating read retry on {}", extraReplica);
+                }
                 logger.trace("speculating read retry on {}", extraReplica);
                 MessagingService.instance().sendWithCallback(retryCommand.createMessage(false), extraReplica.endpoint(),
                         handler);
@@ -415,12 +538,18 @@ public abstract class AbstractReadExecutor {
                 throw e;
             }
         }
-
         // return immediately, or begin a read repair
+        // if (command.metadata().keyspace.equals("ycsb")) {
+        // setResult(digestResolver.getData());
+        // if (!digestResolver.responsesMatch()) {
+        // logger.debug("[Tinoryj] ReadExecutor awaitResponses() digest mismatch,
+        // starting read repair for key {}",
+        // getKey());
+        // }
+        // } else {
         if (digestResolver.responsesMatch()) {
             setResult(digestResolver.getData());
         } else {
-            Tracing.trace("Digest mismatch: Mismatch for key {}", getKey());
             readRepair.startRepair(digestResolver, this::setResult);
             if (logBlockingReadRepairAttempt) {
                 logger.info("Blocking Read Repair triggered for query [{}] at CL.{} with endpoints {}",
@@ -429,6 +558,7 @@ public abstract class AbstractReadExecutor {
                         replicaPlan().contacts());
             }
         }
+        // }
     }
 
     public void awaitReadRepair() throws ReadTimeoutException {
diff --git a/src/java/org/apache/cassandra/service/reads/DataResolver.java b/src/java/org/apache/cassandra/service/reads/DataResolver.java
index 8dc314b6..c5aac96d 100644
--- a/src/java/org/apache/cassandra/service/reads/DataResolver.java
+++ b/src/java/org/apache/cassandra/service/reads/DataResolver.java
@@ -60,87 +60,80 @@ import org.apache.cassandra.service.reads.repair.RepairedDataVerifier;
 
 import static com.google.common.collect.Iterables.*;
 
-public class DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E, P>> extends ResponseResolver<E, P>
-{
+public class DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E, P>> extends ResponseResolver<E, P> {
     private final boolean enforceStrictLiveness;
     private final ReadRepair<E, P> readRepair;
     private final boolean trackRepairedStatus;
 
-    public DataResolver(ReadCommand command, Supplier<? extends P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime)
-    {
+    public DataResolver(ReadCommand command, Supplier<? extends P> replicaPlan, ReadRepair<E, P> readRepair,
+            long queryStartNanoTime) {
         this(command, replicaPlan, readRepair, queryStartNanoTime, false);
     }
 
-    public DataResolver(ReadCommand command, Supplier<? extends P> replicaPlan, ReadRepair<E, P> readRepair, long queryStartNanoTime, boolean trackRepairedStatus)
-    {
+    public DataResolver(ReadCommand command, Supplier<? extends P> replicaPlan, ReadRepair<E, P> readRepair,
+            long queryStartNanoTime, boolean trackRepairedStatus) {
         super(command, replicaPlan, queryStartNanoTime);
         this.enforceStrictLiveness = command.metadata().enforceStrictLiveness();
         this.readRepair = readRepair;
         this.trackRepairedStatus = trackRepairedStatus;
     }
 
-    public PartitionIterator getData()
-    {
+    public PartitionIterator getData() {
         ReadResponse response = responses.get(0).payload;
         return UnfilteredPartitionIterators.filter(response.makeIterator(command), command.nowInSec());
     }
 
-    public boolean isDataPresent()
-    {
+    public boolean isDataPresent() {
         return !responses.isEmpty();
     }
 
-    public PartitionIterator resolve()
-    {
+    public PartitionIterator resolve() {
         return resolve(null);
     }
 
-    public PartitionIterator resolve(@Nullable Runnable runOnShortRead)
-    {
-        // We could get more responses while this method runs, which is ok (we're happy to ignore any response not here
-        // at the beginning of this method), so grab the response count once and use that through the method.
+    public PartitionIterator resolve(@Nullable Runnable runOnShortRead) {
+        // We could get more responses while this method runs, which is ok (we're happy
+        // to ignore any response not here
+        // at the beginning of this method), so grab the response count once and use
+        // that through the method.
         Collection<Message<ReadResponse>> messages = responses.snapshot();
         assert !any(messages, msg -> msg.payload.isDigestResponse());
 
         E replicas = replicaPlan().readCandidates().select(transform(messages, Message::from), false);
 
-        // If requested, inspect each response for a digest of the replica's repaired data set
+        // If requested, inspect each response for a digest of the replica's repaired
+        // data set
         RepairedDataTracker repairedDataTracker = trackRepairedStatus
-                                                  ? new RepairedDataTracker(getRepairedDataVerifier(command))
-                                                  : null;
-        if (repairedDataTracker != null)
-        {
+                ? new RepairedDataTracker(getRepairedDataVerifier(command))
+                : null;
+        if (repairedDataTracker != null) {
             messages.forEach(msg -> {
-                if (msg.payload.mayIncludeRepairedDigest() && replicas.byEndpoint().get(msg.from()).isFull())
-                {
+                if (msg.payload.mayIncludeRepairedDigest() && replicas.byEndpoint().get(msg.from()).isFull()) {
                     repairedDataTracker.recordDigest(msg.from(),
-                                                     msg.payload.repairedDataDigest(),
-                                                     msg.payload.isRepairedDigestConclusive());
+                            msg.payload.repairedDataDigest(),
+                            msg.payload.isRepairedDigestConclusive());
                 }
             });
         }
 
-        if (!needsReplicaFilteringProtection())
-        {
+        if (!needsReplicaFilteringProtection()) {
             ResolveContext context = new ResolveContext(replicas);
             return resolveWithReadRepair(context,
-                                         i -> shortReadProtectedResponse(i, context, runOnShortRead),
-                                         UnaryOperator.identity(),
-                                         repairedDataTracker);
+                    i -> shortReadProtectedResponse(i, context, runOnShortRead),
+                    UnaryOperator.identity(),
+                    repairedDataTracker);
         }
 
         return resolveWithReplicaFilteringProtection(replicas, repairedDataTracker);
     }
 
-    private boolean needsReplicaFilteringProtection()
-    {
+    private boolean needsReplicaFilteringProtection() {
         if (command.rowFilter().isEmpty())
             return false;
 
         IndexMetadata indexMetadata = command.indexMetadata();
 
-        if (indexMetadata == null || !indexMetadata.isCustom())
-        {
+        if (indexMetadata == null || !indexMetadata.isCustom()) {
             return true;
         }
 
@@ -155,64 +148,64 @@ public class DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<
         return index.supportsReplicaFilteringProtection(command.rowFilter());
     }
 
-    private class ResolveContext
-    {
+    private class ResolveContext {
         private final E replicas;
         private final DataLimits.Counter mergedResultCounter;
 
-        private ResolveContext(E replicas)
-        {
+        private ResolveContext(E replicas) {
             this.replicas = replicas;
             this.mergedResultCounter = command.limits().newCounter(command.nowInSec(),
-                                                                   true,
-                                                                   command.selectsFullPartition(),
-                                                                   enforceStrictLiveness);
+                    true,
+                    command.selectsFullPartition(),
+                    enforceStrictLiveness);
         }
 
-        private boolean needsReadRepair()
-        {
+        private boolean needsReadRepair() {
             return replicas.size() > 1;
         }
 
-        private boolean needShortReadProtection()
-        {
-            // If we have only one result, there is no read repair to do and we can't get short reads
-            // Also, so-called "short reads" stems from nodes returning only a subset of the results they have for a
-            // partition due to the limit, but that subset not being enough post-reconciliation. So if we don't have limit,
+        private boolean needShortReadProtection() {
+            // If we have only one result, there is no read repair to do and we can't get
+            // short reads
+            // Also, so-called "short reads" stems from nodes returning only a subset of the
+            // results they have for a
+            // partition due to the limit, but that subset not being enough
+            // post-reconciliation. So if we don't have limit,
             // don't bother protecting against short reads.
             return replicas.size() > 1 && !command.limits().isUnlimited();
         }
     }
 
     @FunctionalInterface
-    private interface ResponseProvider
-    {
+    private interface ResponseProvider {
         UnfilteredPartitionIterator getResponse(int i);
     }
 
-    private UnfilteredPartitionIterator shortReadProtectedResponse(int i, ResolveContext context, @Nullable Runnable onShortRead)
-    {
+    private UnfilteredPartitionIterator shortReadProtectedResponse(int i, ResolveContext context,
+            @Nullable Runnable onShortRead) {
         UnfilteredPartitionIterator originalResponse = responses.get(i).payload.makeIterator(command);
 
         return context.needShortReadProtection()
-               ? ShortReadProtection.extend(context.replicas.get(i),
-                                            () -> { responses.clearUnsafe(i); if (onShortRead != null) onShortRead.run(); },
-                                            originalResponse,
-                                            command,
-                                            context.mergedResultCounter,
-                                            queryStartNanoTime,
-                                            enforceStrictLiveness)
-               : originalResponse;
+                ? ShortReadProtection.extend(context.replicas.get(i),
+                        () -> {
+                            responses.clearUnsafe(i);
+                            if (onShortRead != null)
+                                onShortRead.run();
+                        },
+                        originalResponse,
+                        command,
+                        context.mergedResultCounter,
+                        queryStartNanoTime,
+                        enforceStrictLiveness)
+                : originalResponse;
     }
 
     private PartitionIterator resolveWithReadRepair(ResolveContext context,
-                                                    ResponseProvider responseProvider,
-                                                    UnaryOperator<PartitionIterator> preCountFilter,
-                                                    RepairedDataTracker repairedDataTracker)
-    {
+            ResponseProvider responseProvider,
+            UnaryOperator<PartitionIterator> preCountFilter,
+            RepairedDataTracker repairedDataTracker) {
         UnfilteredPartitionIterators.MergeListener listener = null;
-        if (context.needsReadRepair() && readRepair != NoopReadRepair.instance)
-        {
+        if (context.needsReadRepair() && readRepair != NoopReadRepair.instance) {
             P sources = replicaPlan.get().withContacts(context.replicas);
             listener = wrapMergeListener(readRepair.getMergeListener(sources), sources, repairedDataTracker);
         }
@@ -221,71 +214,90 @@ public class DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<
     }
 
     @SuppressWarnings("resource")
-    private PartitionIterator resolveWithReplicaFilteringProtection(E replicas, RepairedDataTracker repairedDataTracker)
-    {
-        // Protecting against inconsistent replica filtering (some replica returning a row that is outdated but that
-        // wouldn't be removed by normal reconciliation because up-to-date replica have filtered the up-to-date version
+    private PartitionIterator resolveWithReplicaFilteringProtection(E replicas,
+            RepairedDataTracker repairedDataTracker) {
+        // Protecting against inconsistent replica filtering (some replica returning a
+        // row that is outdated but that
+        // wouldn't be removed by normal reconciliation because up-to-date replica have
+        // filtered the up-to-date version
         // of that row) involves 3 main elements:
-        //   1) We combine short-read protection and a merge listener that identifies potentially "out-of-date"
-        //      rows to create an iterator that is guaranteed to produce enough valid row results to satisfy the query
-        //      limit if enough actually exist. A row is considered out-of-date if its merged from is non-empty and we
-        //      receive not response from at least one replica. In this case, it is possible that filtering at the
-        //      "silent" replica has produced a more up-to-date result.
-        //   2) This iterator is passed to the standard resolution process with read-repair, but is first wrapped in a
-        //      response provider that lazily "completes" potentially out-of-date rows by directly querying them on the
-        //      replicas that were previously silent. As this iterator is consumed, it caches valid data for potentially
-        //      out-of-date rows, and this cached data is merged with the fetched data as rows are requested. If there
-        //      is no replica divergence, only rows in the partition being evalutated will be cached (then released
-        //      when the partition is consumed).
-        //   3) After a "complete" row is materialized, it must pass the row filter supplied by the original query
-        //      before it counts against the limit.
+        // 1) We combine short-read protection and a merge listener that identifies
+        // potentially "out-of-date"
+        // rows to create an iterator that is guaranteed to produce enough valid row
+        // results to satisfy the query
+        // limit if enough actually exist. A row is considered out-of-date if its merged
+        // from is non-empty and we
+        // receive not response from at least one replica. In this case, it is possible
+        // that filtering at the
+        // "silent" replica has produced a more up-to-date result.
+        // 2) This iterator is passed to the standard resolution process with
+        // read-repair, but is first wrapped in a
+        // response provider that lazily "completes" potentially out-of-date rows by
+        // directly querying them on the
+        // replicas that were previously silent. As this iterator is consumed, it caches
+        // valid data for potentially
+        // out-of-date rows, and this cached data is merged with the fetched data as
+        // rows are requested. If there
+        // is no replica divergence, only rows in the partition being evalutated will be
+        // cached (then released
+        // when the partition is consumed).
+        // 3) After a "complete" row is materialized, it must pass the row filter
+        // supplied by the original query
+        // before it counts against the limit.
 
         // We need separate contexts, as each context has his own counter
         ResolveContext firstPhaseContext = new ResolveContext(replicas);
         ResolveContext secondPhaseContext = new ResolveContext(replicas);
         ReplicaFilteringProtection<E> rfp = new ReplicaFilteringProtection<>(replicaPlan().keyspace(),
-                                                                             command,
-                                                                             replicaPlan().consistencyLevel(),
-                                                                             queryStartNanoTime,
-                                                                             firstPhaseContext.replicas,
-                                                                             DatabaseDescriptor.getCachedReplicaRowsWarnThreshold(),
-                                                                             DatabaseDescriptor.getCachedReplicaRowsFailThreshold());
+                command,
+                replicaPlan().consistencyLevel(),
+                queryStartNanoTime,
+                firstPhaseContext.replicas,
+                DatabaseDescriptor.getCachedReplicaRowsWarnThreshold(),
+                DatabaseDescriptor.getCachedReplicaRowsFailThreshold());
 
         PartitionIterator firstPhasePartitions = resolveInternal(firstPhaseContext,
-                                                                 rfp.mergeController(),
-                                                                 i -> shortReadProtectedResponse(i, firstPhaseContext, null),
-                                                                 UnaryOperator.identity());
+                rfp.mergeController(),
+                i -> shortReadProtectedResponse(i, firstPhaseContext, null),
+                UnaryOperator.identity());
 
         PartitionIterator completedPartitions = resolveWithReadRepair(secondPhaseContext,
-                                                                      i -> rfp.queryProtectedPartitions(firstPhasePartitions, i),
-                                                                      results -> command.rowFilter().filter(results, command.metadata(), command.nowInSec()),
-                                                                      repairedDataTracker);
+                i -> rfp.queryProtectedPartitions(firstPhasePartitions, i),
+                results -> command.rowFilter().filter(results, command.metadata(), command.nowInSec()),
+                repairedDataTracker);
 
-        // Ensure that the RFP instance has a chance to record metrics when the iterator closes.
+        // Ensure that the RFP instance has a chance to record metrics when the iterator
+        // closes.
         return PartitionIterators.doOnClose(completedPartitions, firstPhasePartitions::close);
     }
 
     @SuppressWarnings("resource")
     private PartitionIterator resolveInternal(ResolveContext context,
-                                              UnfilteredPartitionIterators.MergeListener mergeListener,
-                                              ResponseProvider responseProvider,
-                                              UnaryOperator<PartitionIterator> preCountFilter)
-    {
+            UnfilteredPartitionIterators.MergeListener mergeListener,
+            ResponseProvider responseProvider,
+            UnaryOperator<PartitionIterator> preCountFilter) {
         int count = context.replicas.size();
         List<UnfilteredPartitionIterator> results = new ArrayList<>(count);
         for (int i = 0; i < count; i++)
             results.add(responseProvider.getResponse(i));
 
         /*
-         * Even though every response, individually, will honor the limit, it is possible that we will, after the merge,
-         * have more rows than the client requested. To make sure that we still conform to the original limit,
-         * we apply a top-level post-reconciliation counter to the merged partition iterator.
+         * Even though every response, individually, will honor the limit, it is
+         * possible that we will, after the merge,
+         * have more rows than the client requested. To make sure that we still conform
+         * to the original limit,
+         * we apply a top-level post-reconciliation counter to the merged partition
+         * iterator.
          *
-         * Short read protection logic (ShortReadRowsProtection.moreContents()) relies on this counter to be applied
-         * to the current partition to work. For this reason we have to apply the counter transformation before
-         * empty partition discard logic kicks in - for it will eagerly consume the iterator.
+         * Short read protection logic (ShortReadRowsProtection.moreContents()) relies
+         * on this counter to be applied
+         * to the current partition to work. For this reason we have to apply the
+         * counter transformation before
+         * empty partition discard logic kicks in - for it will eagerly consume the
+         * iterator.
          *
-         * That's why the order here is: 1) merge; 2) filter rows; 3) count; 4) discard empty partitions
+         * That's why the order here is: 1) merge; 2) filter rows; 3) count; 4) discard
+         * empty partitions
          *
          * See CASSANDRA-13747 for more details.
          */
@@ -298,128 +310,129 @@ public class DataResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<
         return Transformation.apply(counted, new EmptyPartitionsDiscarder());
     }
 
-    protected RepairedDataVerifier getRepairedDataVerifier(ReadCommand command)
-    {
+    protected RepairedDataVerifier getRepairedDataVerifier(ReadCommand command) {
         return RepairedDataVerifier.verifier(command);
     }
 
-    private String makeResponsesDebugString(DecoratedKey partitionKey)
-    {
-        return Joiner.on(",\n").join(transform(getMessages().snapshot(), m -> m.from() + " => " + m.payload.toDebugString(command, partitionKey)));
+    private String makeResponsesDebugString(DecoratedKey partitionKey) {
+        return Joiner.on(",\n").join(transform(getMessages().snapshot(),
+                m -> m.from() + " => " + m.payload.toDebugString(command, partitionKey)));
     }
 
-    private UnfilteredPartitionIterators.MergeListener wrapMergeListener(UnfilteredPartitionIterators.MergeListener partitionListener,
-                                                                         P sources,
-                                                                         RepairedDataTracker repairedDataTracker)
-    {
-        // Avoid wrapping no-op listener as it doesn't throw, unless we're tracking repaired status
+    private UnfilteredPartitionIterators.MergeListener wrapMergeListener(
+            UnfilteredPartitionIterators.MergeListener partitionListener,
+            P sources,
+            RepairedDataTracker repairedDataTracker) {
+        // Avoid wrapping no-op listener as it doesn't throw, unless we're tracking
+        // repaired status
         // in which case we need to inject the tracker & verify on close
-        if (partitionListener == UnfilteredPartitionIterators.MergeListener.NOOP)
-        {
+        if (partitionListener == UnfilteredPartitionIterators.MergeListener.NOOP) {
             if (repairedDataTracker == null)
                 return partitionListener;
 
-            return new UnfilteredPartitionIterators.MergeListener()
-            {
+            return new UnfilteredPartitionIterators.MergeListener() {
 
-                public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)
-                {
+                public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey,
+                        List<UnfilteredRowIterator> versions) {
                     return UnfilteredRowIterators.MergeListener.NOOP;
                 }
 
-                public void close()
-                {
+                public void close() {
                     repairedDataTracker.verify();
                 }
             };
         }
 
-        return new UnfilteredPartitionIterators.MergeListener()
-        {
-            public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)
-            {
-                UnfilteredRowIterators.MergeListener rowListener = partitionListener.getRowMergeListener(partitionKey, versions);
-
-                return new UnfilteredRowIterators.MergeListener()
-                {
-                    public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions)
-                    {
-                        try
-                        {
+        return new UnfilteredPartitionIterators.MergeListener() {
+            public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey,
+                    List<UnfilteredRowIterator> versions) {
+                UnfilteredRowIterators.MergeListener rowListener = partitionListener.getRowMergeListener(partitionKey,
+                        versions);
+
+                return new UnfilteredRowIterators.MergeListener() {
+                    public void onMergedPartitionLevelDeletion(DeletionTime mergedDeletion, DeletionTime[] versions) {
+                        try {
                             rowListener.onMergedPartitionLevelDeletion(mergedDeletion, versions);
-                        }
-                        catch (AssertionError e)
-                        {
-                            // The following can be pretty verbose, but it's really only triggered if a bug happen, so we'd
+                        } catch (AssertionError e) {
+                            // The following can be pretty verbose, but it's really only triggered if a bug
+                            // happen, so we'd
                             // rather get more info to debug than not.
                             TableMetadata table = command.metadata();
-                            String details = String.format("Error merging partition level deletion on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
-                                                           table,
-                                                           mergedDeletion == null ? "null" : mergedDeletion.toString(),
-                                                           '[' + Joiner.on(", ").join(transform(Arrays.asList(versions), rt -> rt == null ? "null" : rt.toString())) + ']',
-                                                           sources.contacts(),
-                                                           makeResponsesDebugString(partitionKey));
+                            String details = String.format(
+                                    "Error merging partition level deletion on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
+                                    table,
+                                    mergedDeletion == null ? "null" : mergedDeletion.toString(),
+                                    '[' + Joiner.on(", ")
+                                            .join(transform(Arrays.asList(versions),
+                                                    rt -> rt == null ? "null" : rt.toString()))
+                                            + ']',
+                                    sources.contacts(),
+                                    makeResponsesDebugString(partitionKey));
                             throw new AssertionError(details, e);
                         }
                     }
 
-                    public Row onMergedRows(Row merged, Row[] versions)
-                    {
-                        try
-                        {
+                    public Row onMergedRows(Row merged, Row[] versions) {
+                        try {
                             return rowListener.onMergedRows(merged, versions);
-                        }
-                        catch (AssertionError e)
-                        {
-                            // The following can be pretty verbose, but it's really only triggered if a bug happen, so we'd
+                        } catch (AssertionError e) {
+                            // The following can be pretty verbose, but it's really only triggered if a bug
+                            // happen, so we'd
                             // rather get more info to debug than not.
                             TableMetadata table = command.metadata();
-                            String details = String.format("Error merging rows on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
-                                                           table,
-                                                           merged == null ? "null" : merged.toString(table),
-                                                           '[' + Joiner.on(", ").join(transform(Arrays.asList(versions), rt -> rt == null ? "null" : rt.toString(table))) + ']',
-                                                           sources.contacts(),
-                                                           makeResponsesDebugString(partitionKey));
+                            String details = String.format(
+                                    "Error merging rows on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
+                                    table,
+                                    merged == null ? "null" : merged.toString(table),
+                                    '[' + Joiner.on(", ")
+                                            .join(transform(Arrays.asList(versions),
+                                                    rt -> rt == null ? "null" : rt.toString(table)))
+                                            + ']',
+                                    sources.contacts(),
+                                    makeResponsesDebugString(partitionKey));
                             throw new AssertionError(details, e);
                         }
                     }
 
-                    public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged, RangeTombstoneMarker[] versions)
-                    {
-                        try
-                        {
-                            // The code for merging range tombstones is a tad complex and we had the assertions there triggered
-                            // unexpectedly in a few occasions (CASSANDRA-13237, CASSANDRA-13719). It's hard to get insights
-                            // when that happen without more context that what the assertion errors give us however, hence the
+                    public void onMergedRangeTombstoneMarkers(RangeTombstoneMarker merged,
+                            RangeTombstoneMarker[] versions) {
+                        try {
+                            // The code for merging range tombstones is a tad complex and we had the
+                            // assertions there triggered
+                            // unexpectedly in a few occasions (CASSANDRA-13237, CASSANDRA-13719). It's hard
+                            // to get insights
+                            // when that happen without more context that what the assertion errors give us
+                            // however, hence the
                             // catch here that basically gather as much as context as reasonable.
                             rowListener.onMergedRangeTombstoneMarkers(merged, versions);
-                        }
-                        catch (AssertionError e)
-                        {
+                        } catch (AssertionError e) {
 
-                            // The following can be pretty verbose, but it's really only triggered if a bug happen, so we'd
+                            // The following can be pretty verbose, but it's really only triggered if a bug
+                            // happen, so we'd
                             // rather get more info to debug than not.
                             TableMetadata table = command.metadata();
-                            String details = String.format("Error merging RTs on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
-                                                           table,
-                                                           merged == null ? "null" : merged.toString(table),
-                                                           '[' + Joiner.on(", ").join(transform(Arrays.asList(versions), rt -> rt == null ? "null" : rt.toString(table))) + ']',
-                                                           sources.contacts(),
-                                                           makeResponsesDebugString(partitionKey));
+                            String details = String.format(
+                                    "Error merging RTs on %s: merged=%s, versions=%s, sources={%s}, debug info:%n %s",
+                                    table,
+                                    merged == null ? "null" : merged.toString(table),
+                                    '[' + Joiner.on(", ")
+                                            .join(transform(Arrays.asList(versions),
+                                                    rt -> rt == null ? "null" : rt.toString(table)))
+                                            + ']',
+                                    sources.contacts(),
+                                    makeResponsesDebugString(partitionKey));
                             throw new AssertionError(details, e);
                         }
 
                     }
 
-                    public void close()
-                    {
+                    public void close() {
                         rowListener.close();
                     }
                 };
             }
 
-            public void close()
-            {
+            public void close() {
                 partitionListener.close();
                 if (repairedDataTracker != null)
                     repairedDataTracker.verify();
diff --git a/src/java/org/apache/cassandra/service/reads/DigestResolver.java b/src/java/org/apache/cassandra/service/reads/DigestResolver.java
index f7993742..4685f3f9 100644
--- a/src/java/org/apache/cassandra/service/reads/DigestResolver.java
+++ b/src/java/org/apache/cassandra/service/reads/DigestResolver.java
@@ -18,6 +18,7 @@
 package org.apache.cassandra.service.reads;
 
 import java.nio.ByteBuffer;
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.concurrent.TimeUnit;
 
@@ -40,20 +41,18 @@ import org.apache.cassandra.utils.ByteBufferUtil;
 import static com.google.common.collect.Iterables.any;
 import static org.apache.cassandra.utils.Clock.Global.nanoTime;
 
-public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E, P>> extends ResponseResolver<E, P>
-{
+public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRead<E, P>>
+        extends ResponseResolver<E, P> {
     private volatile Message<ReadResponse> dataResponse;
 
-    public DigestResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, long queryStartNanoTime)
-    {
+    public DigestResolver(ReadCommand command, ReplicaPlan.Shared<E, P> replicaPlan, long queryStartNanoTime) {
         super(command, replicaPlan, queryStartNanoTime);
         Preconditions.checkArgument(command instanceof SinglePartitionReadCommand,
-                                    "DigestResolver can only be used with SinglePartitionReadCommand commands");
+                "DigestResolver can only be used with SinglePartitionReadCommand commands");
     }
 
     @Override
-    public void preprocess(Message<ReadResponse> message)
-    {
+    public void preprocess(Message<ReadResponse> message) {
         super.preprocess(message);
         Replica replica = replicaPlan().lookup(message.from());
         if (dataResponse == null && !message.payload.isDigestResponse() && replica.isFull())
@@ -61,37 +60,31 @@ public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRea
     }
 
     @VisibleForTesting
-    public boolean hasTransientResponse()
-    {
+    public boolean hasTransientResponse() {
         return hasTransientResponse(responses.snapshot());
     }
 
-    private boolean hasTransientResponse(Collection<Message<ReadResponse>> responses)
-    {
+    private boolean hasTransientResponse(Collection<Message<ReadResponse>> responses) {
         return any(responses,
                 msg -> !msg.payload.isDigestResponse()
                         && replicaPlan().lookup(msg.from()).isTransient());
     }
 
-    public PartitionIterator getData()
-    {
+    public PartitionIterator getData() {
         Collection<Message<ReadResponse>> responses = this.responses.snapshot();
 
-        if (!hasTransientResponse(responses))
-        {
+        if (!hasTransientResponse(responses)) {
             return UnfilteredPartitionIterators.filter(dataResponse.payload.makeIterator(command), command.nowInSec());
-        }
-        else
-        {
-            // This path can be triggered only if we've got responses from full replicas and they match, but
+        } else {
+            // This path can be triggered only if we've got responses from full replicas and
+            // they match, but
             // transient replica response still contains data, which needs to be reconciled.
-            DataResolver<E, P> dataResolver
-                    = new DataResolver<>(command, replicaPlan, NoopReadRepair.instance, queryStartNanoTime);
+            DataResolver<E, P> dataResolver = new DataResolver<>(command, replicaPlan, NoopReadRepair.instance,
+                    queryStartNanoTime);
 
             dataResolver.preprocess(dataResponse);
             // Reconcile with transient replicas
-            for (Message<ReadResponse> response : responses)
-            {
+            for (Message<ReadResponse> response : responses) {
                 Replica replica = replicaPlan().lookup(response.from());
                 if (replica.isTransient())
                     dataResolver.preprocess(response);
@@ -101,29 +94,53 @@ public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRea
         }
     }
 
-    public boolean responsesMatch()
-    {
+    public boolean responsesMatch() {
         long start = nanoTime();
 
         // validate digests against each other; return false immediately on mismatch.
         ByteBuffer digest = null;
         Collection<Message<ReadResponse>> snapshot = responses.snapshot();
         assert snapshot.size() > 0 : "Attempted response match comparison while no responses have been received.";
-        if (snapshot.size() == 1)
+        if (snapshot.size() == 1) {
             return true;
-
+        }
         // TODO: should also not calculate if only one full node
-        for (Message<ReadResponse> message : snapshot)
-        {
+        Boolean isDigestMatchFlag = true;
+        ByteBuffer digestSet[] = new ByteBuffer[snapshot.size()];
+        ArrayList<InetAddressAndPort> endpoints = new ArrayList<>();
+        int digestIndex = 0;
+        for (Message<ReadResponse> message : snapshot) {
             if (replicaPlan().lookup(message.from()).isTransient())
                 continue;
 
             ByteBuffer newDigest = message.payload.digest(command);
-            if (digest == null)
+            digestSet[digestIndex] = newDigest;
+            if (digest == null) {
                 digest = newDigest;
-            else if (!digest.equals(newDigest))
+            }
+            if (!digest.equals(newDigest)) {
                 // rely on the fact that only single partition queries use digests
-                return false;
+                isDigestMatchFlag = false;
+            }
+            endpoints.add(message.from());
+            digestIndex++;
+        }
+        int noDataCount = 0;
+        for (int i = 0; i < digestIndex; i++) {
+            logger.debug(
+                    "[Tinoryj] Read operation get digest from {}, digest = {}",
+                    endpoints.get(i), "0x" + ByteBufferUtil.bytesToHex(digestSet[i]));
+            String digestStr = "0x" + ByteBufferUtil.bytesToHex(digestSet[i]);
+            if (digestStr.equals("0xd41d8cd98f00b204e9800998ecf8427e")) {
+                noDataCount++;
+            }
+        }
+        if (noDataCount != 0) {
+            logger.debug("[Tinoryj] Read operation get only {} success data response", snapshot.size() - noDataCount);
+        }
+
+        if (isDigestMatchFlag == false) {
+            return false;
         }
 
         if (logger.isTraceEnabled())
@@ -132,16 +149,13 @@ public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRea
         return true;
     }
 
-    public boolean isDataPresent()
-    {
+    public boolean isDataPresent() {
         return dataResponse != null;
     }
 
-    public DigestResolverDebugResult[] getDigestsByEndpoint()
-    {
+    public DigestResolverDebugResult[] getDigestsByEndpoint() {
         DigestResolverDebugResult[] ret = new DigestResolverDebugResult[responses.size()];
-        for (int i = 0; i < responses.size(); i++)
-        {
+        for (int i = 0; i < responses.size(); i++) {
             Message<ReadResponse> message = responses.get(i);
             ReadResponse response = message.payload;
             String digestHex = ByteBufferUtil.bytesToHex(response.digest(command));
@@ -150,14 +164,12 @@ public class DigestResolver<E extends Endpoints<E>, P extends ReplicaPlan.ForRea
         return ret;
     }
 
-    public static class DigestResolverDebugResult
-    {
+    public static class DigestResolverDebugResult {
         public InetAddressAndPort from;
         public String digestHex;
         public boolean isDigestResponse;
 
-        private DigestResolverDebugResult(InetAddressAndPort from, String digestHex, boolean isDigestResponse)
-        {
+        private DigestResolverDebugResult(InetAddressAndPort from, String digestHex, boolean isDigestResponse) {
             this.from = from;
             this.digestHex = digestHex;
             this.isDigestResponse = isDigestResponse;
diff --git a/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java b/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java
index e207fcdd..422e68e0 100644
--- a/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java
+++ b/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java
@@ -145,12 +145,15 @@ public class BlockingPartitionRepair
 
     @VisibleForTesting
     protected void sendRR(Message<Mutation> message, InetAddressAndPort endpoint) {
-        logger.debug("[Tinoryj] Send read request by sendRR func, message payload is {}, endpoint: {}",
-                message.payload.getClass(), endpoint);
+        // logger.debug("[Tinoryj] Send read request by sendRR func, message payload is
+        // {}, endpoint: {}",
+        // message.payload.getClass(), endpoint);
         MessagingService.instance().sendWithCallback(message, endpoint, this);
     }
 
     public void sendInitialRepairs() {
+        // logger.debug("[Tinoryj] Send initial repairs, pendingRepairs: {}",
+        // pendingRepairs);
         mutationsSentTime = nanoTime();
         Replicas.assertFull(pendingRepairs.keySet());
 
